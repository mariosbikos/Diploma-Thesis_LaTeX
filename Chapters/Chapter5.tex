%*******10********20********30********40********50********60********70********80

% For all chapters, use the newdefined chap{} instead of chapter{}
% This will make the text at the top-left of the page be the same as the chapter

\chap{Μελλοντικές Επεκτάσεις}
%\section{Διαδραστική Επαυξημένη Πραγματικότητα}
%OTI ΔΕΝ ΕΙΠΑΜΕ ΣΤΗΝ ΕΙΣΑΓΩΓΗ ΓΙΑ AR, ΚΥΡΙΩΣ ΑΝΑΦΟΡΑ ΕΡΓΑΣΙΩΝ ΠΟΥ ΑΣΧΟΛΟΥΝΤΑΙ ΜΕ INTERACTION OF VIRTUAL OBJECTS, KAI MEΘΟΔΟΥΣ ΠΟΥ ΧΡΗΣΙΜΟΠΟΙΟΥΝΤΑΙ, ΓΙΑΤΙ ΕΙΝΑΙ ΧΡΗΣΙΜΗ Η ΑΛΛΗΛΕΠΙΔΡΑΣΗ ΣΤΟ ar


Συμπερασματικά, τα Κβαντικά Αποτυπώματα αποτελούν μια κλάση κβαντικών καταστάσεων με τεράστιο ενδιαφέρον τόσο στη θεωρητική όσο και στην πειραματική Κβαντική Πληροφορία. Επιπλέον, μετά τη από τη θεώρησή τους από τη σκοπιά της Κρυπτογραφίας, έγινε προφανές ότι τα Κβαντικά Αποτυπώματα παρουσιάζουν βασικές ιδιότητες απόκρυψης πληροφορίας και μπορούν να χρησιμοποιηθούν ποικιλοτρόπως στην κατασκευή κρυπτογραφικών πρωτοκόλλων. Ως καινοτόμο παράδειγμα προτείναμε την κατασκευή ενός σχήματος Κβαντικών Χρημάτων δημόσιου κλειδιού (public-key Quantum Money scheme) όπου τα Κβαντικά Αποτυπώματα χρησιμοποιούνται τόσο για την επαλήθευση του εκδότη του Κβαντικού χαρτονομίσματος μέσω του ψηφιακώς υπογεγραμμένου σειριακού αριθμού όσο και του κατόχου, δηλαδή της κβαντικής κατάστασης του χαρτονομίσματος. 

Ωστόσο, η πρότασή μας βρίσκεται ακόμη σε πρώιμο στάδιο και απαιτείται εντεταμένη θεωρητική εργασία για την εκτέλεση αποδείξεων ασφαλείας. Στην κατεύθυνση αυτή, πιστεύουμε ότι τα Αποκρύπτοντα Αποτυπώματα (hiding fingerprints) \cite{secrets} συσχετίζονται άμεσα με τους κρυμμένους υποχώρους (hidden subspaces) του Aaronson \cite{hidden}. Εάν αποδειχτεί αυτή η συσχέτιση, τότε οι αποδείξεις ασφαλείας και ορθότητας του σχήματος Κβαντικών Χρημάτων δημοσίου κλειδιού του Aaronson μπορούν να εφαρμοστούν και στο δικό μας σχήμα Κβαντικών Χρημάτων με Αποτυπώματα, καθιστώντας το έτσι απεριόριστα ασφαλές απέναντι σε κάθε γνωστή επίθεση εναντίων σχημάτων Κβαντικών Χρημάτων.

Βέβαια, τα εγγενή πλεονεκτήματα του σχήματος που προτείνουμε έναντι των υπόλοιπων προκύπτουν από τις πολλά υποσχόμενες πειραματικές υλοποιήσεις των Κβαντικών Αποτυπωμάτων με χρήση στοιχείων της Κβαντικής Οπτικής, όπως αναλύσαμε στο \cref{c:exper}. Ταυτόχρονα, όμως, πρέπει να εξεταστεί αν με την προτεινόμενη υλοποίηση μέσω σύμφωνων καταστάσεων (coherent states) παραμένουν ανέπαφες οι αποκρύπτουσες ιδιότητες των Κβαντικών Αποτυπωμάτων. Είμαστε αισιόδοξοι για μια θετική έκβαση, καθώς η θεωρία πίσω από τους τυχαίους ψευδο-γραμμικούς κώδικες των Gavinsky και Ito \cite{secrets} παρέχει απεριόριστη ασφάλεια ακόμη και αν ανακτηθεί από τον αντίπαλο ολόκληρη η κωδική λέξη π.χ. μετρώντας τη φάση κάθε παλμού σε ένα Οπτικό Κβαντικό Αποτύπωμα (βλ. \cref{c:exper}). Συνοψίζοντας, η εργασία αυτή αποτελεί μια προσπάθεια διεύρυνσης της ερευνητικής προσέγγισης για πειραματικώς υλοποιήσιμα πρωτόκολλα Κβαντικής Κρυπτογραφίας, που βασίζονται σε Κβαντικά Αποτυπώματα.



Based on the results obtained, the future work include several improvements to the current prototype and additional features to manage a seamless interaction in AR. The fully mobile compatibility is a goal that should be addressed, as mentioned previously, its integration on mobile devices would permit to design a variety of applications, and an early approach with AR is an area of value. The occlusion handling of real hand is an important next step to replace the virtual hand, Vuforia provides access to the background video, from which it is possible to use it as texture and get rid of the virtual hand by replacing a real hand visualization as texture by segmenting it and generating shaders from it. Increase the recognition of more gestures, based on the taxonomy for AR interaction reviewed in the chapter 3, which would consists on implement the gestures and create a gesture-manager to deal with the control and correct recognition of different gestures. Use of machine learning techniques to get better results on the gesture recognition. Although the gesture recognition is based on the pinch positions, machine learning techniques could be implemented to infer gestures in time, defining a ground truth data set with movement’s directions specific for the Leap Motion, where the raw data is not exposed but if we rely on its data, machine learning techniques could improve effectively the gesture recognition. Improve the visual feedback to enhance the user’s perception of the virtual world, as the treatment of shadows and illumination in the AR. There are some implementations already carried out to deal with this feature.


Many prototypes implemented with sophisticated computer vision algorithms to robustly recognize hand gestures have demonstrated that gesture recognition is rather complex and compu- tationally expensive. That is why our methodology is better, since we dont use many resources and it is not computationally expensive. Based on the results obtained, future work may include several improvements to the current prototype and additional features to manage a seamless interaction in AR and an even more immersive experience for the users. The whole pinch gesture detection algorithm has been designed and developed, so that it would be possible to integrate the Realsense Development Kit Camera on top of an Oculus Rift Device. Our algorithms take into account the egocentric viewpoint of the user and his distance from a real table, thus they can work correctly without any changes. In order to achieve the integration of Oculus into our program, one would have to render whatever the application renders, including the video stream from the Realsense Camera, into the texture of a Framebuffer object of openGL. Afterwards, this texture could be applied in a quad and this quad could be placed in a virtual scene made by OpenGL. This virtual scene and the quad that is located inside it would then have to be also copied in the texture of a framebuffer object and passed into the Oculus SDK for modi cations, such as distortion and creation of a stereo view from different viewpoints per eye. In order to get more accurate and robust results on the exact 3D position of the pinch-point which is detected when thumb and fore nger come together a ltering of frames could be implemented to infer gestures in time and this could improve effectively the interaction errors. One could also improve the visual feedback to enhance the users perception of the virtual world. The use of shadows and light source estimation for correct illumination of the scene and virtual objects can greatly affect the realistic rendering of the virtual content in augmented reality. Furthermore, in order for the user to better perceive the depth of the scene, enhanced visualizations can be utilized such as projections in all planes of the scene. In our application we have already used one such visualization by rendering a straight line of projection from the moving chess piece to the chessboard and it was obvious that users can move virtual chess pieces and better understand where the new position of the object is going to be. Finally, since we use 3D models for the virtual chess pieces, a broad collection of different gures could be used, so that user would be able to play chess with different sets each time, ranging from dragons to soldiers. Adding multiple sets of chess models could de nitely improve user experience, but in order to further improve the visual experience attack and death animations could be implemented. Instead of pieces disappearing, destruction through an explosion or decapitation would be even better. Taking everything into consideration, our approach seems to work pretty nicely and the user is able to play a chess game with virtual objects from the beginning to the end without any serious problems.

\section{Ενίσχυση της Οπτικής Αντίληψης}
%des SILTANEN

Use of the tabletop as an AR gaming surface provides a number of interesting user interface opportunities for HMD-based systems. A clear option for AR gaming is the ability to make the game more animated. For example, the game ofWizard’s Chess depicted in the movie \textit{Harry Potter and the Philosopher’s Stone}(Warner Brothers Motion Pictures.) provides graphic animations of chess pieces fighting whenever a piece is captured. The use of AR allows for animated game pieces to interact with each other and with the players of the game. Animation could assist the following characteristics of the game: tuition on how to play the game, tactics, and current attributes for playing a piece (power level, strength, or ability).

An interesting feature of HMD-based AR is that different kinds of information may be displayed to different users. In the case of displaying current attributes, private information about a playing piece could be displayed to the user who controls the piece. This private information could include visualization of potential placement of pieces or future actions. Szalav´ ari et al. [1998] investigated these issues with an AR version of the classic Chinese game Mah-Jong. They employed face-mapping for quick and accurate placement of game pieces to improve the game experience. Because Mah-Jong requires both public and private information, they developed what they term a powerful automatic privacy mechanism. The players hold the private information on handheld PIPs (personal information panels). For this game, a PIP is a magnetic, tracked passive prop. The game is played at a table and AR displays all the game pieces. The AR for this game is the combination of virtual game pieces and the physical game space (people and table).
A tabletop AR version of the fantasy game Jumanji, set in Singapore, was developed by Zhou et al. [2004]. Instead of employing dangerous creatures, the game virtually transports the user to Singapore shopping locations. The user employs dice with MXRToolkit3 fiducialmarkers (similar to ARToolkitmarkers) as the means for game control. The use of physical dice adds a nice tangible feel to the game. Minatani et al. [2007] developed an AR version of Othello with the ARToolkit as the tracking technology. What makes this a very interesting game is the fact that you play a remote user with a physical board. Your opponent and the opponent’s pieces are displayed to each player as virtual objects. The authors explored the rendering space to enable users to “feel” as if the other player was sitting at the same table.


\subsection{Eye Tracking}
Using tiny cameras to observe user pupils and determine
the direction of their gaze is a technology with potential for
AR. The difficulties are that it needs be incorporated into the
eye-wear, calibrated to the user to filter out involuntary eye
movement, and positioned at a fixed distance. With enough
error correction, gaze tracking alternatives for the mouse
such as Stanford‟s EyePoint17 [94] provides a dynamic history
of user‟s interests and intentions that may help the UI
adapt to the future contexts.
\subsection{Φωτορεαλιστική Απεικόνιση}

Το 2006, o Fischer et al. ανέπτυξαν ένα σύστημα ζωγραφικής επαυξημένης πραγματικότητας, το οποίο αλλάζει τα φυσικά και εικονικά αντικείμενα, με αποτέλεσμα να μπορούν να απεικονιστούν με 3 διαφορετικούς τρόπους: με τρόπο που θυμίζει γελοιογραφία που αποτελείται από χρωματιστές κηλίδες και σιλουέτες, με τρόπο που επιτρέπει τη χρήση μικρών πινελιών που προσομοιώνουν πουαντιγιστική τεχνοτροπία ζωγραφικής και, τέλος, ένα είδος ασπρόμαυρης τεχνικής απεικόνισης. Οι συγγραφείς ερεύνησαν έναν αριθμό διαφορετικών μεθόδων μη φωτορεαλιστικής απεικόνισης, με σκοπό να προσομοιώσουν διαφορετικές μορφές τέχνης και γελοιογραφιών [Fischer et al. 2005].


\subsection{Σκιές και Ανίχνευση Σύγκρουσης }


\subsection{Απτική Ανάδραση }

The haptic sense is divided into the kinaesthetic sense
(force, motion) and the tactile sense (tact, touch). Force
feedback devices like joysticks and steering wheels can
suggest impact or resistance and are well-known among
gamers. A popular 6DOF haptic device in teleoperation and
other areas is the PHANTOM (Fig. 11). It optionally provides
7DOF interaction through a pinch or scissors extension.
Tactile feedback devices convey parameters such as roughness,
rigidity, and temperature. Benali-Khoudja et al. [27]
survey tactile interfaces used in teleoperation, 3D surface
simulation, games, etc.
Data gloves use diverse technologies to sense and actuate
and are very reliable, flexible and widely used in VR for
gesture recognition. In AR however they are suitable only for
brief, casual use, as they impede the use of hands in real
world activities and are somewhat awkward looking for
general application. Buchmann et al. [37] connected buzzers
to the fingertips informing users whether they are „touching‟
a virtual object correctly for manipulation, much like the
CyberGlove with CyberTouch by SensAble15.

\section{Το μέλλον της Επαυξημένης Πραγματικότητας}

For Augmented Reality to become a mainstream tool, it must robustly provide useful information at rate that is synonymous with that of human sensory perception. The experimental results of this simple augmented interaction system provide evidence that real-time Augmented Reality is more than a theoretical vision. Using modern computer technology, it is clear that the first step towards the real-time computer perception of human behaviour can be taken. This can be as simple as the classification of basic human actions based on a pre-defined model or as complex as a continuous learning system able to mimic the communication performed by another human being. Many avenues are being explored in this field, all of which await the arrival of the required technology to process the observed information in real-time







There are some restrictions in AR. For example, a system is able to show the augmented view only from those viewpoints from where it has a real image. For example, the user can see a virtual building from ground level looking through a display, but is unable to see the scene from a bird's eye view. In order to provide such visualisations, applications often complement AR with virtual reality mode. Other limitations are due to restricted capacity of devices: their power consumption is too high and their processing, telecommunication and memory capacities are too low, the resolution of cameras is too low, etc. Engineers develop new and better devices, and the capacity of devices increases, they have more built-in sensors, therefore future devices will solve many of current obstacles. Cloud services will in turn help with computationally intensive tasks in future.

From the application developers’ point of view, the diversity of platforms is problematic: they need to port the application to different platforms, as a lot of the code is platform-dependent. HTML5 will be a step towards device-independent applications. It is supported by a large number of mobile devices. It enables presenting videos and audio on web pages as easily as presenting text and images is now, without a need for any plug-ins [271]. HTML5 offers a number of useful properties such as the user’s locations based on GPS or WLAN, canvas technology for dynamic graphics, etc. Although HTML5 is already available for application developers, the standard will be finished probably only on 2014 in W3C. Integration on global databases such as Google Earth, with GPS and local information for example from security cameras together with cloud computing, could lead to real applications similar to the one presented in [272], where the AR navigator is able to visualise cars coming out of the driver’s view (e.g. behind buildings). Another new type of application is security guard guidance system that is able to visualise people behind or inside buildings using AR as means for visualising information from security cameras. The usability of the see-through-devices needs to be improved before they are suitable for mass-market applications, especially the head-mounted see-through devices, which are clumsy as we discussed in Section 7.3. In future, the seethrough portable devices, such as the See-Through Laptop Samsung presented in 2010, might provide a better platform for mobile AR applications (see Figure 97).


Virtual retinal displays that render images with a laser directly on the user’s retina may become more common (see Section 8.2). Another alternative is the use of augmented reality contact lenses as for example envisioned in [274, 275] In future users may use devices intuitively by bending, twisting and squeezing, e.g. the Nokia Kinetic phone presented at Nokia World 2011. This kind of user interface works even if the user wears gloves in cold weather, where touchscreens are unpractical. The displays may be flexible and foldable, such as Polymer Vision’s Readius presented in 2008. Samsung has announced that their new mobile device line-up will feature flexible screens starting in 2012. The whole surface of a mobile device could be touchscreen as in Nokia’s GEM phone concept [276]. While waiting for reasonably sized foldable mobile device with interactive surface (and long-lasting batteries), people might find tablets to be a nice platform for AR applications: they have bigger screens than phones for visualisations, they are lighter than laptops, and they have built-in cameras and sensors. In the early days of virtual reality, Ivan Sutherland visioned “The ultimate display would, of course, be a room within which the computer can control the existence of matter. A chair displayed in such a room would be good enough to sit in. Handcuffs displayed in such a room would be confining, and a bullet displayed in such a room would be fatal.” [277]. It is hard to believe that researchers will ever build a virtual environment that will fulfil the last sentence. One of the exact benefits of virtual and augmented reality is that they enable safe environment for training and experiencing situations that would be too dangerous in reality. On the other hand, researchers have developed such physically altering environments that Sutherland visioned. Physically rendered environment is an environment where the system can alter the physical environment and manipulate grids of “moving physical pixels” (“moxels”). The system can raise and lower moxels and render physical shapes. It is a world-size version of the 3D pin art table where the user presses an object on the pins that will raise and create a 3D replica of the object’s shape. Today the quality of such systems is poor and their density is low. Furthermore, most current systems are able to model only vertical shapes, e.g. the Holodec presented in [278]. 3D holograms, furthermore, are able to visually render arbitrary 3D shapes, but without interaction possibilities. The University of Arizona presented a dynamic 3D hologram, which allows the three-dimensional projection, without the need for special eyewear. The resolution and speed (refreshes every two seconds) leave space for improvement, however [279–281]. In future we will probably see interactive large-scale 3D augmentations, virtual or physical. This will bring telepresence to a different level and enable new possibilities for mixed reality environments, e.g. telesurgery. Besides large 3D mixed environments, there is a trend towards mobile handheld AR with projective and 3D reconstruction capabilities.

Small projectors have also been demonstrated for augmented reality [282]. The integration of small projectors to mobile devices will give a boost for hand-held projective AR. As the development of the mobile devices continues (e.g. processing capacity and battery life), it will open way to applications that are currently computationally too demanding, such as robust feature tracking on mobile phones. In addition, 3D reconstruction will become feasible on mobile devices. Researchers have already demonstrated it using a mobile phone [283, 284]. The first mobile devices with a 3D display are already on the market; we may assume that they will become more common in the future. Easy mobile 3D reconstruction enables a new type of interaction with virtual worlds: people can scan real objects and bring them to virtual world (Second Life, Habbo Hotel, etc). The development of augmented reality enables a new type of interactive TV production; one of the first examples was BBC’s Bamzooki, an augmented reality TV game show that aired in 2009, where participants on the show shout instructions to control virtual autonomous game creatures called Zooks. It is easy to imagine that this kind of TV production will become more common in future. Current technology enables also real-time virtual staging (augmented virtuality), where the whole environment is virtual with real people acting in it. For example, Aalto University Media Centre Lume has a studio that enables this kind of production (see Figure 98). Future AR will bring film effects to live TV broadcasts.

Existing mobile object\/image recognition services such as Google Goggles [247], Kooaba [290] and Snaptell [291] give us only a hint of all the future possibilities. One problem with current solutions is the long lag, typically tens of seconds, between snapping the picture and receiving the response to the query [292]. It is possible to speed up detection using low bit-rate local descriptors and data compression [292], but a better transmission rate and better indexing of data also enable faster queries. The business model of search software is also altering. The SmartAds service that Kooaba offers is an example of the Software-as-a-Service (SaaS) model. The user is charged for access and use of the search and recognition engine. It allows customers to turn their printed products into virtual hyperlinks to additional information [293]. Another new type of AR application is CrowdOptic [294], which received the Frost \& Sullivan 2011 Annual Innovation Award for live-event technology. It is an AR application bound to certain events, e.g. football matches, concerts, etc. It lets users point their smartphones at an athlete or performer and see additional information about the target in real time. Users obtain coaching insights and the stats of the target, and they receive exclusive invitations, ticket discounts and other material through the system. In addition, the event organisers get information on crowd behaviour. The system detects where the attention of people is at any given moment, using GPS data, triangulations and analysing what is being photographed or videoed. Organisers can immediately consider the crowd’s interests in the production. We could call this real-time crowdsourcing or a crowd behaviour analysis tool. The trend in mobile AR browsers is to link information with other social media, use other technologies such as object recognition and face detection, etc. They take advantage of additional sensors and use remote computing and data storage facilities. Future mixed reality concept probably connects location-based services, usercreated content, social media, etc. with the physical world (magazines, billboards, buildings, places, etc.) It allows users to comment, share and link ideas, as well as attach bookmarks, tag physical objects and get information related to them. It provides a platform for interact with the concept of internet of things, where all objects are networked with information and services.

Future augmented reality development interconnects with robotics in several levels. ARDrone (see Figure 101) is a flying iPhone accessory (helicopter) equipped with a multitude of sensors and intelligence. It is controlled via iPhone using simple upper-level commands such as forward, rotate, up, hover, land, take-off, etc. It turns commands into signals for the four on-board motors using information from various sensors such as accelerometer and gyros. Several ARDrones operate in the same environment and users can play augmented reality games with them. It is easy to invent a useful application for such small robots. As we mentioned earlier (in Section 8.3.6) augmented reality has proved useful as an aid to robot operators. One straightforward idea is to use small remote-operated maintenance robots, which are able to augment maintenance instructions and other information for the human operator. Another idea is to use small autonomous robots capable of object recognition to search for missing items.

In James Cameron’s film Avatar (2009), humans are mining a valuable mineral on Pandora, an Earth-like moon with an atmosphere poisonous to humans. The venue is in Alpha Centauri star system in far future. Pandora is inhabited by the Na’vi, three-metre-tall, blue-skinned humanoids. In order to explore Pandora scientists create Na’vi-Human hybrid bodies, which a genetically matched human can mentally operate. The human operating a hybrid body has an illusion of being inside the avatar body, although lying in an avatar link tank at the base. This kind of avatar technology is far in the future, but perhaps not as far as we might think. Researchers have demonstrated that it is possible to create a perceptual illusion of body swapping, i.e. being in a body other than one’s own [295]. The illusion of being in a different body is possible to achieve even with an artificial body of extreme size. Test persons experienced being Barbie (30 cm) as well as a large doll (4 m) in the experiments reported in [296]. The key factor affecting the perception of being in another body is synchronous multisensory input: the user needs to feel touch when she or he sees the artificial body touched. In the abovementioned Barbie/doll experiment, the test person was lying on a table with a head-mounted video display. On the other table was a doll (of a different size). The camera providing the video images for the test person was mounted on a tripod on the place where the dolls head would have been, facing the body. This way the user had the feeling of looking at the doll’s body from a first-persons view. The multi-sensory input was created by touching the doll’s body (in view) and simultaneously touching the participant’s body (out-of view) at the corresponding location. Based on these findings it would be possible to create an illusion of being inside a virtual avatar as well. A human would explore the world through a virtual avatar’s eyes (and see the virtual body from a first-person perspective). With a haptic suit, it would be possible to “feel” virtual collisions that the avatar experiences. This could be the future of Second Life. Furthermore, in the field of Brain-Computer Interface (BCI) and Brain-Machine Interface (BMI), people have been able to create direct neural interfaces to operate artificial limbs, for example (e.g. [297, 298]). Studies with monkeys (which have neural systems that are considered to be very similar to those of humans) demonstrate the ability to control a computer or a robotic arm with their thoughts [299]. In addition, modern humanoid robots such as ASIMO [300] are able to mimic human movements: walk, run, climb stairs, grab with a hand, etc. Creating a real avatar experience becomes a matter of cross-disciplinary cooperation in the future. In principle, people could merge all these technologies and use direct neural interface to operate a humanoid robot. The robot would have cameras on head and these cameras would provide the view for the user (rendered using a retinal display). The robot’s microphones would record audio, which is played to the user’s headphones. The humanoid robot would naturally be equipped with a number of sensors, and their input would be transferred to the user using a multi-sensory (haptic, thermal, etc.) suit.
Embodied and tangible haptic input/output devices enable transferring sense of touch between the virtual and real world. The person using such a device is able to sense physical interaction with a remote or virtual person. Multi-sensory environments are a substantial research area. For example, researchers in CUTE centre (National University of Singapore and Keio University) and MXR Lab in Singapore have done a lot of research in the area of multisensory environments. Huggy pajama [301] is one of their research projects (see Figure 102), where the system is able to transfer a hug over the internet. The user at the other end touches an input device embedded with sensors. The information is transferred to the other end, where the other user receives the same touch from an output device equipped with a haptic interface. Kissenger (aka Kiss Messenger) is a similar system; with special input/output devices, it is able to transfer a kiss over the internet [302] (see Figure 103).Augmented reality systems most commonly employ haptic, visual and audio sensory feedback. The immersive feeling increases if even more senses receive input from the system.Sense of taste is composed of five basic tastes (sweet, salty, bitter, sour, umami), which are relatively easy to produce by blending the appropriate chemicals. Nonetheless, the actual gustation is a combination of taste, smell and food texture, plus some other factors. Therefore, it is possible to bluff sense of taste to some extent with scent and visuals as in the Meta Cookie demonstration we discussed in Section 2.5.2. However, humans can recognise thousands of different smells and are able to detect smells even in infinitesimal quantities. Therefore, it is impossible to produce a set of primary smells to produce all possible smells for the virtual environment (unlike primary colours (red, green, blue) for sense of sight). Sensation is ultimately formed in the human brain when the brain analyses electrical pulses coming from sense organs. Therefore senses can be provoked digitally, by feeding electrical pulses to sensory nerves. The digital taste interface presented in [304] produced sense of taste by actuating the tongue through electrical and thermal stimulations. The experimental results suggested that sourness and saltiness are the main sensations that could be evoked while there is evidence of sweet and bitter sensations too. In medical science, devices enabling digitally produced vision and audition have been used for over a decade (e.g. cochlear implants for deaf). Today’s multi-sensory interface devices are still clumsy and it will take a while before the input/output devices are mature enough to feel natural. In the future, digital gustatory devices become more accurate, and perhaps researchers will be able to produce a substantial amount of digital odours as well support a wide variety of virtual tastes and odours. The haptic devices will improve, immersive display systems become feasible, etc. The way users experience virtual environments is going to change. In future multi-sensory mixed reality environments people will be able to sense temperature, touch, taste, smell, etc., and the whole of the atmosphere. Such environments will support immersive vision and sound systems, and people will be able to sense physical interaction with virtual characters. Today people share their experiences in social media; they send multimedia messages; they use mobile video conferencing to show what they see (the “see-what-I-see” paradigm). The future mixed reality aims to enable a “sense-what-I-sense” paradigm. Telepresence is brought to a new level. People accept the idea that they could love or be loved by a robot [305], and people fall in love with celebrities they have never personally met. It is not farfetched to imagine an intimate relationship with a virtual character. The future multi-sensory mixed reality environment will be able to provide a “multi-sensorysecond- life” where people can interact, communicate and live together with avatars mastered by other humans, robots or computer. Naturally, this development will provoke some ethical issues, which we leave open in this discussion. Furthermore, researchers have been able to reconstruct images that people have seen from brain activity using functional magnetic resonance imaging (fMRI) [306]. The future brain interface could be bidirectional; the system reads from the user’s brain what she/he senses and simulates the other user’s brain accordingly. This would really be sharing experiences.


%thomas cie
My experience with the limitations of current technology is similar to problems reported in the literature. The availability of affordable sensors with the required precision and accuracy has been and still is a real issue. There is no sense in developing gaming technology that is way beyond the price of current consumer-grade technology. I found that the collaboration between technologists and game design/artists creates successful games. As with any electronic gaming, this process is a fusion of the power of modern computing technological advances and creative graphics, storytelling, gameplay, and design. Technology is never going to outperform good gameplay. New technology will enable game designers to develop different and innovative gaming styles


%krevelen
Imagine a technology with which you could see more than
others see, hear more than others hear, and perhaps even
touch, smell and taste things that others can not. What if we
had technology to perceive completely computational elements
and objects within our real world experience, entire
creatures and structures even that help us in our daily activities,
while interacting almost unconsciously through mere
gestures and speech?

Augmented reality (AR) is this technology to create a
“next generation, reality-based interface” [77] and is moving
from laboratories around the world into various industries
and consumer markets. AR supplements the real world with
virtual (computer-generated) objects that appear to coexist in
the same space as the real world. AR was recognised as an
emerging technology of 2007 [79], and with today‟s smart
phones and AR browsers we are starting to embrace this very
new and exciting kind of human-computer interaction


%krevelen
AR has come a long way but still has some distance to go
before industries, the military and the general public will
accept it as a familiar user interface. For example, Airbus
CIMPA still struggles to get their AR systems for assembly
support accepted by the workers [163]. On the other hand,
companies like Information in Place estimated that by 2014,
30\% of mobile workers will be using augmented reality.
Within 5-10 years, Feiner [57] believes that “augmented
reality will have a more profound effect on the way in which
we develop and interact with future computers.” With the
advent of such complementary technologies as tactile networks,
artificial intelligence, cybernetics, and (non-invasive)
brain-computer interfaces, AR might soon pave the way for
ubiquitous (anytime-anywhere) computing [162] of a more
natural kind [13] or even human-machine symbiosis as
Licklider [99] already envisioned in the 1950‟s.




One performance goal for an augmented reality system is that the user can naturally interact with the virtual objects. This interaction should include not only moving the objects, but also feeling their surfaces and the forces applied to them by gravity and by other objects in the environment. Haptic relates to the sense of touch. The user of a haptic interface receives tactile feedback that adds the ability to feel objects. There is no work in the literature that describes haptic extensions in augmented reality systems. All the previous haptic research is in the areas of telemanipulation and virtual reality. We can apply the work in these two areas for haptic interaction with the virtual objects but it does not provide insights into the problems of registration with the real scene or interactions between real and virtual objects. One of the reasons stated by Mine, Brooks, et. al. (1997) for the paucity of virtual-environment applications that have left the laboratory setting is the lack of haptic feedback. Brooks, Ouh-Young, et. al. (1990) describes one of the first examples of a haptic interface used in virtual reality. This system uses a large sized telemanipulation arm that is driven by motors to give force feedback to the user. Molecular docking is the application area addressed by the project. The user operates in an immersive environment experimenting with finding positions for bonding two molecules together. The forces of repulsion and attraction for a bonding operation are correctly simulated and applied to the manipulator. The molecular experts using this system find that this addition of haptic sensation greatly improves their ability to discover novel compound arrangements. Two reported medical applications of haptic interfaces in virtual environments are for medical training. Ziegler, Brandt, et. al. (1997) describe a simulator for arthroscopic surgery that uses force feedback in the virtual environment to train surgeons in the procedure. Using the Rutgers Master II force feedback device Dinsmore, Langrana, et. al. (1997) built a virtual environment for training physicians to locate and palpate tumor masses. The haptic device that is most suited for our augmented reality applications is the Phantom. Its applicability is due not only to its small size but also for the range of haptic feedback that is available. This tabletop device provides force feedback to the user’s fingertip. Because of its size it is well suited for workspaces that fall into the category of virtual-environment interaction that is “working within arm’s reach” (Mine, Brooks et al. 1997). The demonstrations of the work of State, Hirota et. al. (1996) show interaction with virtual objects. Correct visual interactions occur when virtual objects move behind a real object. Using the metaphor of a magnetic finger, the user is also able to attach a virtual object to his fingertip and move it within the workspace. This is all done within a framework of hybrid position tracking that requires knowledge of the 3D location of fiducial points in the scene. There is neither haptic feedback nor dynamic interactions between the virtual and real objects. Using the registration technique of Uenohara and Kanade (1995), Yokokohji, Hollis et. al. (1996) demonstrate a haptic interface for an augmented reality system. They use a Puma 560 robot for the force feedback and track a small plane attached to the end effector of the robot. The entire real scene is draped in blue cloth to allow for easy chroma-keying of the video image of the user’s arm and hand. The augmented display merges a virtual cube located where the small plane was detected and live video of the user’s arm and hand. There is neither motion of the virtual object nor interactions between virtual and real objects in their system.



%ziegler
State of the Art Recent tracking systems produced very good results for indoor applications. Predominantly, they use hybrid approaches and work within a limited range (cp. [ZDB08, HF04, ABB+01]). According to Zhou et al.[ZDB08] current tracking systems consist of two stages. The first stage is dedicated to either learning and training or feature extraction. The second stage takes care of the tracking itself, using the knowledge gained through training or the features that have been extracted. The first stage usually requires the most computational resources, if the system uses a learning algorithm. Using a learning stage can reduce the resources the on-line tracking needs, enabling the system to work in real time. 2.3.5.1 Limitations and Challenges Even though tracking systems are accurate enough to achieve good results, the environments they work in are usually restricted not only to being indoors but also to being known in advance[ABB+01]. Dynamical adaption to unknown environments still poses a challenge. Complex scenes are challenging for real-time 3D tracking as is the motion of target objects[ZDB08]. Coping with rapid camera movements is difficult as resulting motion-blur hinders the re-observation of features. Rapid and unpredictable changes, that may occur in outdoor environments, constrain tracking results[HF04]. Especially illumination changes, which often and repeatedly occur outdoors, complicate the tracking process[ZDB08]. Basically all changes which cannot be controlled or anticipated are hard to handle. Some systems feature automatic reinitialisation, but the recovery of the camera pose, when the tracking has failed, cannot be achieved easily[ZDB08]. It is limited to applications which possess enough knowledge about the environment or which do not solely rely on vision-based tracking. 2.3.5.2 Trends Current research features many tracking approaches. Coping with unknown outdoor environments is an important topic. One way researchers are trying to achieve that is by further investigating hybrid approaches. As the growing number of publications during the past years indicate, Mobile AR becomes more and more popular among researches[SW07, WRM+08]. The growing computational resources of mobile devices present novel possibilities. The number of commercial applications from which users can choose continually rises. Among them are Layar9, Nokia Point \& Find10, Twitter AR11 and Virus Killer 36012. Building a reference presentation of the environment while tracking is a popular trend, research focusing especially on Simultaneous Localisation and Mapping (SLAM)[CGKM07, DRMS07, KM09]. Such systems usually require a high amount of computational resources. However, through certain restrictions, SLAM works on a mobile phone, too, as has recently been shown by the work of Klein and Murray[KM09]. Instead of using as many features as possible and hoping that some of the chosen features provide robust tracking, researchers try to find methods to detect only suitable and useful features in the first place[ZDB08, ST94]. Researchers try to find ways of making initialisation processes automatic[SKSK07]. Focusing on model-based tracking is popular as well[FL07, WS07]. Last but not least, ubiquitous tracking, that is tracking acquired by forming a dense network of sensors that enables tracking everywhere, seems to be achievable in the near future[HPK+07].


%--
Μία από τις μεγαλύτερες προκλήσεις με τις οποίες έρχονται αντιμέτωποι οι δημιουργοί εφαρμογών επαυξημένης πραγματικότητας είναι η σωστή τοποθέτηση του εικονικού αντικειμένου εντός του πραγματικού περιβάλλοντος, έτσι ώστε η συνθετική πληροφορία να δίνει την εντύπωση ότι ανήκει σε αυτό. Η διαδικασία αυτή είναι γνωστή ως registration (γεωαναφορά). Η σωστή συγχώνευση και ευθυγράμμιση των δύο κόσμων – του φυσικού και του παραγόμενου από υπολογιστή – είναι κύρια προϋπόθεση για την εκπλήρωση του στόχου των εφαρμογών επαυξημένης πραγματικότητας, ενώ λάθη ή ανακρίβειες στην τοποθέτηση του εικονικού αντικειμένου θα έχουν ως αποτέλεσμα να χαθεί η ψευδαίσθηση ότι οι δύο κόσμοι συνυπάρχουν [3]. Πολλές εφαρμογές, μάλιστα, όπως για παράδειγμα στην ιατρική, απαιτούν ακριβή γεωαναφορά και δεν είναι επιτρεπτά λάθη και αστοχίες. Για τη σωστή γεωαναφορά, απαραίτητη προϋπόθεση είναι η πρότερη ανίχνευση της θέσης και του προσανατολισμού της κάμερας – και γενικά της συσκευής μέσω της οποίας επαυξάνεται η πραγματικότητα – ή του κεφαλιού του χρήστη (π.χ. σε εφαρμογή με HMD). Η διαδικασία αυτή είναι γνωστή ως tracking (ανίχνευση), απαντά στα ερωτήματα: πού βρίσκεται ο χρήστης, πού εστιάζεται το ενδιαφέρον του και πού πρέπει να παρουσιαστεί το εικονικό αντικείμενο [72] και συνεπώς η σωστή και ακριβής, ανάλογα με την εφαρμογή διεκπεραίωσή της είναι κρίσιμη για τη δημιουργία πειστικών εφαρμογών επαυξημένης πραγματικότητας. Για την επίτευξη των τελευταίων, η ανίχνευση πρέπει πρακτικά να διεξάγεται σε πραγματικό χρόνο, δηλαδή η εκτίμηση της θέσης να γίνεται σε χιλιοστά του δευτερολέπτου, καθώς επίσης και να είναι εύρωστη, δηλαδή να δίνει ικανοποιητικά αποτελέσματα κάτω από ποικίλες συνθήκες, όπως για παράδειγμα σε μεταβαλλόμενο φωτισμό [68]. Υπάρχουν πολλές μέθοδοι ανίχνευσης 6 βαθμών ελευθερίας (6DOF tracking), όπως το μηχανικό tracking, τεχνική που υιοθετήθηκε και από το πρώτο σύστημα επαυξημένης πραγματικότητας του Sutherland, το υπερηχητικό tracking και το οπτικό tracking. Υπάρχουν και άλλες που υπολογίζουν μόνο θέση ή προσανατολισμό, όπως για παράδειγμα το tracking που βασίζεται σε πληροφορίες μόνο από GPS ή μόνο από γυροσκόπια [15]. Για εφαρμογές που απαιτούν ακριβή γεωαναφορά, ωστόσο, απαιτείται ακριβές στιγμιαίο 6DOF tracking υπό οποιεσδήποτε συνθήκες. Επειδή η τέλεια ανίχνευση είναι – τουλάχιστον προς το παρόν – ανέφικτη, εξαιτίας των χρονικών καθυστερήσεων ή και των περιορισμών λόγω ακριβείας, κύρια πρόκληση αποτελεί η εύρεση της μεθόδου ανίχνευσης που είναι ιδανική για τη συγκεκριμένη κάθε φορά εφαρμογή [73]. Υπάρχει ένας ακόμη αριθμός προκλήσεων που συνδέονται με το πρόβλημα της ανίχνευσης και τοποθέτησης του εικονικού αντικειμένου εντός του φυσικού περιβάλλοντος [4], η κυριότερη από τις οποίες είναι οι αποκρύψεις (occlusion). Σύμφωνα με την τελευταία, πρέπει να επιτυγχάνεται η πλήρης ή μερική απόκρυψη του εικονικού αντικειμένου όταν κάποιο άλλο αντικείμενο του πραγματικού περιβάλλοντος τοποθετείται μπροστά από αυτό και το κρύβει, πλήρως ή μερικώς. Άλλη δυσκολία στις εφαρμογές επαυξημένης πραγματικότητας, σχετική με την οπτική ανίχνευση, είναι η μη εστιασμένη κάμερα στο marker ή στο πρότυπο που πρέπει να αναγνωριστεί για την επαύξηση της πραγματικότητας, γεγονός το οποίο μπορεί να οδηγήσει στη μη αναγνώρισή του, ή σε λάθη στην τοποθέτηση του εικονικού αντικειμένου, λόγω της χαμηλότερης ακρίβειας με την οποία αποδίδεται. Μία ακόμη πρόκληση, συγγενική με την οπτική ανίχνευση (visual tracking), είναι ο μη ομοιόμορφος φωτισμός, λόγω του οποίου ένα marker μπορεί να συσκοτιστεί σε κάποια τμήματά του και να μην αναγνωρίζεται από το πρόγραμμα ή να αναγνωρίζεται ως διαφορετικό marker. Όμοια, λόγω μεταβαλλόμενου φωτισμού, υπάρχει η πιθανότητα μη αναγνώρισης της εικόνας που έχει οριστεί ως πρότυπο. Τέλος, η θαμπάδα που μπορεί να προκληθεί λόγω γρήγορης κίνησης της κάμερας – κυρίως μίας κινητής συσκευής – είναι ένας ακόμα παράγοντας που δύναται να δυσκολέψει τη σωστή επαύξηση της πραγματικής σκηνής. Ένα κύριο στοιχείο των εφαρμογών επαυξημένης πραγματικότητας είναι η απεικόνιση του εικονικού αντικειμένου στην πραγματική σκηνή, δηλαδή η δημιουργία της συνθετικής επαυξημένης σκηνής, σε πραγματικό χρόνο (real-time rendering). Η φύση κάποιων εφαρμογών απαιτεί οι γραφικές πληροφορίες να ενσωματώνονται στο φυσικό περιβάλλον με τέτοιο τρόπο ώστε ο παρατηρητής να μην μπορεί να ξεχωρίσει ποιο είναι το πραγματικό και ποιο το εικονικό. Στις εφαρμογές αυτές, εκτός από το σωστό και ακριβές tracking και registration, απαιτείται ταυτόχρονα και φωτορεαλιστικό rendering, με σωστή σκίαση και φωτισμό του εικονικού αντικειμένου και οποιαδήποτε άλλη αυτόματη από το λογισμικό επεξεργασία πραγματικού χρόνου αυτό συνεπάγεται [6]. Καθοριστικό στοιχείο στη βελτίωση της ποιότητας του rendering αποτελεί η ικανότητα της εφαρμογής να λαμβάνει και να αξιοποιεί πληροφορία για το φωτισμό του περιβάλλοντος και την ανάκλαση [3]. Ένα ακόμη βασικό στοιχείο των εφαρμογών επαυξημένης πραγματικότητας είναι, όπως έχει ήδη αναφερθεί, η τεχνολογία θέασης, η οποία ταυτόχρονα αποτελεί και μία πρόκληση, καθώς η βελτίωσή της, ανάλογα με τις απαιτήσεις της εκάστοτε εφαρμογής, μπορεί να συντελέσει σε μεγαλύτερη αποδοχή της τεχνολογίας αυτής από το κοινό. Πράγματι, είναι πολύ πιο βολικό και «γνώριμο» στον άνθρωπο να φορέσει γυαλιά ή φακούς που θα επαυξήσουν την πραγματικότητά του σε σχέση με κάποια βαριά και μεγάλη συσκευή που προσαρτάται στο κεφάλι του (HMD ή HMPD). Εκτός από τέτοιου είδους περιορισμούς, που οφείλονται στον ανθρώπινο παράγοντα και για τους οποίους γίνονται σήμερα πολλές προσπάθειες βελτίωσης, υφίστανται και άλλες προκλήσεις σχετικές με την τεχνολογία θέασης της επαυξημένης πραγματικότητας [6], όπως είναι για παράδειγμα οι οπτικοί περιορισμοί λόγω του περιορισμένου οπτικού πεδίου του χρήστη, καθώς και οι τεχνικοί περιορισμοί, όπως η περιορισμένη ανάλυση και διάφοροι άλλοι παράγοντες. Εκτός των παραπάνω σημαντικών προκλήσεων με τις οποίες έρχονται αντιμέτωπες οι εφαρμογές επαυξημένης πραγματικότητας, υπάρχουν και άλλα τεχνικά ζητήματα σχετικά με αυτές, όπως είναι η αλληλεπίδραση του χρήστη με την εικονική πληροφορία, γεγονός το οποίο θα του δώσει την αίσθηση της πλήρους ενσωμάτωσης στο συνθετικό αυτό κόσμο, που συνδυάζει το πραγματικό με το εικονικό. Τέλος, οι δημιουργοί των εφαρμογών επαυξημένης πραγματικότητας πρέπει να δίνουν σημασία και στο πλήθος των εικονικών πληροφοριών που υπερτίθενται στο πραγματικό περιβάλλον του χρήστη, έτσι ώστε αυτό να μην εμποδίζει το χρήστη από τη θέαση του φυσικού κόσμου, αλλά ούτε και να είναι ανεπαρκές.
 
%krevelen

AR faces technical challenges regarding for example binocular
(stereo) view, high resolution, colour depth, luminance,
contrast, field of view, and focus depth. However,
before AR becomes accepted as part of user‟s everyday life,
just like mobile phones and personal digital assistants
(PDAs), issues regarding intuitive interfaces, costs, weight,
power usage, ergonomics, and appearance must also be addressed.
A number of limitations, some of which have been
mentioned earlier, are categorised here.


-Tracking and (auto)calibration
Tracking in unprepared environments remains a challenge
but hybrid approaches are becoming small enough to be added to mobile phones or PDAs. Calibration of these devices
is still complicated and extensive, but this may be
solved through calibration-free or auto-calibrating approaches
that minimise set-up requirements. The latter use
redundant sensor information to automatically measure and
compensate for changing calibration parameters [19].
Latency A large source of dynamic registration errors are
system delays [19]. Techniques like precalculation, temporal
stream matching (in video see-through such as live broadcasts),
and prediction of future viewpoints may solve some
delay. System latency can also be scheduled to reduce errors
through careful system design, and pre-rendered images may
be shifted at the last instant to compensate for pan-tilt motions.
Similarly, image warping may correct delays in 6DOF
motion (both translation and rotation).

-Depth perception
One difficult registration problem is accurate depth perception.
Stereoscopic displays help, but additional problems
including accommodation-vergence conflicts or low resolution
and dim displays cause object to appear further away
than they should be [52]. Correct occlusion ameliorates some
depth problems [138], as does consistent registration for
different eyepoint locations [158].
In early video see-through systems with a parallax, users
need to adapt to vertical displaced viewpoints. In an experiment
by Biocca and Rolland [35], subjects exhibit a large
overshoot in a depth-pointing task after removing the HMD.

-Overload and over-reliance
Aside from technical challenges, the user interface must
also follow some guidelines as not to overload the user with
information while also preventing the user to overly rely on
the AR system such that important cues from the environment
are missed [156]. At BMW, Bengler and Passaro [29] use
guidelines for AR system design in cars, including orientation
on the driving task, no moving or obstructing imagery,
add only information that improves driving performance,
avoid side effects like tunnel vision and cognitive capture,
and only use information that does not distract, intrude or
disturb given different situations.


%---
The diversity of AR platforms, devices, tools and applications is stunning. Overall,
augmented reality is a pronounced visualisation method, which is used in many
application areas. It is especially advantageous in on-site real-time visualisations
of database information and for purposes where there is a need to enhance the
3D perceptive skills of the user. Augmented reality enables natural interactions
and is a good tool to create interactive games and enhance user experience in
other areas as well. In this work, we aim to give a thorough overview of the whole
field, whilst concentrating on the fundamental issues of single-camera visual augmented
reality.


In conclusion, the augmented reality application developer needs to take into consideration several different issues: technical, application and other issues affecting the user experience. The main technological issues relate directly to the definition of augmented reality (real-time, interactive, 3D, combining real and virtual). Application issues arise from the ease of creating AR applications. Other important issues relate to user experience. The main technological issues in augmented reality are performance, interaction and alignment.

The main application issues are content creation and authoring. Other important issues affecting the user experience are visual perception user interface evices and power consumption. Next, we review what we mean by these issues and how they affect the usability and user experience of an AR application. An augmented reality system needs to be able to perform in real-time. Otherwise, the system may augment old or flawed information, or the augmentation may not correspond to the current state of the environment. Performance issues are characteristic to all AR algorithm and application development. Research results from other fields (e.g. image processing) are not directly applicable to AR. For instance, traditional image inpainting methods do not fulfil the real-time requirement, and therefore they cannot be used for diminished reality as such (see Section 6.2). Performance is an issue especially in mobile environment where the processing power and memory are limited. The user should be able to interact with the system naturally. The usability and the user experience are disturbed if the interaction is unnatural. The interaction needs to be natural in the user interface level as we discussed in the Section 7.1. The same holds true at the application level; the interaction between the real world objects and virtual objects needs to be smooth as well. Application needs to adapt virtual elements according to real scene, as for example in our interior design application where the user was able to adjust virtual lights easily according to real ones (see Section 6.1.3). At times, the application needs to remove existing objects virtually to be able to augment virtual objects on the same place. We discussed in Section 6.2 how to handle this kind of interaction with diminished reality. The camera calibration needs to be correct and the tracking needs to be accurate. Otherwise, the augmented data is shifted in the real environment: the virtual overlay is in the wrong place or it flutters. People find this alignment error annoying. In Chapter 3, we concentrated on marker-based approaches for accurate tracking, and in Chapter 4, on alternative tracking methods, mainly feature-based tracking and hybrid tracking methods. In addition, Appendix C gives an overview of camera calibration. The content creation is also an important aspect of application development. An application can visualise information from a database (e.g. in augmented assembly) or provide textual information (e.g. in AR browsers). Sometimes the information in database is in unsuitable format and format conversion is needed. In addition, when no database is available someone needs to create the content. Furthermore, if nice graphics are required, they need to be created to the approboth mobile environments and high quality visualisation. Besides content creation, authoring is a big application issue as we discussed in Section 7.4. Creation of AR applications should be brought to a non-expert nonprogramming level, where users can combine objects, interactions and events at a conceptual level. Visual perception should support the purpose of the application as we discussed in Chapter 6. Some applications require (photo-)realistic rendering, other applications benefit from focus and content -type highlighting of augmented objects. The user should be able to concentrate on the task, and the visual perception should sustain the task, without distracting the user. The user interface should be, as always, easy to use and intuitive. It should support the task at hand and make the user experience smooth as discussed in Section 7.1. The AR application should run on the appropriate device; mobile applications on lightweight devices, high-end visualisations on larger good-quality monitors. Furthermore, the terminal device should be taken into account already at the application design stage. There is no point in implementing computationally intensive methods on mobile phones if the application would then run on a slow frame rate. Devices often play very important role in the development process. The diversity of mobile platforms is perhaps the main obstacle for wider use of mobile AR applications. Applications need to be ported mostly to each platform separately, which deprives resources from application development. Furthermore, mobile devices are an ideal platform for consumer applications; they are equipped with cameras and new models with various additional sensors; people carry them with them all the time. Likewise, in special applications where an expert operates the system, it is feasible to invest in special devices such as HMDs, 3D displays, additional sensors, etc. if they support the task. One more aspect that significantly affects user experience is power consumption. Many applications require the user to be able to move freely, and thus wireless devices are optimal and then battery life plays a big role. A mobile application that discharges the battery in 15 minutes is unrealistic. We once tested a HMD where the camera ran out of batteries in less than two hours. The user had to change the batteries often, which was annoying especially as the camera and projector were wired to a computer anyway. It is hard to imagine this kind of setup in practical use, e.g. in a factory. In conclusion, the most important issue of augmented reality application development is the user experience, which is affected by all technological, application and other issues.
