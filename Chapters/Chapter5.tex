%*******10********20********30********40********50********60********70********80

% For all chapters, use the newdefined chap{} instead of chapter{}
% This will make the text at the top-left of the page be the same as the chapter

\chap{Μελλοντικές Επεκτάσεις}
%\section{Διαδραστική Επαυξημένη Πραγματικότητα}
%OTI ΔΕΝ ΕΙΠΑΜΕ ΣΤΗΝ ΕΙΣΑΓΩΓΗ ΓΙΑ AR, ΚΥΡΙΩΣ ΑΝΑΦΟΡΑ ΕΡΓΑΣΙΩΝ ΠΟΥ ΑΣΧΟΛΟΥΝΤΑΙ ΜΕ INTERACTION OF VIRTUAL OBJECTS, KAI MEΘΟΔΟΥΣ ΠΟΥ ΧΡΗΣΙΜΟΠΟΙΟΥΝΤΑΙ, ΓΙΑΤΙ ΕΙΝΑΙ ΧΡΗΣΙΜΗ Η ΑΛΛΗΛΕΠΙΔΡΑΣΗ ΣΤΟ ar


Συμπερασματικά, τα Κβαντικά Αποτυπώματα αποτελούν μια κλάση κβαντικών καταστάσεων με τεράστιο ενδιαφέρον τόσο στη θεωρητική όσο και στην πειραματική Κβαντική Πληροφορία. Επιπλέον, μετά τη από τη θεώρησή τους από τη σκοπιά της Κρυπτογραφίας, έγινε προφανές ότι τα Κβαντικά Αποτυπώματα παρουσιάζουν βασικές ιδιότητες απόκρυψης πληροφορίας και μπορούν να χρησιμοποιηθούν ποικιλοτρόπως στην κατασκευή κρυπτογραφικών πρωτοκόλλων. Ως καινοτόμο παράδειγμα προτείναμε την κατασκευή ενός σχήματος Κβαντικών Χρημάτων δημόσιου κλειδιού (public-key Quantum Money scheme) όπου τα Κβαντικά Αποτυπώματα χρησιμοποιούνται τόσο για την επαλήθευση του εκδότη του Κβαντικού χαρτονομίσματος μέσω του ψηφιακώς υπογεγραμμένου σειριακού αριθμού όσο και του κατόχου, δηλαδή της κβαντικής κατάστασης του χαρτονομίσματος. 

Ωστόσο, η πρότασή μας βρίσκεται ακόμη σε πρώιμο στάδιο και απαιτείται εντεταμένη θεωρητική εργασία για την εκτέλεση αποδείξεων ασφαλείας. Στην κατεύθυνση αυτή, πιστεύουμε ότι τα Αποκρύπτοντα Αποτυπώματα (hiding fingerprints) \cite{secrets} συσχετίζονται άμεσα με τους κρυμμένους υποχώρους (hidden subspaces) του Aaronson \cite{hidden}. Εάν αποδειχτεί αυτή η συσχέτιση, τότε οι αποδείξεις ασφαλείας και ορθότητας του σχήματος Κβαντικών Χρημάτων δημοσίου κλειδιού του Aaronson μπορούν να εφαρμοστούν και στο δικό μας σχήμα Κβαντικών Χρημάτων με Αποτυπώματα, καθιστώντας το έτσι απεριόριστα ασφαλές απέναντι σε κάθε γνωστή επίθεση εναντίων σχημάτων Κβαντικών Χρημάτων.

Βέβαια, τα εγγενή πλεονεκτήματα του σχήματος που προτείνουμε έναντι των υπόλοιπων προκύπτουν από τις πολλά υποσχόμενες πειραματικές υλοποιήσεις των Κβαντικών Αποτυπωμάτων με χρήση στοιχείων της Κβαντικής Οπτικής, όπως αναλύσαμε στο \cref{c:exper}. Ταυτόχρονα, όμως, πρέπει να εξεταστεί αν με την προτεινόμενη υλοποίηση μέσω σύμφωνων καταστάσεων (coherent states) παραμένουν ανέπαφες οι αποκρύπτουσες ιδιότητες των Κβαντικών Αποτυπωμάτων. Είμαστε αισιόδοξοι για μια θετική έκβαση, καθώς η θεωρία πίσω από τους τυχαίους ψευδο-γραμμικούς κώδικες των Gavinsky και Ito \cite{secrets} παρέχει απεριόριστη ασφάλεια ακόμη και αν ανακτηθεί από τον αντίπαλο ολόκληρη η κωδική λέξη π.χ. μετρώντας τη φάση κάθε παλμού σε ένα Οπτικό Κβαντικό Αποτύπωμα (βλ. \cref{c:exper}). Συνοψίζοντας, η εργασία αυτή αποτελεί μια προσπάθεια διεύρυνσης της ερευνητικής προσέγγισης για πειραματικώς υλοποιήσιμα πρωτόκολλα Κβαντικής Κρυπτογραφίας, που βασίζονται σε Κβαντικά Αποτυπώματα.



Με βάση τα αποτελέσματα της αξιολόγησης, οι μελλοντικές επεκτάσεις θα περιλαμβάνουν αρκετές βελτιώσεις στην πρωτότυπη έκδοση της εφαρμογής. Η βελτίωση της αναγνώρισης χειρονομιών για την αναγνώριση περισσότερων τύπων χειρονομιών. Use of machine learning techniques to get better results on the gesture recognition. Although the gesture recognition is based on the pinch positions, machine learning techniques could be implemented to infer gestures in time, defining a ground truth data set with movement’s directions specific for the Leap Motion, where the raw data is not exposed but if we rely on its data, machine learning techniques could improve effectively the gesture recognition. Improve the visual feedback to enhance the user’s perception of the virtual world, as the treatment of shadows and illumination in the AR. There are some implementations already carried out to deal with this feature.


Many prototypes implemented with sophisticated computer vision algorithms to robustly recognize hand gestures have demonstrated that gesture recognition is rather complex and compu- tationally expensive. That is why our methodology is better, since we dont use many resources and it is not computationally expensive. Based on the results obtained, future work may include several improvements to the current prototype and additional features to manage a seamless interaction in AR and an even more immersive experience for the users. The whole pinch gesture detection algorithm has been designed and developed, so that it would be possible to integrate the Realsense Development Kit Camera on top of an Oculus Rift Device. Our algorithms take into account the egocentric viewpoint of the user and his distance from a real table, thus they can work correctly without any changes. In order to achieve the integration of Oculus into our program, one would have to render whatever the application renders, including the video stream from the Realsense Camera, into the texture of a Framebuffer object of openGL. Afterwards, this texture could be applied in a quad and this quad could be placed in a virtual scene made by OpenGL. This virtual scene and the quad that is located inside it would then have to be also copied in the texture of a framebuffer object and passed into the Oculus SDK for modi cations, such as distortion and creation of a stereo view from different viewpoints per eye. In order to get more accurate and robust results on the exact 3D position of the pinch-point which is detected when thumb and fore nger come together a ltering of frames could be implemented to infer gestures in time and this could improve effectively the interaction errors. One could also improve the visual feedback to enhance the users perception of the virtual world. The use of shadows and light source estimation for correct illumination of the scene and virtual objects can greatly affect the realistic rendering of the virtual content in augmented reality. Furthermore, in order for the user to better perceive the depth of the scene, enhanced visualizations can be utilized such as projections in all planes of the scene. In our application we have already used one such visualization by rendering a straight line of projection from the moving chess piece to the chessboard and it was obvious that users can move virtual chess pieces and better understand where the new position of the object is going to be. Finally, since we use 3D models for the virtual chess pieces, a broad collection of different gures could be used, so that user would be able to play chess with different sets each time, ranging from dragons to soldiers. Adding multiple sets of chess models could de nitely improve user experience, but in order to further improve the visual experience attack and death animations could be implemented. Instead of pieces disappearing, destruction through an explosion or decapitation would be even better. Taking everything into consideration, our approach seems to work pretty nicely and the user is able to play a chess game with virtual objects from the beginning to the end without any serious problems.

\section{Ενίσχυση της Οπτικής Αντίληψης}
%des SILTANEN

Use of the tabletop as an AR gaming surface provides a number of interesting user interface opportunities for HMD-based systems. A clear option for AR gaming is the ability to make the game more animated. For example, the game ofWizard’s Chess depicted in the movie \textit{Harry Potter and the Philosopher’s Stone}(Warner Brothers Motion Pictures.) provides graphic animations of chess pieces fighting whenever a piece is captured. The use of AR allows for animated game pieces to interact with each other and with the players of the game. Animation could assist the following characteristics of the game: tuition on how to play the game, tactics, and current attributes for playing a piece (power level, strength, or ability).

An interesting feature of HMD-based AR is that different kinds of information may be displayed to different users. In the case of displaying current attributes, private information about a playing piece could be displayed to the user who controls the piece. This private information could include visualization of potential placement of pieces or future actions. Szalav´ ari et al. [1998] investigated these issues with an AR version of the classic Chinese game Mah-Jong. They employed face-mapping for quick and accurate placement of game pieces to improve the game experience. Because Mah-Jong requires both public and private information, they developed what they term a powerful automatic privacy mechanism. The players hold the private information on handheld PIPs (personal information panels). For this game, a PIP is a magnetic, tracked passive prop. The game is played at a table and AR displays all the game pieces. The AR for this game is the combination of virtual game pieces and the physical game space (people and table).
A tabletop AR version of the fantasy game Jumanji, set in Singapore, was developed by Zhou et al. [2004]. Instead of employing dangerous creatures, the game virtually transports the user to Singapore shopping locations. The user employs dice with MXRToolkit3 fiducialmarkers (similar to ARToolkitmarkers) as the means for game control. The use of physical dice adds a nice tangible feel to the game. Minatani et al. [2007] developed an AR version of Othello with the ARToolkit as the tracking technology. What makes this a very interesting game is the fact that you play a remote user with a physical board. Your opponent and the opponent’s pieces are displayed to each player as virtual objects. The authors explored the rendering space to enable users to “feel” as if the other player was sitting at the same table.


\subsection{Eye Tracking}
Using tiny cameras to observe user pupils and determine
the direction of their gaze is a technology with potential for
AR. The difficulties are that it needs be incorporated into the
eye-wear, calibrated to the user to filter out involuntary eye
movement, and positioned at a fixed distance. With enough
error correction, gaze tracking alternatives for the mouse
such as Stanford‟s EyePoint17 [94] provides a dynamic history
of user‟s interests and intentions that may help the UI
adapt to the future contexts.


\subsection{Depth perception}
One difficult registration problem is accurate depth perception.
Stereoscopic displays help, but additional problems
including accommodation-vergence conflicts or low resolution
and dim displays cause object to appear further away
than they should be [52]. Correct occlusion ameliorates some
depth problems [138], as does consistent registration for
different eyepoint locations [158].
In early video see-through systems with a parallax, users
need to adapt to vertical displaced viewpoints. In an experiment
by Biocca and Rolland [35], subjects exhibit a large
overshoot in a depth-pointing task after removing the HMD.
\subsection{Φωτορεαλιστική Απεικόνιση}

Το 2006, o Fischer et al. ανέπτυξαν ένα σύστημα ζωγραφικής επαυξημένης πραγματικότητας, το οποίο αλλάζει τα φυσικά και εικονικά αντικείμενα, με αποτέλεσμα να μπορούν να απεικονιστούν με 3 διαφορετικούς τρόπους: με τρόπο που θυμίζει γελοιογραφία που αποτελείται από χρωματιστές κηλίδες και σιλουέτες, με τρόπο που επιτρέπει τη χρήση μικρών πινελιών που προσομοιώνουν πουαντιγιστική τεχνοτροπία ζωγραφικής και, τέλος, ένα είδος ασπρόμαυρης τεχνικής απεικόνισης. Οι συγγραφείς ερεύνησαν έναν αριθμό διαφορετικών μεθόδων μη φωτορεαλιστικής απεικόνισης, με σκοπό να προσομοιώσουν διαφορετικές μορφές τέχνης και γελοιογραφιών [Fischer et al. 2005].


\subsection{Σκιές και Ανίχνευση Σύγκρουσης }


\subsection{Απτική Ανάδραση }

The haptic sense is divided into the kinaesthetic sense
(force, motion) and the tactile sense (tact, touch). Force
feedback devices like joysticks and steering wheels can
suggest impact or resistance and are well-known among
gamers. A popular 6DOF haptic device in teleoperation and
other areas is the PHANTOM (Fig. 11). It optionally provides
7DOF interaction through a pinch or scissors extension.
Tactile feedback devices convey parameters such as roughness,
rigidity, and temperature. Benali-Khoudja et al. [27]
survey tactile interfaces used in teleoperation, 3D surface
simulation, games, etc.
Data gloves use diverse technologies to sense and actuate
and are very reliable, flexible and widely used in VR for
gesture recognition. In AR however they are suitable only for
brief, casual use, as they impede the use of hands in real
world activities and are somewhat awkward looking for
general application. Buchmann et al. [37] connected buzzers
to the fingertips informing users whether they are „touching‟
a virtual object correctly for manipulation, much like the
CyberGlove with CyberTouch by SensAble15.

\section{Το μέλλον της Επαυξημένης Πραγματικότητας}

For Augmented Reality to become a mainstream tool, it must robustly provide useful information at rate that is synonymous with that of human sensory perception. The experimental results of this simple augmented interaction system provide evidence that real-time Augmented Reality is more than a theoretical vision. Using modern computer technology, it is clear that the first step towards the real-time computer perception of human behaviour can be taken. This can be as simple as the classification of basic human actions based on a pre-defined model or as complex as a continuous learning system able to mimic the communication performed by another human being. Many avenues are being explored in this field, all of which await the arrival of the required technology to process the observed information in real-time







There are some restrictions in AR. For example, a system is able to show the augmented view only from those viewpoints from where it has a real image. For example, the user can see a virtual building from ground level looking through a display, but is unable to see the scene from a bird's eye view. In order to provide such visualisations, applications often complement AR with virtual reality mode. Other limitations are due to restricted capacity of devices: their power consumption is too high and their processing, telecommunication and memory capacities are too low, the resolution of cameras is too low, etc. Engineers develop new and better devices, and the capacity of devices increases, they have more built-in sensors, therefore future devices will solve many of current obstacles. Cloud services will in turn help with computationally intensive tasks in future.

From the application developers’ point of view, the diversity of platforms is problematic: they need to port the application to different platforms, as a lot of the code is platform-dependent. HTML5 will be a step towards device-independent applications. It is supported by a large number of mobile devices. It enables presenting videos and audio on web pages as easily as presenting text and images is now, without a need for any plug-ins [271]. HTML5 offers a number of useful properties such as the user’s locations based on GPS or WLAN, canvas technology for dynamic graphics, etc. Although HTML5 is already available for application developers, the standard will be finished probably only on 2014 in W3C. Integration on global databases such as Google Earth, with GPS and local information for example from security cameras together with cloud computing, could lead to real applications similar to the one presented in [272], where the AR navigator is able to visualise cars coming out of the driver’s view (e.g. behind buildings). Another new type of application is security guard guidance system that is able to visualise people behind or inside buildings using AR as means for visualising information from security cameras. The usability of the see-through-devices needs to be improved before they are suitable for mass-market applications, especially the head-mounted see-through devices, which are clumsy as we discussed in Section 7.3. In future, the seethrough portable devices, such as the See-Through Laptop Samsung presented in 2010, might provide a better platform for mobile AR applications (see Figure 97).


Virtual retinal displays that render images with a laser directly on the user’s retina may become more common (see Section 8.2). Another alternative is the use of augmented reality contact lenses as for example envisioned in [274, 275] In future users may use devices intuitively by bending, twisting and squeezing, e.g. the Nokia Kinetic phone presented at Nokia World 2011. This kind of user interface works even if the user wears gloves in cold weather, where touchscreens are unpractical. The displays may be flexible and foldable, such as Polymer Vision’s Readius presented in 2008. Samsung has announced that their new mobile device line-up will feature flexible screens starting in 2012. The whole surface of a mobile device could be touchscreen as in Nokia’s GEM phone concept [276]. While waiting for reasonably sized foldable mobile device with interactive surface (and long-lasting batteries), people might find tablets to be a nice platform for AR applications: they have bigger screens than phones for visualisations, they are lighter than laptops, and they have built-in cameras and sensors. In the early days of virtual reality, Ivan Sutherland visioned “The ultimate display would, of course, be a room within which the computer can control the existence of matter. A chair displayed in such a room would be good enough to sit in. Handcuffs displayed in such a room would be confining, and a bullet displayed in such a room would be fatal.” [277]. It is hard to believe that researchers will ever build a virtual environment that will fulfil the last sentence. One of the exact benefits of virtual and augmented reality is that they enable safe environment for training and experiencing situations that would be too dangerous in reality. On the other hand, researchers have developed such physically altering environments that Sutherland visioned. Physically rendered environment is an environment where the system can alter the physical environment and manipulate grids of “moving physical pixels” (“moxels”). The system can raise and lower moxels and render physical shapes. It is a world-size version of the 3D pin art table where the user presses an object on the pins that will raise and create a 3D replica of the object’s shape. Today the quality of such systems is poor and their density is low. Furthermore, most current systems are able to model only vertical shapes, e.g. the Holodec presented in [278]. 3D holograms, furthermore, are able to visually render arbitrary 3D shapes, but without interaction possibilities. The University of Arizona presented a dynamic 3D hologram, which allows the three-dimensional projection, without the need for special eyewear. The resolution and speed (refreshes every two seconds) leave space for improvement, however [279–281]. In future we will probably see interactive large-scale 3D augmentations, virtual or physical. This will bring telepresence to a different level and enable new possibilities for mixed reality environments, e.g. telesurgery. Besides large 3D mixed environments, there is a trend towards mobile handheld AR with projective and 3D reconstruction capabilities.



%thomas cie
My experience with the limitations of current technology is similar to problems reported in the literature. The availability of affordable sensors with the required precision and accuracy has been and still is a real issue. There is no sense in developing gaming technology that is way beyond the price of current consumer-grade technology. I found that the collaboration between technologists and game design/artists creates successful games. As with any electronic gaming, this process is a fusion of the power of modern computing technological advances and creative graphics, storytelling, gameplay, and design. Technology is never going to outperform good gameplay. New technology will enable game designers to develop different and innovative gaming styles





%krevelen
AR has come a long way but still has some distance to go
before industries, the military and the general public will
accept it as a familiar user interface. For example, Airbus
CIMPA still struggles to get their AR systems for assembly
support accepted by the workers [163]. On the other hand,
companies like Information in Place estimated that by 2014,
30\% of mobile workers will be using augmented reality.
Within 5-10 years, Feiner [57] believes that “augmented
reality will have a more profound effect on the way in which
we develop and interact with future computers.” With the
advent of such complementary technologies as tactile networks,
artificial intelligence, cybernetics, and (non-invasive)
brain-computer interfaces, AR might soon pave the way for
ubiquitous (anytime-anywhere) computing [162] of a more
natural kind [13] or even human-machine symbiosis as
Licklider [99] already envisioned in the 1950‟s.




One performance goal for an augmented reality system is that the user can naturally interact with the virtual objects. This interaction should include not only moving the objects, but also feeling their surfaces and the forces applied to them by gravity and by other objects in the environment. Haptic relates to the sense of touch. The user of a haptic interface receives tactile feedback that adds the ability to feel objects. We can apply the work in these two areas for haptic interaction with the virtual objects but it does not provide insights into the problems of registration with the real scene or interactions between real and virtual objects. One of the reasons stated by Mine, Brooks, et. al. (1997) for the paucity of virtual-environment applications that have left the laboratory setting is the lack of haptic feedback. Brooks, Ouh-Young, et. al. (1990) describes one of the first examples of a haptic interface used in virtual reality. This system uses a large sized telemanipulation arm that is driven by motors to give force feedback to the user. Molecular docking is the application area addressed by the project. The user operates in an immersive environment experimenting with finding positions for bonding two molecules together. The forces of repulsion and attraction for a bonding operation are correctly simulated and applied to the manipulator. The molecular experts using this system find that this addition of haptic sensation greatly improves their ability to discover novel compound arrangements. Two reported medical applications of haptic interfaces in virtual environments are for medical training. Ziegler, Brandt, et. al. (1997) describe a simulator for arthroscopic surgery that uses force feedback in the virtual environment to train surgeons in the procedure. Using the Rutgers Master II force feedback device Dinsmore, Langrana, et. al. (1997) built a virtual environment for training physicians to locate and palpate tumor masses. The haptic device that is most suited for our augmented reality applications is the Phantom. Its applicability is due not only to its small size but also for the range of haptic feedback that is available. This tabletop device provides force feedback to the user’s fingertip. Because of its size it is well suited for workspaces that fall into the category of virtual-environment interaction that is “working within arm’s reach” (Mine, Brooks et al. 1997). The demonstrations of the work of State, Hirota et. al. (1996) show interaction with virtual objects. Correct visual interactions occur when virtual objects move behind a real object. Using the metaphor of a magnetic finger, the user is also able to attach a virtual object to his fingertip and move it within the workspace. This is all done within a framework of hybrid position tracking that requires knowledge of the 3D location of fiducial points in the scene. There is neither haptic feedback nor dynamic interactions between the virtual and real objects. Using the registration technique of Uenohara and Kanade (1995), Yokokohji, Hollis et. al. (1996) demonstrate a haptic interface for an augmented reality system. They use a Puma 560 robot for the force feedback and track a small plane attached to the end effector of the robot. The entire real scene is draped in blue cloth to allow for easy chroma-keying of the video image of the user’s arm and hand. The augmented display merges a virtual cube located where the small plane was detected and live video of the user’s arm and hand. There is neither motion of the virtual object nor interactions between virtual and real objects in their system.



%ziegler
Limitations and Challenges 
Even though tracking systems are accurate enough to achieve good results, the environments they work in are usually restricted not only to being indoors but also to being known in advance[ABB+01]. Dynamical adaption to unknown environments still poses a challenge. Complex scenes are challenging for real-time 3D tracking as is the motion of target objects[ZDB08]. Coping with rapid camera movements is difficult as resulting motion-blur hinders the re-observation of features. Rapid and unpredictable changes, that may occur in outdoor environments, constrain tracking results[HF04]. Especially illumination changes, which often and repeatedly occur outdoors, complicate the tracking process[ZDB08]. Basically all changes which cannot be controlled or anticipated are hard to handle. Some systems feature automatic reinitialisation, but the recovery of the camera pose, when the tracking has failed, cannot be achieved easily[ZDB08]. It is limited to applications which possess enough knowledge about the environment or which do not solely rely on vision-based tracking. 2.3.5.2 Trends Current research features many tracking approaches. Coping with unknown outdoor environments is an important topic. One way researchers are trying to achieve that is by further investigating hybrid approaches. As the growing number of publications during the past years indicate, Mobile AR becomes more and more popular among researches[SW07, WRM+08]. The growing computational resources of mobile devices present novel possibilities. The number of commercial applications from which users can choose continually rises. Among them are Layar9, Nokia Point \& Find10, Twitter AR11 and Virus Killer 36012. Building a reference presentation of the environment while tracking is a popular trend, research focusing especially on Simultaneous Localisation and Mapping (SLAM)[CGKM07, DRMS07, KM09]. Such systems usually require a high amount of computational resources. However, through certain restrictions, SLAM works on a mobile phone, too, as has recently been shown by the work of Klein and Murray[KM09]. Instead of using as many features as possible and hoping that some of the chosen features provide robust tracking, researchers try to find methods to detect only suitable and useful features in the first place[ZDB08, ST94]. Researchers try to find ways of making initialisation processes automatic[SKSK07]. Focusing on model-based tracking is popular as well[FL07, WS07]. Last but not least, ubiquitous tracking, that is tracking acquired by forming a dense network of sensors that enables tracking everywhere, seems to be achievable in the near future[HPK+07].


%--
Μία από τις μεγαλύτερες προκλήσεις με τις οποίες έρχονται αντιμέτωποι οι δημιουργοί εφαρμογών επαυξημένης πραγματικότητας είναι η σωστή τοποθέτηση του εικονικού αντικειμένου εντός του πραγματικού περιβάλλοντος, έτσι ώστε η συνθετική πληροφορία να δίνει την εντύπωση ότι ανήκει σε αυτό. Η διαδικασία αυτή είναι γνωστή ως registration (γεωαναφορά). Η σωστή συγχώνευση και ευθυγράμμιση των δύο κόσμων – του φυσικού και του παραγόμενου από υπολογιστή – είναι κύρια προϋπόθεση για την εκπλήρωση του στόχου των εφαρμογών επαυξημένης πραγματικότητας, ενώ λάθη ή ανακρίβειες στην τοποθέτηση του εικονικού αντικειμένου θα έχουν ως αποτέλεσμα να χαθεί η ψευδαίσθηση ότι οι δύο κόσμοι συνυπάρχουν [3]. Πολλές εφαρμογές, μάλιστα, όπως για παράδειγμα στην ιατρική, απαιτούν ακριβή γεωαναφορά και δεν είναι επιτρεπτά λάθη και αστοχίες. Για τη σωστή γεωαναφορά, απαραίτητη προϋπόθεση είναι η πρότερη ανίχνευση της θέσης και του προσανατολισμού της κάμερας – και γενικά της συσκευής μέσω της οποίας επαυξάνεται η πραγματικότητα – ή του κεφαλιού του χρήστη (π.χ. σε εφαρμογή με HMD). Η διαδικασία αυτή είναι γνωστή ως tracking (ανίχνευση), απαντά στα ερωτήματα: πού βρίσκεται ο χρήστης, πού εστιάζεται το ενδιαφέρον του και πού πρέπει να παρουσιαστεί το εικονικό αντικείμενο [72] και συνεπώς η σωστή και ακριβής, ανάλογα με την εφαρμογή διεκπεραίωσή της είναι κρίσιμη για τη δημιουργία πειστικών εφαρμογών επαυξημένης πραγματικότητας. Για την επίτευξη των τελευταίων, η ανίχνευση πρέπει πρακτικά να διεξάγεται σε πραγματικό χρόνο, δηλαδή η εκτίμηση της θέσης να γίνεται σε χιλιοστά του δευτερολέπτου, καθώς επίσης και να είναι εύρωστη, δηλαδή να δίνει ικανοποιητικά αποτελέσματα κάτω από ποικίλες συνθήκες, όπως για παράδειγμα σε μεταβαλλόμενο φωτισμό [68]. Υπάρχουν πολλές μέθοδοι ανίχνευσης 6 βαθμών ελευθερίας (6DOF tracking), όπως το μηχανικό tracking, τεχνική που υιοθετήθηκε και από το πρώτο σύστημα επαυξημένης πραγματικότητας του Sutherland, το υπερηχητικό tracking και το οπτικό tracking. Υπάρχουν και άλλες που υπολογίζουν μόνο θέση ή προσανατολισμό, όπως για παράδειγμα το tracking που βασίζεται σε πληροφορίες μόνο από GPS ή μόνο από γυροσκόπια [15]. Για εφαρμογές που απαιτούν ακριβή γεωαναφορά, ωστόσο, απαιτείται ακριβές στιγμιαίο 6DOF tracking υπό οποιεσδήποτε συνθήκες. Επειδή η τέλεια ανίχνευση είναι – τουλάχιστον προς το παρόν – ανέφικτη, εξαιτίας των χρονικών καθυστερήσεων ή και των περιορισμών λόγω ακριβείας, κύρια πρόκληση αποτελεί η εύρεση της μεθόδου ανίχνευσης που είναι ιδανική για τη συγκεκριμένη κάθε φορά εφαρμογή [73]. Υπάρχει ένας ακόμη αριθμός προκλήσεων που συνδέονται με το πρόβλημα της ανίχνευσης και τοποθέτησης του εικονικού αντικειμένου εντός του φυσικού περιβάλλοντος [4], η κυριότερη από τις οποίες είναι οι αποκρύψεις (occlusion). Σύμφωνα με την τελευταία, πρέπει να επιτυγχάνεται η πλήρης ή μερική απόκρυψη του εικονικού αντικειμένου όταν κάποιο άλλο αντικείμενο του πραγματικού περιβάλλοντος τοποθετείται μπροστά από αυτό και το κρύβει, πλήρως ή μερικώς. Άλλη δυσκολία στις εφαρμογές επαυξημένης πραγματικότητας, σχετική με την οπτική ανίχνευση, είναι η μη εστιασμένη κάμερα στο marker ή στο πρότυπο που πρέπει να αναγνωριστεί για την επαύξηση της πραγματικότητας, γεγονός το οποίο μπορεί να οδηγήσει στη μη αναγνώρισή του, ή σε λάθη στην τοποθέτηση του εικονικού αντικειμένου, λόγω της χαμηλότερης ακρίβειας με την οποία αποδίδεται. Μία ακόμη πρόκληση, συγγενική με την οπτική ανίχνευση (visual tracking), είναι ο μη ομοιόμορφος φωτισμός, λόγω του οποίου ένα marker μπορεί να συσκοτιστεί σε κάποια τμήματά του και να μην αναγνωρίζεται από το πρόγραμμα ή να αναγνωρίζεται ως διαφορετικό marker. Όμοια, λόγω μεταβαλλόμενου φωτισμού, υπάρχει η πιθανότητα μη αναγνώρισης της εικόνας που έχει οριστεί ως πρότυπο. Τέλος, η θαμπάδα που μπορεί να προκληθεί λόγω γρήγορης κίνησης της κάμερας – κυρίως μίας κινητής συσκευής – είναι ένας ακόμα παράγοντας που δύναται να δυσκολέψει τη σωστή επαύξηση της πραγματικής σκηνής. Ένα κύριο στοιχείο των εφαρμογών επαυξημένης πραγματικότητας είναι η απεικόνιση του εικονικού αντικειμένου στην πραγματική σκηνή, δηλαδή η δημιουργία της συνθετικής επαυξημένης σκηνής, σε πραγματικό χρόνο (real-time rendering). Η φύση κάποιων εφαρμογών απαιτεί οι γραφικές πληροφορίες να ενσωματώνονται στο φυσικό περιβάλλον με τέτοιο τρόπο ώστε ο παρατηρητής να μην μπορεί να ξεχωρίσει ποιο είναι το πραγματικό και ποιο το εικονικό. Στις εφαρμογές αυτές, εκτός από το σωστό και ακριβές tracking και registration, απαιτείται ταυτόχρονα και φωτορεαλιστικό rendering, με σωστή σκίαση και φωτισμό του εικονικού αντικειμένου και οποιαδήποτε άλλη αυτόματη από το λογισμικό επεξεργασία πραγματικού χρόνου αυτό συνεπάγεται [6]. Καθοριστικό στοιχείο στη βελτίωση της ποιότητας του rendering αποτελεί η ικανότητα της εφαρμογής να λαμβάνει και να αξιοποιεί πληροφορία για το φωτισμό του περιβάλλοντος και την ανάκλαση [3]. Ένα ακόμη βασικό στοιχείο των εφαρμογών επαυξημένης πραγματικότητας είναι, όπως έχει ήδη αναφερθεί, η τεχνολογία θέασης, η οποία ταυτόχρονα αποτελεί και μία πρόκληση, καθώς η βελτίωσή της, ανάλογα με τις απαιτήσεις της εκάστοτε εφαρμογής, μπορεί να συντελέσει σε μεγαλύτερη αποδοχή της τεχνολογίας αυτής από το κοινό. Πράγματι, είναι πολύ πιο βολικό και «γνώριμο» στον άνθρωπο να φορέσει γυαλιά ή φακούς που θα επαυξήσουν την πραγματικότητά του σε σχέση με κάποια βαριά και μεγάλη συσκευή που προσαρτάται στο κεφάλι του (HMD ή HMPD). Εκτός από τέτοιου είδους περιορισμούς, που οφείλονται στον ανθρώπινο παράγοντα και για τους οποίους γίνονται σήμερα πολλές προσπάθειες βελτίωσης, υφίστανται και άλλες προκλήσεις σχετικές με την τεχνολογία θέασης της επαυξημένης πραγματικότητας [6], όπως είναι για παράδειγμα οι οπτικοί περιορισμοί λόγω του περιορισμένου οπτικού πεδίου του χρήστη, καθώς και οι τεχνικοί περιορισμοί, όπως η περιορισμένη ανάλυση και διάφοροι άλλοι παράγοντες. Εκτός των παραπάνω σημαντικών προκλήσεων με τις οποίες έρχονται αντιμέτωπες οι εφαρμογές επαυξημένης πραγματικότητας, υπάρχουν και άλλα τεχνικά ζητήματα σχετικά με αυτές, όπως είναι η αλληλεπίδραση του χρήστη με την εικονική πληροφορία, γεγονός το οποίο θα του δώσει την αίσθηση της πλήρους ενσωμάτωσης στο συνθετικό αυτό κόσμο, που συνδυάζει το πραγματικό με το εικονικό. Τέλος, οι δημιουργοί των εφαρμογών επαυξημένης πραγματικότητας πρέπει να δίνουν σημασία και στο πλήθος των εικονικών πληροφοριών που υπερτίθενται στο πραγματικό περιβάλλον του χρήστη, έτσι ώστε αυτό να μην εμποδίζει το χρήστη από τη θέαση του φυσικού κόσμου, αλλά ούτε και να είναι ανεπαρκές.
 



%---
The diversity of AR platforms, devices, tools and applications is stunning. Overall, augmented reality is a pronounced visualisation method, which is used in many application areas. It is especially advantageous in on-site real-time visualisations of database information and for purposes where there is a need to enhance the 3D perceptive skills of the user. Augmented reality enables natural interactions and is a good tool to create interactive games and enhance user experience in other areas as well. In this work, we aim to give a thorough overview of the whole field, whilst concentrating on the fundamental issues of single-camera visual augmented reality.


In conclusion, the augmented reality application developer needs to take into consideration several different issues: technical, application and other issues affecting the user experience. The main technological issues relate directly to the definition of augmented reality (real-time, interactive, 3D, combining real and virtual). Application issues arise from the ease of creating AR applications. Other important issues relate to user experience. The main technological issues in augmented reality are performance, interaction and alignment.

The main application issues are content creation and authoring. Other important issues affecting the user experience are visual perception user interface evices and power consumption. Next, we review what we mean by these issues and how they affect the usability and user experience of an AR application. An augmented reality system needs to be able to perform in real-time. Otherwise, the system may augment old or flawed information, or the augmentation may not correspond to the current state of the environment. Performance issues are characteristic to all AR algorithm and application development. Research results from other fields (e.g. image processing) are not directly applicable to AR. For instance, traditional image inpainting methods do not fulfil the real-time requirement, and therefore they cannot be used for diminished reality as such (see Section 6.2). Performance is an issue especially in mobile environment where the processing power and memory are limited. The user should be able to interact with the system naturally. The usability and the user experience are disturbed if the interaction is unnatural. The interaction needs to be natural in the user interface level as we discussed in the Section 7.1. The same holds true at the application level; the interaction between the real world objects and virtual objects needs to be smooth as well. Application needs to adapt virtual elements according to real scene, as for example in our interior design application where the user was able to adjust virtual lights easily according to real ones (see Section 6.1.3). At times, the application needs to remove existing objects virtually to be able to augment virtual objects on the same place. We discussed in Section 6.2 how to handle this kind of interaction with diminished reality. The camera calibration needs to be correct and the tracking needs to be accurate. Otherwise, the augmented data is shifted in the real environment: the virtual overlay is in the wrong place or it flutters. People find this alignment error annoying. In Chapter 3, we concentrated on marker-based approaches for accurate tracking, and in Chapter 4, on alternative tracking methods, mainly feature-based tracking and hybrid tracking methods. In addition, Appendix C gives an overview of camera calibration. The content creation is also an important aspect of application development. An application can visualise information from a database (e.g. in augmented assembly) or provide textual information (e.g. in AR browsers). Sometimes the information in database is in unsuitable format and format conversion is needed. In addition, when no database is available someone needs to create the content. Furthermore, if nice graphics are required, they need to be created to the approboth mobile environments and high quality visualisation. Besides content creation, authoring is a big application issue as we discussed in Section 7.4. Creation of AR applications should be brought to a non-expert nonprogramming level, where users can combine objects, interactions and events at a conceptual level. Visual perception should support the purpose of the application as we discussed in Chapter 6. Some applications require (photo-)realistic rendering, other applications benefit from focus and content -type highlighting of augmented objects. The user should be able to concentrate on the task, and the visual perception should sustain the task, without distracting the user. The user interface should be, as always, easy to use and intuitive. It should support the task at hand and make the user experience smooth as discussed in Section 7.1. The AR application should run on the appropriate device; mobile applications on lightweight devices, high-end visualisations on larger good-quality monitors. Furthermore, the terminal device should be taken into account already at the application design stage. There is no point in implementing computationally intensive methods on mobile phones if the application would then run on a slow frame rate. Devices often play very important role in the development process. The diversity of mobile platforms is perhaps the main obstacle for wider use of mobile AR applications. Applications need to be ported mostly to each platform separately, which deprives resources from application development. Furthermore, mobile devices are an ideal platform for consumer applications; they are equipped with cameras and new models with various additional sensors; people carry them with them all the time. Likewise, in special applications where an expert operates the system, it is feasible to invest in special devices such as HMDs, 3D displays, additional sensors, etc. if they support the task. One more aspect that significantly affects user experience is power consumption. Many applications require the user to be able to move freely, and thus wireless devices are optimal and then battery life plays a big role. A mobile application that discharges the battery in 15 minutes is unrealistic. We once tested a HMD where the camera ran out of batteries in less than two hours. The user had to change the batteries often, which was annoying especially as the camera and projector were wired to a computer anyway. It is hard to imagine this kind of setup in practical use, e.g. in a factory. In conclusion, the most important issue of augmented reality application development is the user experience, which is affected by all technological, application and other issues.



%krevelen
Imagine a technology with which you could see more than
others see, hear more than others hear, and perhaps even
touch, smell and taste things that others can not. What if we
had technology to perceive completely computational elements
and objects within our real world experience, entire
creatures and structures even that help us in our daily activities, while interacting almost unconsciously through mere
gestures and speech?

Augmented reality (AR) is this technology to create a
“next generation, reality-based interface” [77] and is moving
from laboratories around the world into various industries
and consumer markets. AR supplements the real world with
virtual (computer-generated) objects that appear to coexist in
the same space as the real world. AR was recognised as an
emerging technology of 2007 [79], and with today‟s smart
phones and AR browsers we are starting to embrace this very
new and exciting kind of human-computer interaction
