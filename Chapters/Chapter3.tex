3%*******10********20********30********40********50********60********70********80

% For all chapters, use the newdefined chap{} instead of chapter{}
% This will make the text at the top-left of the page be the same as the chapter

\chap{Σχεδιασμός \& Υλοποίηση Εφαρμογής} \label{c:crypto}



Στο κεφάλαιο 2, διερευνήσαμε το πεδίο της επαυξημένης πραγματικότητας και τις μαθηματικές αρχές από τις οποίες διέπεται. Παρουσιάσαμε επίσης τη χρησιμότητα της ενσωμάτωσης δυνατοτήτων αναγνώρισης χειρονομιών σε εφαρμογές, καθώς και ερευνητικές εργασίες σχετικές με τη διπλωματική αυτή εργασία. 
Όπως αναφέρθηκε, πολλές εφαρμογές επαυξημένης πραγματικότητας έχουν αναπτυχθεί στον τομέα των βιντεοπαιχνιδιών. 

The suitable architecture and frameworks presented previously, chapter 3 are implemented in this section, starting with a summary of the development tools and equipment required. At first, we show the implementation of the basic scenario in the Unity engine, after we present the
description of the evaluation performed with two tasks and the experimental set-up used.



Στις παρακάτω ενότητες θα πραγματοποιηθεί ανάλυση των μεθόδων που αναπτύχθηκαν για την δημιουργία ενός σκακιού επαυξημένης πραγματικότητας, όπου ο χρήστης μπορεί να χειριστεί εικονικά πιόνια μόνο με τα χέρια του με χειρονομίες "τσιμπήματος". Αρχικά θα παρουσιαστούν τα εργαλεία και ο αισθητήρας που χρησιμοποιήθηκαν, καθώς και η πειραματική εγκατάσταση που χρησιμοποιήθηκε. Τα επόμενα στάδια περιλαμβάνουν την ανάλυση της διαδικασίας βαθμονόμησης και την παρουσίαση των μεθόδων ανίχνευσης της θέσης του τσιμπήματος και χειρισμού των εικονικών αντικειμένων. %..............++

Πριν από την παρουσίαση της υλοποίησης της εφαρμογής, παρουσιάζονται τα εργαλεία ανάπτυξης και οι συσκευές που χρησιμοποιήθηκαν.


%eftekhari
Η ανάπτυξη και η υλοποίηση ενός συστήματος επαυξημένης πραγματικότητας προϋποθέτει από το σχεδιαστή να πάρει αποφάσεις σχετικά με τις επιλογές για την υλοποίηση, κάτι που μπορεί να επιφέρει προδιάθεση στην ανθρώπινη απόδοση. Οι επιλογές τόσο σε software όσο και σε  hardware πρέπει να εξεταστούν πριν ξεκινήσει η διαδικασία της ανάπτυξης. Οι ανθρώπινοι παράγοντες που συζητήθηκαν προηγουμένως και σχετίζονται με προσεγγίσεις σε software και hardwareείναι κλειδί για τη σωστή προσέγγιση. Ορισμένες επιλογές που παίζουν σημαντικό ρόλο στο σχεδιασμό έχουν να κάνουν με τη μέθοδο ανίχνευσης, το είδος των marker που θα χρησιμοποιηθούν, τον τύπο HMD ή monitor, και το λογισμικό που θα υλοποιηθεί. Άλλοι περιορισμοί που πρέπει να ληφθούν υπόψη αφορούν το κόστος και τη δυσκολίας ενσωμάτωσης.


Επειδή στους αλγόριθμους που αναπτύχθηκαν και θα παρουσιαστούν στη συνέχεια γίνεται έντονη χρήση nested loops, διαφόρων κατωφλίων και συνθηκών, επιλέχθηκε η παρουσίασή τους να μη γίνει με τη μορφή κειμένου, αλλά να παρουσιάστει με τη μορφή διαγραμμάτων ροής.




\section{Η Συσκευή Intel\textregistered\ RealSense\texttrademark{} 3D F200 }
%ΒΑΛΕ ΕΙΚΟΝΕΣ ΤΗΣ ΣΥΣΚΕΥΗΣ


Η πλατφόρμα Intel\textregistered\ RealSense\texttrademark{}, παλαιότερα γνωστή ως Intel\textregistered\ Perceptual Computing, είναι μία πλατφόρμα που επιτρέπει την υλοποίηση τεχνικών αλληλεπίδρασης ανθρώπου-υπολογιστή με βάση τις χειρονομίες.Αποτελείται από μία σειρά 3D αισθητήρων και μία βιβλιοθήκη αντίληψης μηχανής που απλοποιεί τη χρήση των αισθητήρων από προγραμματιστές λογισμικού. \cite{RealsenseCamera}

Η συγκεκριμένη τεχνολογία θεωρείται διάδοχος της τεχνολογίας του αισθητήρα Microsoft Kinect, με κύριο στόχο τη δημιουργία εφαρμογών για τεχνολογίες της αγοράς πέρα από τα βιντεοπαιχνίδια.

Από το Μάρτιο του 2015, πολλοί κατασκευαστές φορητών υπολογιστών και tablets\cite{Realsenselaptops}, όπως οι Asus, HP, Dell, Lenovo, and Acer διαθέτουν συσκευές με ενσωματωμένο τον αισθητήρα Intel\textregistered\ RealSense\texttrademark{}. 

Ένας τέτοιος αισθητήραςπεριλαμβάνει τα παρακάτω 4 εξαρτήματα: 


\begin{itemize}
  \item 1 συμβατική κάμερα
  \item 1 προβολέα υπερύθρων ακτίνων laser (infrared laser projector)
  \item 1 κάμερα υπερύθρων
  \item 2 μικρόφωνα
\end{itemize}



Ο προβολέας υπερύθρων ακτίνων laser προβάλλει ένα πλέγμα στη σκηνή ( σε υπέρυθρο φως που είναι αόρατο στο ανθρώπινο μάτι) και η κάμερα υπερύθρων το καταγράφει με στόχο να υπολογίσει πληροφορίες για το βάθος της σκηνής.
Τα μικρόφωνα επιτρέπουν τον εντοπισμό πηγών ήχου στο χώρο και την ακύρωση θορύβου παρασκηνίου.


Ανακοινώθηκαν 3 διαφορετικά μοντέλα αισθητήρων, με συγκεκριμένες ιδιότητες και προβλεπόμενη χρήση. 

\begin{description}
  \item[Intel\textregistered\ RealSense\texttrademark{} 3D Camera (Front F200)] \hfill \\
  Πρόκειται για έναν αισθητήρα που μπορεί να συνδεθεί με φορητούς ή σταθερούς υπολογιστές και προορίζεται για χρήσεις όπως η αλληλεπίδραση με φυσικές χειρονομίες, η αναγνώριση προσώπου, οι τηλεδιασκέψεις, η 3D σάρωση και το gaming[8]
  
  \item[Intel\textregistered\ RealSense\texttrademark{} Snapshot] \hfill \\
 Ο αισθηρήρας Snapshot προορίζεται για χρήση μέσω tablets και πιθανότατα smartphones. Η χρήση του περιλαμβάνει λήψη φωτογραφιών και ιδιότητες όπως επανεστίαση, υπολογισμοί αποστάσεων και φίλτρα κίνησης. 


  \item[Intel\textregistered\ RealSense\texttrademark{} 3D Camera (Rear R200)] \hfill \\
  Το τρίτο είδος αισθητήρα της Intel προορίζεται για προσαρμογή στο πίσω μέρος συσκευών όπως το Microsoft Surface ή παρόμοιων tablets. Δεν είναι ακόμα διαθέσιμο στην αγορά, ωστόσο προορίζεται για εφαρμογές επαυξημένης πραγματικότητας, δημιουργία περιεχομένου και σάρωση αντικειμένων.
\end{description}
[edit]



Στη συγκεκριμένη εργασία χρησιμοποιήσαμε το πρώτο είδος αισθητήρα, Intel\textregistered\ RealSense\texttrademark{} 3D F200 το οποίο διατίθεται από την Intel μέσω ενός Development Kit στην τιμή των 99 δολλαρίων. Διαθέτει ανάλυση βάθους Full VGA, κάμερα RGB 1080p, εύρος περίπου 0.2–1.2 μέτρα και συνδέεται μέσω USB 3.0.


Ο αισθητήρας βάθους μέσω υπερύθρων προσφέρει καλύτερη αναγνώριση χειρονομιών από μία παραδοσιακή κάμερα. Μέσω του αισθητήρα της Intel, μπορούμε να προσθέσουμε δυνατότητες αναγνώρισης χειρονομιών σε ήδη υπάρχοντα συστήματα βιντεοπαιχνιδιών.
ενώ μας δίνεται η δυνατότητας να αξιοποιήσουμε τα χέρια μας ως χειριστήρια.

Η συσκευή RealSense 3D επιλέχθηκε για τις δυνατότητές της σε σχέση με άλλους αισθητήρες όπως το Microsoft Kinect, λόγω του μικρού μεγέθους και της δυνατότητας ανίχνευσης βάθους σε κοντινότερες αποστάσεις. Επίσης προσφέρει εύκολο χειρισμό των ακατέργαστων δεδομένων (raw data), καθώς επίσης μέσω του SDK προσφέρονται αλγόριθμοι για την εξαγωγή blobs, το φιλτράρισμα και τον καθορισμό παραμέτρων. 



\begin{figure}[H]
    \centering
    \includegraphics[scale=0.3, angle=0]{Files/Figures/RealSenseCamera.jpg}
    \caption[Η Συσκευή Intel\textregistered\ RealSense\texttrademark{} 3D F200]{Η Συσκευή Intel\textregistered\ RealSense\texttrademark{} 3D F200}
    \label{fig:realsense}
\end{figure}



\section{Επιλογή Εργαλείων Ανάπτυξης}
%ΠΕΣ ΓΙΑ ARUCO,OPENCV,KLP


Πριν το σχεδιασμό μιας εφαρμογής επαυξημένης πραγματικότητας, η διαδικασία επιλογής των κατάλληλων βιβλιοθηκών και εργαλείων λογισμικού που θα χρησιμοποιηθούν, αποτελεί μία σημαντική πτυχή για την πετυχημένη υλοποίηση της εφαρμογής. 

Πέρα από την κατάλληλη επιλογή αισθητήρα χρώματος - βάθους,
έπρεπε να επιλεγούν εργαλεία τα οποία θα ήταν συμβατά με τον αισθητήρα και τις απαιτήσεις του. Πιο συγκεκριμένα, για τη σωστή λειτουργία του αισθητήρα απαιτείται υπολογιστική μονάδα με επεξεργαστή Intel\textregistered\ Core\texttrademark{} τουλάχιστον 4ης γενιάς και λειτουργικό Windows 8.1 (64bit). Επιπλέον η υπολογιστική μονάδα πρέπει να διαθέτει θύρες USB 3.0 για τη σύνδεση με τον αισθητήρα, ενώ σαν γλώσσες προγραμματισμού υποστηρίζονται οι C++, JavaScript, C\#, Java και Processing 2.1.2 ή πιο πρόσφατη.


Για τους παραπάνω λόγους, η υλοποίηση της εφαρμογής έλαβε μέρος σε ένα φορητό υπολογιστή  MacBook Pro με επεξεργαστή Intel\textregistered\ Core\texttrademark{} i5 4278U (2.6GHz) με μνήμη RAM στα 16GB και λειτουργικό σύστημα Windows 8.1 Professional (64bit). Επίσης λόγω της πολυπλοκότητας υλοποίησης απαιτείται η χρήση πολλών διαφορετικών βιβλιοθηκών και η αξιοποίηση των χαρακτηριστικών τους. Για τον ευκολοτερο συνδυασμό των βιβλιοθηκών, χρησιμοποίηθηκε ως IDE το Visual Studio 2010.


Μεταξύ μιας ποικιλίας βιβλιοθηκών επαυξημένης πραγματικότητας, επιλέχθηκε η βιβλιοθήκη ArUco λόγω της απλότητάς της (ανίχνευση markers με μία μόνο γραμμή κώδικα C++) και της δυνατότητας δημιουργίας markerboards που επιτρέπουν την απρόσκοπτη ανίχνευσή τους. Η έκδοση που χρησιμοποιήθηκε (1.2.5) διαθέτει υποστήριξη για διαφορετικές πλατφόρμες (Java, C++, Python) και είναι καλά τεκμηριωμένη (documentation). Επιπλέον διαθέτει ελεύθερη άδεια προς χρήση σε σύγκριση με εργαλεία όπως αυτά που προσφέρουν άλλες εταιρίες όπως η Metaio. Το γεγονός ότι μπορεί εύκολα να συνδυαστεί με την OpenCV και την OpenGL είναι αυτό που συνετέλεσε κυρίως στην επιλογή της.


Οι βιβλιοθήκες λογισμικού που αξιοποιήθηκαν εμφανίζονται παρακάτω:

\begin{description}


\item[OpenGL \& GLUT:] Πρόκειται για μία διαγλωσσική διεπαφή προγραμματισμού εφαρμογών που υποστηρίζει πολλές πλατφόρμες για την απεικόνιση 2D και 3D γραφικών. Χρησιμοποείται στην εφαρμογή μας για την απεικόνιση των εικονικών αντικειμένων πάνω στο βίντεο το οποίο καταγράφει την πραγματική σκηνή. Παρά το γεγονός ότι θα μπορούσε να χρησιμοποιηθεί μία μηχανή βιντεοπαιχνιδιών όπως η Unity3D για την ευκολότερη διαχείριση των ιδιοτήτων των εικονικών αντικειμένων, επιλέξαμε τη χρήση της OpenGL, προκειμένου να κατανοηθούν οι βασικές τεχνικές που χρησιμοποιούνται στην ανάπτυξη εφαρμογών επαυξημένης πραγματικότητας.



\item[OpenCV (2.4.10):] Γνωστή βιβλιοθήκη προγραμματιστικών συναρτήσεων που έχουν ως στόχο την ανάπτυξη εφαρμογών υπολογιστικής όρασης και την επεξεργασία εικόνας και βίντεο. Η συγκεκριμένη βιβλιοθήκη ανοιχτού κώδικα σχεδιάστηκε για να είναι αποδοτική ώστε να υποστηρίζει εφαρμογές πραγματικού χρόνου, ενώ υποστηρίζει διάφορες πλατφόρμες όπως Windows, Linux, Mac OS X, iOS και Android και διαθέτει διεπαφές για τις γλώσσες C,C++ και Java. 


\item[Qt (5.4)]: Πρόκειται για ένα framrwork ανάπτυξης που υποστηρίζει πολλές πλατφόρμες για την δημιουργία εφαρμογών, διεπαφών χρηστών (UI) και συσκευών. Για την ανάπτυξη της εφαρμογής σκακιού επαυξημένης πραγματικότητας, χρησιμοποιήθηκαν μόνο οι ιδιότητες της κλάσης \textbf{QProcess} που επιτρέπει την επικοινωνία με εκτελέσιμα αρχεία.


\item[Realsense SDK:] Το συγκεκριμένο SDK παρέχει πρόσβαση στα δεδομένα των αισθητήρων χρώματος και βάθους της κάμερας, καθώς επίσης έτοιμους αλγορίθμους για την εξαγωγή blob και περιγραμμάτων (contours), τον εντοπισμό χεριών και την αναγνώριση ομιλίας. 



\end{description}



\section{Πειραματική Εγκατάσταση}

Ο σχεδιασμός και η αρχιτεκτονική του συστήματός μας σχεδιάστηκε έτσι, ώστε ο αισθητήρας Realsense 3D να μπορεί να τοποθετηθεί πάνω σε ένα HMD όπως το Oculus Rift. Μέσα από το HMD οι χρήστες θα μπορούσαν να δουν την έγχρωμη εικόνα της πραγματικής σκηνής που καταγράφει η Realsense camera. Έτσι ουσιαστικά δημιουργούμε μία συσκευή video see-through display. Ωστόσο λόγω χρονικών και περιορισμών και πολυπλοκότητας, αποφασίστηκε ότι η ενσωμάτωση του Oculus Rift στην αρχιτεκτονική της εφαρμογής θα ήταν υπερβολική. Συνεπώς, η συγκεκριμένη εφαρμογή ορίστηκε σε ένα πλαίσιο πειραματικής εγκατάστασης που προσομοιώνει ωστόσο τις παραμέτρους του ύψους και της γωνίας θέασης ενός χρήστη αν τοποθετούσε τον αισθητήρα επάνω σε ένα HMD. Επομένως οι αλγόριθμοι που αναπτύχθηκαν μπορούν να λειτουργήσουν άψογα αν στο μέλλον ενσωματωθεί ο αισθητήρας σε ένα HMD όπως το Oculus Rift. 


Με στόχο να μπορούμε να αλληλεπιδράσουμε με το εικονικό περιεχόμενο και να αξιοποιήσουμε την αλληλεπίδραση με βάση τη χειρονομία "τσιμπήματος", ορίσαμε μία περιοχή δοκιμών, όπου το markerboard τοποθετείται επάνω σε ένα τραπέζι και είναι εύκολα προσβάσιμο από έναν χρήστη που κάθεται μπροστά του. 


\begin{figure}[H]
    \centering
    \includegraphics[scale=0.8, angle=0]{Files/Figures/planviewoftheexperimentalsetup.png}
    \caption[Κάτοψη της σχεδιασμού της πειραματικής εγκατάστασης]{Κάτοψη της σχεδιασμού της πειραματικής εγκατάστασης}
    \label{fig:ps3game}
\end{figure}


Η κάμερα Realsense 3D τοποθετείται στο πίσω μέρος ενός απλού καπέλου το οποίο φοράει ο χρήστης κατά τη διάρκεια των δοκιμών, εμπνευσμένο από άλλη εργασία \cite{Mathews2007}. Όσο ο χρήστης φορά το καπέλο με τον αισθητήρα, ο αισθητήρας βλέπει προς το markerboard με αποτέλεσμα να "κοιτά" προς την περιοχή της αλληλεπίδρασης. Τέλος, ένας φορητός υπολογιστής τοποθετείται μπροστά από το markerboard και το χρήστη, ώστε να μπορεί να δει το επαυξημένο video που καταγράφει η κάμερα. Η κάμερα είναι συνδεδεμένη με το φορητό υπολογιστή, στον οποίο τρέχει η εφαρμογή. Η εικόνα~\ref{fig:test} δείχνει την πειραματική εγκατάσταση και έναν χρήστη κατά τη διάρκεια δοκιμής της εφαρμογής.



\begin{figure}[H]
\begin{subfigure}{.5\textwidth}
  \centering
  \includegraphics[width=.8\linewidth]{Files/Figures/user1.png}
  \caption{1a}
  \label{fig:sfig1}
\end{subfigure}%
\begin{subfigure}{.5\textwidth}
  \centering
  \includegraphics[width=.7\linewidth]{Files/Figures/user2.png}
  \caption{1b}
  \label{fig:sfig2}
\end{subfigure}\\
\caption{Φωτογραφία χρήστη κατά τη διάρκεια των δοκιμών}
\label{fig:test}
\end{figure}



\section{Βαθμονόμηση της Κάμερας}


Για την εύρεση των εσωτερικών παραμέτρων της κάμερας επιλέχθηκε η απλή, έτοιμη και αποτελεσματική λύση του εργαλείου βαθμονόμησης που παρέχεται μέσω της OpenCV. Η OpenCV διαθέτει έτοιμες συναρτήσεις και εργαλεία για την βαθμονόμηση της κάμερας που βασίζονται σε μεθόδους που αναφέρθηκαν στο κεφάλαιο 2. Επιπλέον μέσω της OpenCV μπορούμε να μάθουμε τους συντελεστές παραμόρφωσης της κάμερας. 


Για τη διεξαγωγή της βαθμονόμησης απαιτείται η χρήση μιας απλής, επίπεδης εικόνας ασπρόμαυρης σκακιέρας, που μπορεί να εκτυπωθεί με ένα κοινό εκτυπωτή. Η διαδικασία αυτή είναι αυτόματη, ενώ τα μόνα δεδομένα εισόδου που θα πρέπει να δώσουμε στο εργαλείο της OpenCV είναι ο αριθμός των εσωτερικών γωνιών των τετραγώνων της στις δύο κάθετες διευθύνσεις. Για παράδειγμα το πρότυπο της βαθμονόμησης φαίνεται στο σχήμα~\ref{fig:pattern}, όπου έχουμε μία σκακιέρα με αριθμό διαστάσεων με βάση τις εσωτερικές γωνίες 7x6.
Οι γωνίες τοποθετούνται σε σε ένα σύστημα 3D συντεταγμένων όπου z=0 (δηλαδή όλα τα σημεία βρίσκονται στο ίδιο επίπεδο). Η πληροφορία αυτή αξιοποιείται μέσω μεθόδων της OpenCV και επιστρέφονται ο πίνακας των εσωτερικών παραμέτρων της κάμερας και οι συντελεστές παραμόρφωσης. 



Η βαθμονόμηση της κάμερας πραγματοποιείται μόνο μία φορά σαν αρχικό βήμα (offline calibration) κατά το αρχικό στάδιο της ανάπτυξης της εφαρμογής. Πιο συγκεκριμένα, κάνουμε ξεχωριστό calibration για κάθε διαφορετικό μοντέλο κάμερας που μπορεί να χρησιμοποιηθεί στην αναπτυσσόμενη εφαρμογή, καθώς και για κάθε διαφορετική ανάλυση αυτών. Το αποτέλεσμα είναι ένα ξεχωριστό αρχείο για κάθε κάμερα και κάθε ανάλυση που περιέχει τις αντίστοιχες εσωτερικές παράμετρους. Ανάλογα με τον εξοπλισμό και τις επιλογές του χρήστη φορτώνεται κατά την έναρξη της εφαρμογής το ανάλογο αρχείο. Έτσι οι εσωτερικές παράμετροι είναι γνωστές, ώστε σε μεταγενέστερο στάδιο να βρεθούν οι εξωτερικές παράμετροι.



\begin{figure}[H]
    \centering
    \includegraphics[scale=0.4, angle=0]{Files/Figures/pattern.png}
    \caption[Πρότυπο σκακιέρας για τη βαθμονόμησης της κάμερας]{Πρότυπο σκακιέρας για τη βαθμονόμησης της κάμερας}
    \label{fig:pattern}
\end{figure}






Στο πλαίσιο της παρούσας εργασίας, έγινε βαθμονόμηση με λήψη εικόνων επίπεδης σκακιέρας με μέγεθος 7 x 6 εσωτερικών γωνιών, με χρήση του παραδείγματος της βιβλιοθήκης OpenCV. Το μέγεθος κάθε τετραγώνου της σκακιέρας ορίστηκε στα 2,5cm. Το πρότυπο αυτό καταγράφηκε μέσω της καμερας σε 25 διαφορετικές θέσεις. Η κάμερα διατηρήθηκε σε σταθερό σημείο και μετακινήσαμε την εκτυπωμένη εικόνα της σκακιέρας σε διαφορετικές θέσεις και με διαφορετικές κλίσεις. 



\begin{figure}
\begin{subfigure}{.5\textwidth}
  \centering
  \includegraphics[width=.9\linewidth]{Files/Figures/calibration.png}
  \caption{1a}
  \label{fig:calibration_screenshot1}
\end{subfigure}%
\begin{subfigure}{.5\textwidth}
  \centering
  \includegraphics[width=.9\linewidth]{Files/Figures/calib2.png}
  \caption{1b}
  \label{fig:calibration_screenshot2}
\end{subfigure}
\caption[Στιγμιότυπο κατά τη διαδικασία της βαθμονόμησης κάμερας μέσω της OpenCV]{Στιγμιότυπο κατά τη διαδικασία της βαθμονόμησης κάμερας μέσω της OpenCV}
\label{fig:calibration_screenshot}
\end{figure}


Η διαδικασία της βαθμονόμησης σε ανάλυση 640 x 480 έδωσε σαν αποτέλεσμα τις παρακάτω εσωτερικές παραμέτρους:


\begin{equation}
K=
\begin{bmatrix}
603.77848970443176 & 0 & 319.5\\
0 & 603.77848970443176 & 239.5\\
0 & 0 & 1
\end{bmatrix}
\end{equation}

Βλέποντας τις τιμές των principal points $c_{x}=319.5$ and $c{y}=239.5$ βλεπουμε ότι είναι περίπου στο κέντρο των επιλεγμένων διαστάσεων δηλαδή κοντά στις τιμές  320 (640/2) και 240(480/2). Επίσης βλέπουμε ότι $f_{x}=f_{y}$ που σημαίνει ότι χρησιμοποιείται κοινή εστιακή απόσταση και για τους 2 άξονες.


Εξ ορισμού η OpenCV μας δίνει 5 συντελεστές παραμόρφωσης, 3 ακτινικούς και 2 εφαπτομενικούς. Συγκεκριμένα κατά τη βαθμονόμηση πήραμε:

\begin{equation}
\begin{aligned}
k1= 0.16901284929874760\\
k2= -1.0214355567073656\\
p1= 0\\
p2= 0\\
k3= 1.3599972288823818 
\end{aligned}
\end{equation}

Το σφάλμα επαναπροβολής (Re-projection error) δίνει μία καλή εκτίμηση της ακρίβειας των παραμέτρων που βρέθηκαν κατά τη βαθμονόμηση της κάμερας. Η τιμή του πρέπει να είναι όσο γίνεται πιο κοντά στο 0. Υπολογίζεται με βάση τους πίνακες εσωτερικών παραμέτρων, συντελεστών παραμόρφωσης, περιστροφής και μετατόπισης. Πρέπει αρχικά να μετατρέψουμε το σημείο του αντικειμένου σε σημείο στην κάμερα. Έπειτα υπολογίζουμε την απόλυτη νόρμα ανάμεσα στο αποτέλεσμα που πήραμε με το μετασχηματισμό και με τον αλγόριθμο εύρεσης γωνιών. Για να βρούμε το μέσα σφάλμα υπολογίζουμε τον αριθμητικό μέσο των σφαλμάτων για όλες τις εικόνες βαθμονόμησης. Μετά τη διαδικασία βαθμονόμησης το σφάλμα που πήραμε ήταν πολύ κοντά στο 0 και συγκεκριμένα :

\begin{equation}
Avg_Reprojection_Error = 0.20315774320090751
\end{equation}


\section{Δημιουργία Markerboard}
%ΑΝΑΛΥΣΕ ΤΟ ARUCO FEATURE KAI ΠΩΣ ΓΙΝΕΤΑΙ ΤΟ MARKER TRACKING ΑΠΟ ΤΗΝ ARUCO ΚΑΙ ΤΟ BOARD.

Ως επαύξηση της πραγματικότητας ορίζεται η διαδικασία πρόσθεσης εικονικής πληροφορίας σε εικόνες ή βίντεο. Για να συμβεί κάτι τέτοιο, πρέπει να γνωρίζουμε που ακριβώς πρέπει να απεικονιστεί η εικονική πληροφορία. Παρά το γεγονός ότι ορισμένες εφαρμογές αξιοποιούν τα φυσικά χαρακτηριστικά μιας σκηνής όπως η υφή ή σημεία κλειδιά, τα markers αποτελούν ακόμα έναν ελκυστικό τρόπο προσέγγισης επειδή είναι εύκολη η γρήγορη ανίχνευσή τους και η ακρίβεια που μπορεί να επιτευχθεί, όπως παρουσιάστηκε και στο~\ref{ssec:markerpose}. Μέσω της ανίχνευσης ενός marker όπως αυτοί που χρησιμοποιούνται από την ArUco μπορούν να υπολογιστούν οι εξωτερικές παράμετροι της κάμερας, δηλαδή η θέση και ο προσανατολισμός της σε σχέση με το marker ώστε να γνωρίζει το πρόγραμμα που πρέπει να σχεδιάσει τα 3D αντικείμενα και που είναι η αρχή των αξόνων (0,0,0) του συστήματος κοσμικών συντεταγμένων της σκηνής. 

Όπως συζητήθηκε και στην ενότητα~\ref{ssec:markerboard} η ανίχνευση ενός και μόνο marker μπορεί να αποτύχεια για πολλούς λόγους. Επομένως, για να αντιμετωπιστεί αυτό το πρόβλημα, αποφασίστηκε η χρήση markerboards της ArUco, δηλαδή ενός marker που αποτελείται από μια διάταξη πολλών markers.

Το γνωστό επιτραπέζιο παιχνίδι του σκακιού απαιτεί τη χρήση μιας σκακιέρας ως βάση προκειμένου να τοποθετηθούν τα πιόνια επάνω της.Kατά τη διάρκεια ενός παιχνιδιού σκακιού, ο χρήστης συχνά πρέπει να μετακινεί τα πιόνια με το χέρι του. Όταν συμβαίνει αυτό, το χέρι του χρήστη περνά πάνω από τη σκακιέρα αποκρύπτοντας ένα αρκετά σημαντικό μέρος της. 

Στα πλαίσια της παρούσας διπλωματικής εργασίας και προκειμένου να δημιουργήσουμε ένα σκάκι επαυξημένης πραγματικότητας θεωρήθηκε ότι η αξιοποίηση ενός markerboard προσομοιώνει τις ιδιότητες μιας σκακιέρας, ενώ η απόκρυψη μέρους του markerboard από το χέρι του χρήστη δεν επηρεάζει την απεικόνιση των εικονικών πιονιών κατά τη διάρκεια της μετακίνησης ενός πιονιού από το χρήστη. Επομένως αποφασίστηκε η δημιουργία ενός markerboard με μια διάταξη από 64 markers, σε ένα πλέγμα 8x8, με διαστάσεις ίδιες με αυτές μιας σκακιέρας.


Οι δημιουργοί της βιβλιοθήκης ArUco δημοσίευσαν μία επιστημονική εργασία \cite{garrido2014automatic} όπου αναλύεται η μεθοδολογία για τη δημιουργία markerboard με markers τα στα οποία αντιστοιχίζονται συγκεκριμένα IDs με στόχο τον υψηλό αριθμό μεταβολών bits ώστε να μην μπερδεύονται τα markers με πραγματικά αντικείμενα του περιβάλλοντος, να ανιχνεύονται δηλαδή ευκολότερα από το σύστημα. 

Προτείνεται λοιπόν μια αυτόματη μέθοδος για την παραγωγή ενός markerboard με τον επιθυμητό αριθμό markers και τον επιθυμητό αριθμό bits. Χρησιμοποιώντας το έτοιμο εργαλείο που παρέχεται από τη βιβλιοθήκη ArUco για το σκοπό αυτό, δημιουργήσαμε μία "σκακιέρα" από markers υψηλής αξιοπιστίας, όπως φαίνεται και στην εικόνα~\ref{fig:markerboard}




\begin{figure}[H]
    \centering
    \includegraphics[width=0.8\textwidth]{Files/Figures/markerboard.jpg}
    \caption[Το markerboard που δημιουργήθηκε μέσω της ArUco]{Το markerboard που δημιουργήθηκε μέσω της ArUco}
    \label{fig:markerboard}
\end{figure}





\section{Αναγνώριση Χειρονομίας Τσιμπήματος} \label{section:pinch}
%ΠΩΣ ΑΚΡΙΒΩΣ ΚΑΝΩ ΤΗ ΧΕΙΡΟΝΟΜΙΑ ΤΣΙΜΠΗΜΑΤΟΣ-ΤΑ ΘΕΩΡΗΤΙΚΑ ΜΕΡΗ



Προκειμένου να ορίσουμε τον τρόπο με τον οποίο θα γίνεται ο εντοπισμός των χειρονομιών, καθώς και το είδος των χειρονομιών που πρέπει να ανιχνευτούν, χρειάζεται να καθοριστούν οι απαιτήσεις του συστήματος με λεπτομέρειες.
Για να αναπτυχθεί ένα σκάκι επαυξημένης πραγματικότητας απαιτείται προφανώς αλληλεπίδραση σε πραγματικό χρόνο, σχετικά φθηνός εξοπλισμός και αλληλεπίδραση με όσο περισσότερο φυσικό τρόπο γίνεται, δηλαδή χωρίς τη χρήση ειδικών γαντιών ή καλωδίων. 

Οι απαιτήσεις του συστήματος για εκτέλεση των αλληλεπιδράσεων σε πραγματικό χρόνο, περιορίζει το επίπεδο της ανίχνευσης χειρονομιών που μπορεί να επιτευχθεί, ενώ παράλληλα οι απαιτήσεις για εξοπλισμό χαμηλού κόστους περιορίζουν την ποιότητα της εικόνας που πρέπει να καταγραφεί. Ο τρίτος περιορισμός που εισάγεται, προϋποθέτει τη χρήση τεχνικών όρασης υπολογιστών για την αναγνώριση των χειρονομιών του χρήστη, η απόδοση των οποίων περιορίζεται από τον επεξεργαστή του συστήματος. 



Η χειρονομία "τσιμπήματος" είναι μία χειρονομία που μπορεί να χρησιμοποιηθεί για την αλλεπίδραση με αντικείμενα στον 3D χώρο. On a user-based gesture taxonomy developed [22], [30], several gestures such as ponting, pinch, grab, stretch (using both hands) have been classified according to user experience. Η χειρονομία τσιμπήματος μπορεί να πραγματοποιηθεί αρκετές φορές και υποδηλώνει φυσική κίνηση για επιλογή ενός αντικειμένου. Επομένως η αναγνώρισή της είναι βασική προϋπόθεση για την υλοποίηση βασικών αλληλεπιδράσεων με τα εικονικά αντικείμενα. 


Κατά τη διάρκεια της μετακίνησης ενός πιονιού στο σκάκι, μέρος του χεριού και των δακτύλων του χρήστη δεν είναι ορατά από την κάμερα. Για το λόγο αυτό, σκεφτήκαμε ότι η χρήση τεχνικών εντοπισμού της πόζας του χεριού μέσω ενός σκελετικού μοντέλου ή μέσω των μεθόδων που παρέχονται από το RealSense SDK θα ήταν υπερβολική για το συγκεκριμένο είδος εφαρμογής. Επομένως, δε χρειάζεται να ανιχνεύσουμε ούτε ολόκληρο το χέρι του χρήστη, ούτε τα δάκτυλα του αυτούσια. 


Με βάση τους παραπάνω περιορισμούς, η ανάπτυξη του συστήματος βασίστηκε στην εργασία του Andrew Wilson \cite{Wilson2006} όπου παρουσιάζεται μία τεχνική υπολογιστικής όρασης για την εύκολη και γρήγορη αναγνώριση της κίνησης κατά την οποία ο αντίχειρα και ο δείκτης ενός χρήστη έρχονται κοντά (χειρονομία "τσιμπήματος") για εφαρμογές μικρής εμβέλειας και σχετικά ελεγχόμενες συνθήκες θέασης. Η τεχνική αυτή αποφεύγει πολύπλοκους και εύθραυστους αλγορίθμους εντοπισμού των χεριών εντοπίζοντας την οπή που σχηματίζεται όταν ο δείκτης αγγίζει τον αντίχειρα του χρήστη και παρουσιάστηκε στην ενότητα~\ref{ssection:pinch}. Το πρόβλημα της αλληλεπίδρασης μετατρέπεται ουσιαστικά σε πρόβλημα αναγνώρισης χειρονομιών. Περιγράφεται, δηλαδή, ένας εύκολος και γρήγορος τρόπος για την αναγνώριση της χειρονομίας τσιμπήματος. 


Στην παρούσα διπλωματική εργασία, υλοποιήθηκε ένα αλγόριθμος ανίχνευσης χειρονομιών, ο οποίος ενεργοποιεί μία εντολή αρπαγής και απελευθέρωσης των εικονικών πιονιών στην σκηνή της επαυξημένης πραγματικότητας. Χρησιμοποιούμε, δηλαδή, τη χειρονομία αυτή για να επιλέξουμε ένα πιόνι στη σκακιέρα και να μπορέσουμε να το μετακινήσουμε σε κάποια άλλη θέση.

Οι συναρτήσεις του SDK επιτρέπουν την ανίχνευση αντικειμένων μπροστά από την κάμερα, όπως είναι τα blobs και την εξαγωγή περιγραμμάτων και σημείων ενδιαφέροντος για αυτά τα blobs. Η ανίχνευση των blobs είναι μία εναλλακτική που βολεύει σε σχέση με τον εντοπισμό των χεριών που παρέχει το SDK, αφού η εφαρμογή μας δεν απαιτεί την ανίχνευση και τον εντοπισμό χεριών. Κάθε blob διαθέτει μία εξωτερική γραμμή περιγράμματος (contour line) και ανάλογα με στο σχήμα του blob, μία ή περισσότερες εσωτερικές γραμμές περιγράμματος. Κάθε τέτοια γραμμή περιγράμματος αναπαρίσταται από μία σειρά σημείων. Οι εξωτερικές γραμμές περιγραμμάτων (εξωτερικά σύνορα), καθώς και οι εσωτερικές γραμμές των περιγραμμάτων ορίζονται από ένα πίνακα σημείων.


Η προσέγγιση μας, αξιοποιεί τα δεδομένα των blob και την οπή που σχηματίζεται όταν ο αντίχειρας ακουμπά ή βρίσκεται πολύ κοντά στον δείκτη. Πιο συγκεκριμένα, υλοποιήθηκε ένας αλγόριθμος που ανιχνεύει πότε λαμβάνει χώρα μία χειρονομία τσιμπήματος και που ακριβώς στον 3Δ χώρο. Γνωρίζοντας το σημείο στο 3Δ χώρο στο οποίο συμβαίνει η χειρονομία, μπορούμε να επιλέξουμε το σωστό πιόνι και να το μετακινήσουμε ανάλογα. Η απρόσκοπτη ανίχνευση μιας χειρονομίας τσιμπήματος είναι η βάση για τη σωστή λειτουργία του συστήματός μας και για αυτό το λόγο θα αναφερθούμε ξεχωριστά στην αναγνώριση και την ανίχνευση της θέσης στην οποία πραγματοποιείται η χειρονομία. Ο αλγόριθμος που σχεδιάστηκε αναλύεται στη συνέχεια.



Σε κάθε frame, παίρνουμε τις εικόνες βάθους και χρώματος που καταγράφει η συσκευή μας. Λόγω της λανθασμένης ευθυγράμμισης και της φυσικής απόκλισης της θέσης του αισθητήρα χρώματος σε σχέση με τον αισθητήρα βάθους (αφού δεν βρίσκονται ο ένας πάνω στον άλλο), χρειάζεται ένας τρόπος να αντιστοιχηθούν τα εικονοστιχεία χρώματος με τα εικονοστοιχεία βάθους και αντίστροφα. Για το συγκεκριμένο λόγο, η βιβλιοθήκη RealSense παρέχει μία δομή UVMap η οποία επιτελεί αυτό το συγκεκριμένο σκοπό.

Στην αρχή της διαδικασίας, χρειάζεται να χρησιμοποιήσουμε τις ιδιότητες τιης βιβλιοθήκης για εξαγωγή των blobs. Πριν συμβεί αυτό, πρέπει να ορίσουμε κάποιες παραμέτρους, όπως τον μέγιστο αριθμό blobs που θέλουμε να ανιχνευτούν και το επίπεδο της εξομάλυνσης κατά την κατάτμηση της εικόνας για να πάρουμε τα σωστά περιγράμματα του blob. 

Στην προσέγγιση μας, επιλέξαμε να ανιχνεύεται το κοντινότερο blob της εικόνας βάθους ως προς την κάμερα, διότι κατά τη διάρκεια ενός παιχνιδιού σκακιού ως προς το σημείο παρατήρησης του χρήστη (π.χ τα μάτια του), το ρόλο αυτό παίζουν τα χέρια του, τα οποία και όντως θέλουμε να εντοπίσουμε. Μόλις αναγνωριστεί το κοντινότερο blob, ωστόσο, πρέπει να το επεξεργαστούμε για να δούμε αν όντως πρόκειται για ένα blob χεριού ή για κάποιο άλλο αντικείμενο.


Για το λόγο αυτό, παίρνουμε μέσω του SDK τον αριθμό των περιγραμμάτων που ανιχνεύθηκαν στο συγκεκριμένο blob. Αν ο αριθμός αυτός είναι μικρότερος του 2, αυτόματα συνεπάγεται ότι δεν υπάρχουν εσωτερικά περιγράμματα και επομένως δεν υπάρχει οπή στην εικόνα μας. Έτσι το πρόγραμμα γνωρίζει σίγουρα ότι ο χρήστης δεν εκτέλεσε χειρονομία "τσιμπήματος". Από την άλλη πλευρά, αν ο αριθμός των περιγραμμάτων είναι μεγαλύτερος του 2, τότε είναι πιθανό να έλαβε χώρα μια χειρονομία τσιμπήματος. 


Για να ελέγξουμε αν συνέβη κάτι τέτοιο, παίρνουμε τον αριθμό των σημείων που απαρτίζουν το εσωτερικό περίγραμμα που ανιχνεύθηκε. Αν ο αριθμός των σημείων αυτών είναι μικρότερος από ένα κατώφλι, δηλαδή μία τιμή που ορίζεται μέσω δοκιμών, τότε μπορούμε είτε να συμπεράνουμε ότι τα δεδομένα του περιγράμματος δε σχετίζονται με το χέρι του χρήστη, είε ότι το χέρι είναι πολύ μακριά από την κάμερα, και μπορούμε να συνεχίσουμε την ανάλυση του επόμενου frame. Αν όμως ο αριθμός των σημείων είναι μεγαλύτερος της τιμής κατωφλίου που ορίστηκε, τότε μπορούμε να αποφανθούμε ότι έχουμε μία χειρονομία "τσιμπήματος". Όταν συμβεί κάτι τέτοιο πρέπει να ελέγξουμε τη θέση στην οποία συνέβη η χειρονομία στο 3D χώρο. Η διαδικασία αυτή περιγράφεται στην επόμενη ενότητα, ενώ ο αλγόριθμος αναγνώρισης χειρονομίας "τσιμπήματος" που περιγράφηκε φαίνεται στο σχήμα~\ref{fig:gesture_rec}.



\begin{figure}[H]
    \centering
    \includegraphics[scale=0.7, angle=0]{Files/Figures/2.png}
    \caption[Παράδειγμα blob χεριού και περιγραμμάτων που σχηματίζονται]{Παράδειγμα blob χεριού και περιγραμμάτων που σχηματίζονται}
    \label{fig:gesture_rec}
\end{figure}



\begin{figure}[H]
    \centering
    \includegraphics[scale=0.5, angle=0]{Files/Figures/pinch_gesture_detection.png}
    \caption[Διάγραμμα του αλγορίθμου ανίχνευσης χειρονομίας "τσιμπήματος"]{Διάγραμμα του αλγορίθμου ανίχνευσης χειρονομίας "τσιμπήματος"}
    \label{fig:gesture_rec}
\end{figure}






\section{Ανίχνευση Θέσης Χειρονομίας "Τσιμπήματος"}
%ΠΩΣ ΑΝΙΧΝΕΥΕΤΑΙ Η 3D ΘΕΣΗ ΤΣΙΜΠΗΜΑΤΟΣ ΜΕ ΒΑΣΗ ΤΗ ΜΕΘΟΔΟΛΟΓΙΑ ΜΟΥ


One of the most significant contributions of this thesis is the following handbased interaction system using gesture recognition. Our goal is to provide a simple gesture recognition system for two-dimensional manipulative interaction. Currently, using a mouse to manipulate a window interface is commonplace. Our system provides a mouse-like gesture based interface to an immersed AR user without the need for the cumbersome mouse. To simulate a mouse requires the recognition of both point and select gestures in order to generate the appropriate mouse-down and mouse-up events at the indicated location. This goal is achieved without the need for a sophisticated gesture recognition system such as [OKA02] involving complex finger tracking for gesture inference through motion. Instead, the gesture model is specialized for the task of mouse replacement. Performing the gesture analysis in pattern-space simplifies the image processing and creates a very robust gesture recognition system.



To determine the location of the user’s select actions, a pointer location must be chosen from the hand point set. To simplify this process, the current system constraints were exploited and a number of assumptions were made. The first useful constraint deals with the amount of target occlusion permitted. The planar tracking system used for augmentation assumes that approximately half of the target corners are visible at all times during the tracking phase. To satisfy this constraint, only a portion of a hand can occlude the target at any given time. For this reason, the assumption is made that the only portion of the hand to occlude the target will be the fingers. From this we get: Assumption 1: Separated fingers will be detected as separate blobs in the image analysis phase. Due to the simplicity of the desired interaction, a second assumption was made: Assumption 2: Fingers will remain extended and relatively parallel to each other.






%δικο μου
Αν κατά το στάδιο της ανίχνευσης μιας χειρονομίας "τσιμπήματος" διαπιστώθηκε ότι έχουμε όντως μία τέτοια χειρονομία, δηλαδή ο αριθμός των περιγραμμάτων του blob είναι μεγαλύτερος ή ίσος του 2 και ο αριθμός των σημείων του εσωτερικού περιγράμματος είναι μεγαλύτερος από μία τιμή κατωφλίου, μπορούμε να συνεχίσουμε για να βρούμε τη θέση στην οποία πραγματοποιήθηκε η χειρονομία ως προς την κάμερα.

Για να συμβεί αυτό, αρχικά επιλέγεται το σημείο του εσωτερικού περιγράμματος που βρίσκεται όσο γίνεται πιο αριστερά, όσον αφορά την εικόνα βάθους, καθώς και το σημείο του εσωτερικού περιγράμματος που βρίσκεται όσο γίνεται πιο δεξιά. Με βάση αυτά τα 2 σημεία, μπορούμε να βρούμε μία μοναδική ευθεία που ορίζεται από αυτά. Στη συνέχεια, μπορούμε να δημιουργήσουμε μία περιοχή ή καλύτερα μία "γειτονιά"  ενός παραμετρικού αριθμού σημείων, που ανήκουν στη συγκεκριμένη ευθεία που υπολογίστηκε και βρίσκονται αριστερά από το πιο αριστερό σημείο του εσωτερικού περιγράμματος.  Ο αριθμός των σημείων που μπορούμε να πάρουμε μπορεί να οριστεί ως παράμετρος στο πρόγραμμά μας. Προφανώς, όσο περισσότερα σημεία επιλέξουμε, τόσο πιο ακριβές θα είναι το αποτέλεσμα, αλλά τόσο πιο αργό θα γίνεται το πρόγραμμα όπως θα διαπιστωθεί στη συνέχεια. 


Μόλις εκτιμηθεί αυτή η "γειτονιά" σημείων στο frame της εικόνας βάθους, μπορούμε να υπολογίσουμε τις τιμές βάθους για καθένα από αυτά τα σημεία και επομένως να εκτιμήσουμε τη μέση τιμή βάθους των εικονοστοιχείων της "γειτονιάς" τα οποία έχουν έγκυρες τιμές. Αυτή η μέση τιμή βάθους, μπορεί να χρησιμποιηθεί αργότερα ως η τιμή βάθους στην οποία πραγματοποιείται η χειρονομία "τσιμπήματος" στον τρισδιάστατο χώρο της σκηνής. 

Αξιοποιώντας, τη δομή UVMap, στην οποία αναφερθήκαμε στην ενότητα~\ref{section:pinch}, μπορούμε να αντιστοιχίσουμε κάθε σημείο, δηλαδή κάθε εικονοστοιχείο της "γειτονιάς" σημείων στην εικόνα χρώματος που καταγράφει ο αισθητήρας στο ίδιο frame. Υπολογίζουμε τη μέση τιμή των συντεταγμένων εικόνας στην οποία βρίσκεται κάθε σημείο και έχει έγκυρες τιμές (αφού κατά την αντιστοίχιση μπορεί να έχουμε λανθασμένες προσεγγίσεις). Ωστόσο οι τιμές αυτές μετρώνται σε pixels ενώ εμείς θέλουμε να βρούμε το σημείο στο οποίο έλαβε χώρα η χειρονομία στο 3D χώρο σε μονάδες του πραγματικού κόσμου (π.χ μέτρα). Επομένως, πρέπει να προβάλλουμε αυτά τα εικονοστοιχεία χρώματος στο σύστημα συντεταγμένων της κάμερας ώστε να πάρουμε τις αποστάσεις σε μονάδες του πραγματικού κόσμου, δηλαδή στην περίπτωσή μας σε μέτρα. 


Μόλις ολοκληρωθεί αυτή η διαδικασία, έχουμε τις τιμές των συντεταγμένων x,y,z για ένα συγκεκριμένο σημείο στον τρισδιάστατο χώρο σε μέτρα, το οποίο θεωρούμε ως το σημείο στο οποίο πραγματοποιήθηκε η χειρονομία "τσιμπήματος" από το χρήστη, ή αλλιώς ως το σημείο στον τρισδιάστατο χώρο όπου ο χρήστης αποφάσισε να "τσιμπήσει" κάποιο εικονικό αντικείμενο. Προφανώς, μόλις συμβεί κάτι τέτοιο και αν οι συντεταγμένες έχουν μία έγκυρη τιμή, ενεργοποιείται μία σημεία που υποδηλώνει στο πρόγραμμά μας ότι πραγματοποίηθηκε μία χειρονομία "τσιμπήματος". 

Εν τέλει, έχουμε το σημείο "τσιμπήματος" στον τρισδιάστατο χώρο ως προς τον αισθητήρα χρώματος της συσκευής. Με βάση την μεθοδολογία που αναφέρθηκε, μπορούμε να αναπτύξουμε την εφαρμογή μας και να υλοποιήσουμε τη λογική του βιντεοπαιχνιδιού σκακιού επαυξημένης πραγματικότητας. 


Αφού όλα τα εικονικά αντικείμενα απεικονίζονται ως προς την αρχή των συντεταγμένων του κέντρου της σκακιέρας, δηλαδή του markerboard, η αλληλεπίδραση ανάμεσα στα πραγματικά και τα εικονικά αντικείμενα θα πρέπει να πραγματοποείται ως προς αυτό το κοινό σύστημα συντεταγμένων. Επομένως, στην επόμενη ενότητα θα παρουσιαστεί η διαδικασία μετατροπής του 3D σημείου όπου πραγματοποιήθηκε η χειρονομία "τσιμπήματος", από το σύστημα συντεταγμένων κάμερας στο σύστημα συντεταγμένων του markerboard.








\begin{figure}
\begin{subfigure}{.5\textwidth}
  \centering
  \includegraphics[width=.8\linewidth]{Files/Figures/2.png}
  \caption{1a}
  \label{fig:sfig1}
\end{subfigure}%
\begin{subfigure}{.5\textwidth}
  \centering
  \includegraphics[width=.8\linewidth]{Files/Figures/3.png}
  \caption{1b}
  \label{fig:sfig2}
\end{subfigure}\\
\begin{subfigure}{.5\textwidth}
  \centering
  \includegraphics[width=.8\linewidth]{Files/Figures/4.png}
  \caption{1a}
  \label{fig:sfig1}
\end{subfigure}%
\begin{subfigure}{.5\textwidth}
  \centering
  \includegraphics[width=.8\linewidth]{Files/Figures/5.png}
  \caption{1b}
  \label{fig:sfig2}
\end{subfigure}
\caption{Pinch Gesture Detection Phases}
\label{fig:fig}
\end{figure}




\section{Tαυτοποίηση Συστημάτων Συντεταγμένων}

Coordinate Systems Matching A first step is to consider a unified coordinate system between the AR target and hand tracking sensor. The simplest coordinate systems matching is based on the assumption that our AR marker and sensor are placed on the same origin, when this is not the case, i.e. the sensor and marker are placed in different areas from the camera view perspective, a translation of the Leap Motion’s pose matrix needs to be performed by using the AR marker as a reference. A solution to this, is to use the image target to act as the world center and bring the Leap Motion’s coordinate system by multiplying the inverse pose of our main target (image target) by the pose of the Leap Motion’s target. This creates an offset matrix that can be used to bring points from Leap Motion to image target coordinate system[47]. Nevertheless, for our prototype purpose, it is assumed that the AR marker and sensor are placed in the same origin, where it matches both coordinate systems. This is done, both programmatically, through the game engine and physically where are adjusted in the same origin from a camera view perspective as seen on the Figure.

Ο μηχανισμός αλληλεπίδρασης που παρουσιάστηκε σε αυτή και την προηγούμενη ενότητα αφορά τη μοντελοποίηση ενός τρόπου επιλογής και χειρισμού εικονικών αντικειμένων που θα μπορούσε κάποιος να παρομοιάσει με το κλικάρισμα ενός ποντικιού και τη μετακίνηση του όπως μετακινούμε ένα εικονίδιο σε μία επιφάνεια εργασίας ενός λειτουργικού συστήματος.

Για να δημιουργηθεί ένα σύστημα αλληλεπίδρασης ανθρώπου - υπολογιστή, οι δράσεις που παράγονται όταν λαμβάνει χώρα μία χειρονομία πρέπει να ορίζονται από ένα σύνολο καταστάσεων του συστήματος. Στις επόμενες ενότητες, θα δούμε πώς η δημιουργία τέτοιων καταστάσεων εξυπηρετεί τη λειτουργικότητα της εφαρμογής κατά τη διάρκεια ενός παιχνιδιού σκακιού επαυξημένης πραγματικότητας.



\section{Απεικόνιση Εικονικών Αντικειμένων} \label{s:rendering}
%clarke


As last part of the experimental studies 3D objects are rendered onto the views. the virtual camera uses the pose calculated by the ArUco's algorithm which should be placed at the same position as the real camera. 

Using the calculated camera poses virtual objects have been rendered onto the images. Knowing where the object should be placed in the real world is essential for an augmented reality application. 



%σελ 25 jimenez
Although the Unity platform deals with the graphics and integration of libraries for this project, it is necessary to give a deeper review of the process by explaining the target pose calculation and content positioning in the AR framework and the hand controller’s integration. The AR framework exposes a 3x4 Pose Matrix, equation 3.2 that represents the pose of a target with respect to the camera plane. The left 3X3 matrix expresses the rotation matrix, which indicates how the target is rotated while the right column is the translation vector, which is the position of the target as seen from the camera[47], e.g. an identity matrix indicates that the target is parallel to the camera plane (rotation) and a vector (0,0,0) indicates the camera and the target are in the same position.

To position content on the target, the first step is to know the projection matrix -4x4 matrix that projects the scene to the image plane of the camera[49]- that is created with the intrinsic camera calibration parameters, vuforia uses a right handed coordinate system (with the camera origin pointing into the positive z-axis, x-axis to the right and y-axis is downwards) because it is the same coordinate system for targets (just rotated 90 degrees around the x axis: x-axis to the right, y-axis is upwards and z-axis points out of the target-plane; in Unity, z-axis is upwards and y-axis points out of the target)[47]. Once we know the projection matrix we need to set a perspective frustrum from it, where a near and far planes must be set and the size of our trackable -defined in the dataset with feature- should fall within these two planes, Figure 18.
With these requirements in mind, the virtual content (vertices, normals, indices, texture coordinates)
is placed according to the pose matrix calculated at runtime.
The Leap Motion coordinates can be used to move a virtual object, firstly, normalizing and
transforming to suitable coordinates in the interaction box, Figure 13 explained previously.

\section{Χειρισμός Εικονικών Αντικειμένων} \label{s:manipulation}
%ΠΩΣ ΜΕΤΑΚΙΝΟΥΝΤΑΙ ΤΑ ΠΙΟΝΙΑ ΜΕ ΒΑΣΗ ΤΟΝ ΑΛΓΟΡΙΘΜΟ ΜΟΥ
%wang-paris-popovi

-Object translation 
When the user pinches with one hand,
we select the closest object, and translate according to the
pinching hand.


%mcdonald
With this visual feedback mechanism in place, a mechanism for initiating interaction with the controls on the panel is needed. The behaviour associated with control manipulation is defined in the normal event driven, object-oriented fashion associated with window– based application programming. Applying the gesture model to this augmented interaction requires only a simple communicative translation between the gestures, including posture parameters, and the event-based control manipulation. This translation is defined in terms of the gesture state machine outlined in figure 5.5. For example, when a selection gesture is recognized immediately following a pointing gesture, a mouse-down event is sent to the actual control panel dialog, along with the pointer location parameter as if it were sent by the mouse hardware. This way, when the gesture occurs over a button on the virtual panel, the event generates the equivalent button press on the dialog box. On the other hand, when a pointing gesture immediately follows a selection gesture, a mouse-up event is sent


\section{Αντιμετώπιση Απόκρυψης Αντικειμένων} \label{s:occlusion}
%OCCLUSION HANDLING



%-----
Για να μπορέσει ένας χρήστης να εκτελέσει επιτυχώς ορισμένες διεργασίες σε περιβάλλοντα επαυξημένης πραγματικότητας, χρειάζεται να παρέχεται από το σύστημα ένα συγκεκριμένο επίπεδο "εμβύθισης" του χρήστη. Πρέπει δηλαδή ο χρήστης να πιστέψει, όσο γίνεται, ότι τα εικονικά αντικείμενα είναι πραγματικά. 
Στις περισσότερες προσεγγίσεις, οι αποκρύψεις των εικονικών αντικειμένων δεν λαμβάνονται υπόψη και επομένως τα εικονικά αντικείμενα απεικονίζονται πάντα πάνω από το frame του βίντεο, συνεπώς πάνω από τα πραγματικά αντικείμενα της σκηνής. 
Ωστόσο, για να δημιουργηθούν καθηλωτικές και ρεαλιστικές εφαρμογές, η απόκρυψη ανάμεσα στα εικονικά και τα πραγματικά αντικείμενα πρέπει να πραγματοποιηθεί σωστά, ώστε οι χρήστες να μπορούν να κοιτάξουν σε μία σκηνή, όπου το εικονικό περιεχόμενο θα είναι εναρμονισμένο με το φυσικό περιβάλλον. Μία τέτοια προσέγγιση μας επιτρέπει να πετύχουμε υψηλό επίπεδο "εμβύθισης" αφού τα εικονικά αντικείμενα μπορούν να εναρμονιστούν με τα χέρια του χρήστη, ώστε να εμφανίζεται σωστά το ένα πάνω από το άλλο ή το αντίθετο. 

Κάτι τέτοιο συνήθως δε συμβαίνει στις περισσότερες εφαρμογές επαυξημένης πραγματικότητας. Στην OpenGL, όταν ένα αντικείμενο απεικονίζεται στην οθόνη, το βάθος ενός pixel που παράγεται (z coordinate) αποθηκεύεται σε ένα buffer που ονομάζεται z-buffer ή buffer βάθους (depth buffer). Αυτός ο buffer ορίζεται συνήθως ως ένα δισδιάστατος πίνακας (x-y) με ένα στοιχείο για κάθε εικονοστοιχείο της οθόνης. Αν κάποιο άλλο αντικείμενο της σκηνής πρέπει να απεικονιστεί στο ίδιο εικονοστοιχείο, η μέθοδος συγκρίνει τα 2 βάθη και αντικαθιστά το τρέχων εικονοστοιχείο, αν το αντικείμενο είναι πιο κοντά στο θεατή. Έπειτα, το επιλεγμένο βάθος αποθηκεύεται στο z-buffer, αντικαθιστώντας την προηγούμενη τιμή. Εν τέλει, ο z-buffer θα επιτρέψει στη μέθοδο να αναπαράγει σωστά την αντίληψη του βάθους, δηλαδή το γεγονός ότι ένα αντικείμενο που βρίσκεται πιο κοντά θα πρέπει να κρύβει ένα άλλο που βρίσκεται πιο μακριά.


Το κύριο πρόβλημα που εμφανίζεται προκειμένου και εμποδίζει την αντιμετώπιση του φαινομένου της απόκρυψης στα επαυξημένα περιβάλλοντα βρίσκεται στο γεγονός ότι, συνήθως, δεν υπάρχει πληροφορία για το βάθος της σκηνής. Για να ξεπεραστεί αυτό το πρόβλημα, πρέπει να αξιοποιηθεί το βάθος της πραγματικής σκηνής όπως το βλέπει ο χρήστης, δηλαδή ως προς τη θέση του χρήστη. 

Στην παρούσα διπλωματική εργασία, χρησιμοποιήθηκε ο αισθητήρας βάθους της κάμερας Realsense 3D, για την υποστήριξη της διαδικασίας απόκτησης ενός χάρτη βάθους της σκηνής του περιβάλλοντος. O αισθητήρας αυτός μπορεί να μετρήσει την απόσταση κάθε αντικειμένου που βλέπει, δημιουργώντας ένα χάρτη βάθους. Με αυτό τον τρόπο το βάθος κάθε pixel του depth frame μπορεί να εκτιμηθεί. Η εικόνα βάθους είναι ουσιαστικά ένας πίνακας διαστάσεων 640 x 480  όπου η τιμή κάθε pixel αναπαριστά την απόσταση της κάμερας βάθους από το συγκεκριμένο σημείο στο 3D χώρο (σε χιλιοστά). 


Όταν μία κάρτα βίντεο απεικονίζει μία εικονική σκηνή, υπολογίζει την απόκρυψη μέσω του depth buffer, δηλαδή την απόσταση z κάθε αντικειμένου από την εικονική σκηνή. Συνήθως, όταν ένα άλλο εικονικό αντικείμενο πρέπει να απεικονιστεί στο ίδιο pixel του z-buffer, η μέθοδος συγκρίνει τα 2 βάθη και αντικαθιστά το τρέχων pixel αν το αντικείμενο είναι πιο κοντά στον παρατηρητή, δηλαδή την κάμερα.

Το τέχνασμα που χρησιμοποιείται είναι η αρχικοποίηση του Z-Buffer της OpenGL με τις τιμές βάθους που παίρνουμε από τον αισθητήρα πριν την απεικόνιση ενός 3D εικονικού αντικειμένου και αφού απεικονίσουμε το πολύγωνο που δείχνει την έγχρωμη εικόνα που καταγράφει το βίντεο. Με αυτό τον τρόπο, όταν τα εικονικά πιόνια απεικονίζονται στην οθόνη, θα αποκρύπτονται από τα χέρια του χρήστη ή από οποιοδήποτε άλλο αντικείμενο περάσει από πάνω τους. 

Η διαδικασία αυτή μοιάζει με την προσομοίωση της απεικόνισης ενός ολόκληρου περιβάλλοντος 3D και τη χρήση των τιμών του z-buffer. Eπομένως, προκειμένου να αντιμετωπίσουμε το πρόβλημα της απόκρυψης αντικειμένων (occlusion handling) έπρεπε να αντιστοιχήσουμε ολόκληρη το χάρτη βάθους στην έγχρωμη εικόνα που καταγράφεται, να μετατρέχουμε τις τιμές βάθους από χιλιοστά σε μέτρα και μετά να γράψουμε κάθε τιμή στο z-buffer της OpenGL.


Ώστόσο, πριν φτάσουμε στο σημείο να γράψουμε στο z-buffer, πρέπει να λάβουμε υπ'οψην μας το είδος της προβολής που χρησιμοποιείται από την OpenGL στην εφαρμογή μας (ορθή ή προοπτική). 


Στη συγκεκριμένη περίπτωση, το είδος της προβολής με την οποία απεικονίζονται τα εικονικά αντικείμενα της σκηνής είναι η προοπτική προβολή και πρέπει να τροποποιήσουμε τα δεδομένα με βάση αυτή. Σε αυτό το είδος προβολής, η σχέση ανάμεσα στην τιμή Z και το βάθος είναι μη-γραμμική και συγκεκριμένα της μορφής:



\begin{equation}
\begin{aligned}
depth=\frac{A}{Z}+B \\
A=\frac{Z_{far}Z_{near}}{Z_{far}-Z_{near}}\\ B=\frac{Z_{far}}{Z_{far}-Z_{near}}
\end{aligned}
\end{equation}

Τέλος πρέπει να προσέξουμε το γεγονός ότι τα σημεία μπροστά από το σημείο παρατήρησης (viewpoint) στην OpenGL έχουν αρνητικές τιμές συντεταγμένων στον άξονα Z. Αφού εφαρμόσουμε το μετασχηματισμό που αναφέρθηκε με την προηγούμενη εξίσωση, μπορούμε να γράψουμε τις τιμές των δεδομένων στον z-buffer. Μόλις συμβεί αυτό, παρατηρούμε ότι τα πραγματικά και τα εικονικά αντικείμενα "δένουν" αρμονικά μαζί στη σκηνή με αρκετά μεγάλη ακρίβεια. 



\begin{figure}[H]
    \centering
    \includegraphics[width=0.8\textwidth]{Files/Figures/occlusion2.png}
    \caption[Το markerboard που δημιουργήθηκε μέσω της ArUco]{Το markerboard που δημιουργήθηκε μέσω της ArUco}
    \label{fig:occlusion}
\end{figure}


\section{Ενσωμάτωση Μηχανής Σκακιού}

Μετά την υλοποίηση του αλγορίθμου ανίχνευσης της χειρονομίας του "τσιμπήματος" και την αντιμετώπιση του προβλήματος απόκρυψης αντικειμένων, κρίθηκε απαραίτητο, ο χρήστης να μπορεί να παίξει ένα παιχνίδι σκακιού ενάντια στον υπολογιστή. Με αυτό τον τρόπο, πετχαίνουμε μία φυσική αλληλουχία γεγονότων στο παιχνίδι του σκακιού και προσμοιώνουμε ολοκληρωμένα ένα πραγματικό παιχνίδι σκακιού απέναντι σε έναν αντίπαλο, κάτι που θα μας βοηθήσει στη συνέχεια με την αξιολόγηση του συστήματος. 


Για να μπορέσει ο χρήστης να παίξει σκάκι ενάντια στον υπολογιστή, κανονικά θα έπρεπε να υλοποιηθούν αλγόριθμοι τεχνητής νοημοσύνης που θα επέτρεπαν στο πρόγραμμα μας να καθορίσει την επόμενη κίνηση του υπολογιστή με βάση τις προηγούμενες κινήσεις που έλαβαν μέρος κατά τη διάρκεια του παιχνιδιού. Ωστόσο η υλοποίηση τέτοιων αλγορίθμων για την εφαρμογή μας δεν βρίσκεται μέσα στους σκοπούς της παρούσας διπλωματικής εργασίας και έπρεπε να αναζητηθεί ένα απλός τρόπος για την ενσωμάτωση λειτουργιών τεχνητής νοημοσύνης. 


Εκεί ακριβώς προκύπτει η λύση των μηχανών σκακιού. Μία μηχανή σκακιού δεν είναι τίποτα άλλο από ένα πρόγραμμα υπολογιστή το οποίο δέχεται σαν είσοδο την θέση ενός πιονιού στη σκακιέρα, αναλύει τις θέσεις όλων των πιονιών και υπολογίζει την καλύτερη δυνατή κίνηση με βάση τη διάταξη των πιονιών στη σκακιέρα μέσα σε ένα χρονικό περιθώριο που ορίζεται εκ των προτέρων. Η μηχανή σκακιού μπορεί να εκτιμήσει τις επόμενες κινήσεις, αλλά συνήθως δεν αλληλεπιδρά απευθείας με το χρήστη, καθώς οι περισσότερες μηχανές δεν έχουν δική του γραφική διεπαφή για την επικοινωνία με το χρήστη. Αντίθετα οι περισσότερες είναι εφαρμογές κονσόλας που επικοινωνούν με μία γραφική διεπαφή μέσω ενός καθορισμένου πρωτοκόλλου. Αυτό επιτρέπει στο χρήστη να παίζει ενάντια σε πολλές διαφορετικές μηχανές χωρίς να χρειάζεται να εκπαιδευτεί εκ νέου σε νέες διεπαφές κάθε φορά και επιτρέπει σε διαφορετικές μηχανές να παίζουν η μία ενάντια στην άλλη. Στη συγκεκριμένη περίπτωση, η εφαρμογή μας παίζει το ρόλο της γραφικής διεπαφής ως τώρα. 


Στις μέρες μας, ο πιο πιο γνωστός τρόπος επικοινωνίας με μηχανές σκακιού πραγματοποιείται μέσω ενός πρωτοκόλλου που ονομάζεται Universal Chess Interface (UCI) Protocol. Πρόκειται για ένα πρωτόκολλο ανοιχτής επικοινωνίας που επιτρέπει σε μία μηχανή σκακιού ενός προγράμματος να επικοινωνεί με τη διεπαφή χρήστη μέσα από ένα σύνολο αυστηρά καθορισμένων εντολών. Επομένως, για να ενσωματώσουμε μία μηχανή σκακιού στην εφαρμογή μας, πρέπει να βρούμε ένα τρόπο ώστε να γίνει ανταλλαγή εντολών (συμβολοσειρές) ανάμεσα στην εφαρμογή μας και τη μηχανή σκακιού. 


Η μηχανή δέχεται εντολές μέσω standard input από μια εφαρμογή και παράγει τις απαντήσεις σε συμβολοσειρές του standard output. Δεν έχει δηλαδή γραφική διεπαφή, είσοδο μέσω ποντικιού ή εικόνες, παρά μόνο ένα απλό παράθυρο κονσόλας, που δεν είναι τίποτα περισσότερο παρά ένα εκτελέσιμο (.exe) αρχείο. Προκειμένου να υλοποιηθεί αυτή η επικοινωνία με το εκτελέσιμο αρχείο μιας μηχανής σκακιού αποφασίστηκε να αξιοποιηθεί η δυνατότητα που προσφέρει η βιβλιοθήκη Qt μέσω μιας κλάσης με το όνομα QProcess, η οποία επιτρέπει στο πρόγραμμά μας να εκκινήσει ένα εκτελέσιμο αρχείο, καθώς και να διαβάσει και να γράψει εντολές συμβολοσειρών από και προς αυτό. Η αλληλεπίδραση με τη μηχανή σκακιού ξεκινά με μία εντολή "uci", η οποία επικοινωνεί με τη μηχανή λέγοντας της να ταυτοποιήσει τον εαυτό της, δηλαδή να δώσει σαν έξοδο τα χαρακτηριστικά της όπως την ονομασία της, την έκδοσή της κλπ.  Έπειτα, δέχεται εντολές που μπορούν να αλλάξουν ορισμένες προεπιλεγμένες τιμές ιδιοτήτων που επηρεάζουν τα δεδομένα τα οποία θα προβάλλονται σαν έξοδος από τη μηχανή. Μετέπειτα, η μηχανή δέχεται ως είσοδο από την εφαρμογή μας την κίνηση που πραγματοποίησε ο χρήστης και εξάγει την επόμενη καλύτερη κίνηση που μπορεί να εκτελέσει ο αντίπαλος, δηλαδή ο υπολογιστής.
Η μηχανή σκακιού δέχεται σαν παράμετρο το χρόνο τον οποίο έχει για να υπολογίσει την κίνηση αυτή, ψάχνοντας για την καλύτερη δυνατή κίνηση. Ξεκινά την αναζήτηση και εκτιμά την καλύτερη κίνηση που υπάρχει πριν λήξει το χρονικό περιθώριο που ορίζεται στην αρχή της εκτέλεσης της μηχανής. 

Για την εφαρμογή μας, θεωρήσαμε ότι το χρονικό αυτό περιθώριο πρέπει να είναι ιδιαίτερα μικρό (40ms), ώστε να μην "παγώνει" η απεικόνιση των εικονικών αντικειμένων και να μπορούμε να παίρνουμε άμεσα feedback από το πρόγραμμα. Συνήθως οι μηχανές σκακιού δεν έχουν τη δυνατότητα να αντιλαμβάνονται αν μία εντολή κίνησης που δίνεται από το χρήστη είναι έγκυρη ή όχι, με βάση τον τύπο των πιονιών και την κατάσταση της σκακιέρας. Για το λόγο αυτό επιλέχθηκε να χρησιμοποιηθεί η μηχανή iCE [link], η οποία ελέγχει αν η κίνηση που γίνεται είναι επιτρεπτή ή όχι. Αν η κίνηση δεν επιτρέπεται, τότε η μηχανή επιστρέφει ως έξοδο τη συμβολοσειρά "Invalid chess move". Αυτή η ιδιότητα επιτρέπει την υλοποίηση μιας αρχιτεκτονικής στον κώδικά μας, η οποία δε θα είναι επιρρεπής σε λανθασμένες κινήσεις όταν αυτές γίνονται κατά λάθος από το χρήστη. Επιπλέον, η μηχανή που χρησιμοποιήθηκε μπορεί να ανιχνεύσει αν το παιχνίδι ολοκληρώθηκε και ποιος είναι νικητής ώστε να μπορούμε να ενημερώσουμε το χρήστη για το αποτέλεσμα του παιχνιδιού. 

Συμπερασματικά, υλοποίησαμε τη βασική λειτουργικότητα για την αποστολή και λήψη συμβολοσειρών προς και από τη μηχανή σκακιού και με βάση αυτές τις συμβολοσειρές το πρόγραμμά μας εκτελεί συγκεκριμένες δράσεις.










