3%*******10********20********30********40********50********60********70********80

% For all chapters, use the newdefined chap{} instead of chapter{}
% This will make the text at the top-left of the page be the same as the chapter

\chap{Ανάλυση \& Σχεδιασμός} \label{c:crypto}


\section{Η Συσκευή Intel RealSense\textregistered 3D F200 }

Intel RealSense\textregistered, formerly known as Intel Perceptual Computing,[1] is a platform for implementing gesture-based Human-Computer Interaction techniques. It consists of series of consumer grade 3D cameras together with an easy to use machine perception library that simplifies supporting the cameras for third-party software developers.[2][3][4]

The project can be considered a successor to Microsoft Kinect camera, which aims to find uses for the technologies pioneered there in the consumer market in applications beyond gaming, something which Kinect struggled with.[5]

As of March 2015, multiple laptop and tablet computer manufactures offer one or more devices with Intel RealSense camera built in. These are Asus, HP, Dell, Lenovo, and Acer.[6]

An Intel RealSense camera contains the following four components. A conventional camera, an infrared laser projector, an infrared camera, and a microphone array. The infrared projector projects a grid onto the scene (in infrared light which is invisible to human eye) and the infrared camera records it to compute depth information. The microphone array allows localizing sound sources in space and performing background noise cancellation.

Three camera models were announced, with distinct specifications and intended use.

Intel RealSense 3D Camera (Front F200)[edit]
This is a stand-alone camera that can be attached to a desktop or laptop computer.[7] It is intended to be used for natural gesture-based interaction, face recognition, immersive, video conferencing and collaboration, gaming and learning and 3D scanning.[8]

There is a version of this camera to be embedded into laptop computers.[9]

Specifications[edit]
Full VGA depth resolution
1080p RGB camera
0.2–1.2 meter range (Specific algorithms may have different range and accuracy)
USB 3.0 interface


Depth-Sensing Technology

RealSense cameras feature three lenses, a standard 2D camera for regular photo and video, along with an infrared camera and an infrared laser projector. The infrared parts allow RealSense to see the distance between objects, separating objects from the background layers behind them and allowing for much better object, facial and gesture recognition than a traditional camera. The devices come in three flavors: front-facing, rear-facing and snapshot. 

Right now, the front-facing cameras are most common, appearing on a number of new PCs and allowing all kinds of gaming, gestures and communication. Rear-facing cameras will arrive on tablets later this year and focus more on 3D scanning. The Dell Venue 8 7000 tablet is the first device with RealSense SnapShot, which is made just for still photography and allows you to change the focus or measure real distances in a photo after it has already been shot.

RealSense can add hand recognition to existing game systems such as the Oculus Rift VR glasses. Editor Mike Andronico played a game of volley ball on the headset, sticking his hands out in front of him to hit the ball. While Oculus has always provided the ability to play games in an immersive virtual world, it normally requires a controller. Adding Intel's depth-sensing cameras allows users to use their hands as the controller.


\section{Επιλογή Εργαλείων Ανάπτυξης}

There have been a number of supporting technologies developed for AR gaming. The most notable is the ARToolkit [Kato and Billinghurst 1999], as this is historically the most popular AR development library. ARToolkit’s public availability, ease of use,robustness, and low cost of entry, allowed for a rapid uptake of this technology.


\section{Multiple Marker Tracking}
\section{Αναγνώριση Χειρονομίας Τσιμπήματος}

Once the captured video frame has been stabilized and occlusion has been detected and defined in terms of binary blobs, the interaction problem becomes one of gesture recognition. As described in chapter 4, target occlusion is detected and defined relative to the target plane. Since all virtual augmentation is defined relative to the target plane, interaction between real and virtual objects can occur within this common coordinate system. One of the most significant contributions of this thesis is the following handbased interaction system using gesture recognition. Our goal is to provide a simple gesture recognition system for two-dimensional manipulative interaction. Currently, using a mouse to manipulate a window interface is commonplace. Our system provides a mouse-like gesture based interface to an immersed AR user without the need for the cumbersome mouse. To simulate a mouse requires the recognition of both point and select gestures in order to generate the appropriate mouse-down and mouse-up events at the indicated location. This goal is achieved without the need for a sophisticated gesture recognition system such as [OKA02] involving complex finger tracking for gesture inference through motion. Instead, the gesture model is specialized for the task of mouse replacement. Performing the gesture analysis in pattern-space simplifies the image processing and creates a very robust gesture recognition system.

In order to define the appropriate gestures, the requirements of the application must be defined in detail. The requirements of the gesture system discussed in this thesis are: • real-time performance • commercial pc and camera hardware • hand-based interaction without hardware or glove-based facilities The real-time requirement of the system poses great restriction on the level of gesture recognition that can be implemented. Commercial hardware may also limit system performance, as well as limit the quality of image capture on which all computer visionbased, image analysis techniques rely. The third requirement forces the use of computer vision to recognize hand gestures, which is performance bound by the processor. Given these restrictions an interactive application is described and a particular hand gesture model is defined. The goal of this interaction system is to provide the user with a virtual interface to control the augmentation system properties. In other words, the goal is to allow the user to change system parameters through gestures in real-time. The interface is designed to be a control panel that is augmented on the planar pattern. The user should be able to interact directly with this augmented control panel on the 2D planar pattern. This allows the user to directly manipulate the set of controls provided on the panel. The original 2D planar target pattern can be fixed in the environment or carried by the user and shown to the camera when the interaction is desired. For these reasons it is assumed that only one hand will be free to perform the gestures over the target pattern. With the application requirements described, a gesture model can be defined. Complex manipulation such as finger tapping can be recognized with the use of multiple cameras to capture finger depth information. However, under the constraints of a single camera system, the occlusion blob detection described in the previous chapter provides only two-dimensional information about the occluding hand. For this reason, the gesture language is based exclusively on hand posture. The hand is described in pixel-space as the union of the detected occlusion blobs (the occluder set found in chapter 4). Each blob representing a finger or a set of grouped fingers. Given that our goal is to replace a mouse, there are only two classifications to which the recognized hand postures can belong: a pointing posture and a selecting posture. The notion of pointing and selecting can vary between applications, so they must be clearly defined for each application. In this application, pointing is the act of indicating a location on the planar target relative to its top left corner. Selecting is the act of indicating the desire to perform an action with respect to the pointer location. In terms of the gesture model, the parameters associated with each posture are: a pointer location defined by the prominent finger tip and a finger count defined by the number of fingers detected by the system. With the gesture model defined, a gesture system can be constructed.

The gesture recognition system proposed in this chapter applies the defined gesture model to a working Augmented Reality application system. The system flow is shown in figure 5.1. The system begins by analyzing the captured video frame using computer vision techniques. At this point, posture analysis is performed to extract the posture parameters in order to classify the gesture. If classification succeeds, the recognized gesture is translated into the event-driven command understood by the interactive application.
\section{Ανίχνευσης Θέσης Τσιμπήματος}



To determine the location of the user’s point and select actions, a pointer location must be chosen from the hand point set. To simplify this process, the current system constraints were exploited and a number of assumptions were made. The first useful constraint deals with the amount of target occlusion permitted. The planar tracking system used for augmentation assumes that approximately half of the target corners are visible at all times during the tracking phase. To satisfy this constraint, only a portion of a hand can occlude the target at any given time. For this reason, the assumption is made that the only portion of the hand to occlude the target will be the fingers. From this we get: Assumption 1: Separated fingers will be detected as separate blobs in the image analysis phase. Due to the simplicity of the desired interaction, a second assumption was made: Assumption 2: Fingers will remain extended and relatively parallel to each other.

The simple gesture model introduced in this chapter describes two gestures classified by the interaction system – point and selection. The point gesture is the combination of a single finger and a pointer location. A single group of fingers along with a pointer location is also classified as the gesture of pointing. The selection gesture is the combination of multiple fingers and a pointer location. Figure 5.3 shows an example of these two gestures, displayed in pattern-space. A sample point and select gesture are shown in figure 5.4(a) and 5.4(b) respectively. These images are the grayscale representations of full colour screenshots. In this demonstration application the gesture system recognizes the colour region occupied by the finger pointer and also recognizes when selection has occurred. The fact that selection has been recognized from the two finger blobs is shown clearly in the text annotation at the top of the figure.

The interaction created by this gesture model is a point and select mechanism similar to the commonly used mouse interaction with a window-based operating system. To allow a closed system of human-computer interaction, the actions generated by the hand gestures define a set of system states. The possible states of the gesture system are pointing, selecting and no hand detection. The transitions between states are triggered by a change in finger count. This transition is represented by a pair of values, (cp,cc), indicating the previous and current finger counts. The possible values for cp and cc are 0, indicating no hand detection, 1, indicating a single detected finger pointer, and n, indicating more than one detected finger pointer. This state machine is shown in figure 5.5 and the system begins in the no hand detection state.