%*******10********20********30********40********50********60********70********80

% For all chapters, use the newdefined chap{} instead of chapter{}
% This will make the text at the top-left of the page be the same as the chapter

\chap{Μελλοντικές Επεκτάσεις}

Συμπερασματικά, τα Κβαντικά Αποτυπώματα αποτελούν μια κλάση κβαντικών καταστάσεων με τεράστιο ενδιαφέρον τόσο στη θεωρητική όσο και στην πειραματική Κβαντική Πληροφορία. Επιπλέον, μετά τη από τη θεώρησή τους από τη σκοπιά της Κρυπτογραφίας, έγινε προφανές ότι τα Κβαντικά Αποτυπώματα παρουσιάζουν βασικές ιδιότητες απόκρυψης πληροφορίας και μπορούν να χρησιμοποιηθούν ποικιλοτρόπως στην κατασκευή κρυπτογραφικών πρωτοκόλλων. Ως καινοτόμο παράδειγμα προτείναμε την κατασκευή ενός σχήματος Κβαντικών Χρημάτων δημόσιου κλειδιού (public-key Quantum Money scheme) όπου τα Κβαντικά Αποτυπώματα χρησιμοποιούνται τόσο για την επαλήθευση του εκδότη του Κβαντικού χαρτονομίσματος μέσω του ψηφιακώς υπογεγραμμένου σειριακού αριθμού όσο και του κατόχου, δηλαδή της κβαντικής κατάστασης του χαρτονομίσματος. 

Ωστόσο, η πρότασή μας βρίσκεται ακόμη σε πρώιμο στάδιο και απαιτείται εντεταμένη θεωρητική εργασία για την εκτέλεση αποδείξεων ασφαλείας. Στην κατεύθυνση αυτή, πιστεύουμε ότι τα Αποκρύπτοντα Αποτυπώματα (hiding fingerprints) \cite{secrets} συσχετίζονται άμεσα με τους κρυμμένους υποχώρους (hidden subspaces) του Aaronson \cite{hidden}. Εάν αποδειχτεί αυτή η συσχέτιση, τότε οι αποδείξεις ασφαλείας και ορθότητας του σχήματος Κβαντικών Χρημάτων δημοσίου κλειδιού του Aaronson μπορούν να εφαρμοστούν και στο δικό μας σχήμα Κβαντικών Χρημάτων με Αποτυπώματα, καθιστώντας το έτσι απεριόριστα ασφαλές απέναντι σε κάθε γνωστή επίθεση εναντίων σχημάτων Κβαντικών Χρημάτων.

Βέβαια, τα εγγενή πλεονεκτήματα του σχήματος που προτείνουμε έναντι των υπόλοιπων προκύπτουν από τις πολλά υποσχόμενες πειραματικές υλοποιήσεις των Κβαντικών Αποτυπωμάτων με χρήση στοιχείων της Κβαντικής Οπτικής, όπως αναλύσαμε στο \cref{c:exper}. Ταυτόχρονα, όμως, πρέπει να εξεταστεί αν με την προτεινόμενη υλοποίηση μέσω σύμφωνων καταστάσεων (coherent states) παραμένουν ανέπαφες οι αποκρύπτουσες ιδιότητες των Κβαντικών Αποτυπωμάτων. Είμαστε αισιόδοξοι για μια θετική έκβαση, καθώς η θεωρία πίσω από τους τυχαίους ψευδο-γραμμικούς κώδικες των Gavinsky και Ito \cite{secrets} παρέχει απεριόριστη ασφάλεια ακόμη και αν ανακτηθεί από τον αντίπαλο ολόκληρη η κωδική λέξη π.χ. μετρώντας τη φάση κάθε παλμού σε ένα Οπτικό Κβαντικό Αποτύπωμα (βλ. \cref{c:exper}). Συνοψίζοντας, η εργασία αυτή αποτελεί μια προσπάθεια διεύρυνσης της ερευνητικής προσέγγισης για πειραματικώς υλοποιήσιμα πρωτόκολλα Κβαντικής Κρυπτογραφίας, που βασίζονται σε Κβαντικά Αποτυπώματα.



Based on the results obtained, the future work include several improvements to the current prototype and additional features to manage a seamless interaction in AR. The fully mobile compatibility is a goal that should be addressed, as mentioned previously, its integration on mobile devices would permit to design a variety of applications, and an early approach with AR is an area of value. The occlusion handling of real hand is an important next step to replace the virtual hand, Vuforia provides access to the background video, from which it is possible to use it as texture and get rid of the virtual hand by replacing a real hand visualization as texture by segmenting it and generating shaders from it. Increase the recognition of more gestures, based on the taxonomy for AR interaction reviewed in the chapter 3, which would consists on implement the gestures and create a gesture-manager to deal with the control and correct recognition of different gestures. Use of machine learning techniques to get better results on the gesture recognition. Although the gesture recognition is based on the pinch positions, machine learning techniques could be implemented to infer gestures in time, defining a ground truth data set with movement’s directions specific for the Leap Motion, where the raw data is not exposed but if we rely on its data, machine learning techniques could improve effectively the gesture recognition. Improve the visual feedback to enhance the user’s perception of the virtual world, as the treatment of shadows and illumination in the AR. There are some implementations already carried out to deal with this feature.


Many prototypes implemented with sophisticated computer vision algorithms to robustly recognize hand gestures have demonstrated that gesture recognition is rather complex and compu- tationally expensive. That is why our methodology is better, since we dont use many resources and it is not computationally expensive. Based on the results obtained, future work may include several improvements to the current prototype and additional features to manage a seamless interaction in AR and an even more immersive experience for the users. The whole pinch gesture detection algorithm has been designed and developed, so that it would be possible to integrate the Realsense Development Kit Camera on top of an Oculus Rift Device. Our algorithms take into account the egocentric viewpoint of the user and his distance from a real table, thus they can work correctly without any changes. In order to achieve the integration of Oculus into our program, one would have to render whatever the application renders, including the video stream from the Realsense Camera, into the texture of a Framebuffer object of openGL. Afterwards, this texture could be applied in a quad and this quad could be placed in a virtual scene made by OpenGL. This virtual scene and the quad that is located inside it would then have to be also copied in the texture of a framebuffer object and passed into the Oculus SDK for modi cations, such as distortion and creation of a stereo view from different viewpoints per eye. In order to get more accurate and robust results on the exact 3D position of the pinch-point which is detected when thumb and fore nger come together a ltering of frames could be implemented to infer gestures in time and this could improve effectively the interaction errors. One could also improve the visual feedback to enhance the users perception of the virtual world. The use of shadows and light source estimation for correct illumination of the scene and virtual objects can greatly affect the realistic rendering of the virtual content in augmented reality. Furthermore, in order for the user to better perceive the depth of the scene, enhanced visualizations can be utilized such as projections in all planes of the scene. In our application we have already used one such visualization by rendering a straight line of projection from the moving chess piece to the chessboard and it was obvious that users can move virtual chess pieces and better understand where the new position of the object is going to be. Finally, since we use 3D models for the virtual chess pieces, a broad collection of different gures could be used, so that user would be able to play chess with different sets each time, ranging from dragons to soldiers. Adding multiple sets of chess models could de nitely improve user experience, but in order to further improve the visual experience attack and death animations could be implemented. Instead of pieces disappearing, destruction through an explosion or decapitation would be even better. Taking everything into consideration, our approach seems to work pretty nicely and the user is able to play a chess game with virtual objects from the beginning to the end without any serious problems.

\section{Ενίσχυση της Οπτικής Αντίληψης}


Use of the tabletop as an AR gaming surface provides a number of interesting user interface opportunities for HMD-based systems. A clear option for AR gaming is the ability to make the game more animated. For example, the game ofWizard’s Chess depicted in the movie \textit{Harry Potter and the Philosopher’s Stone}(Warner Brothers Motion Pictures.) provides graphic animations of chess pieces fighting whenever a piece is captured. The use of AR allows for animated game pieces to interact with each other and with the players of the game. Animation could assist the following characteristics of the game: tuition on how to play the game, tactics, and current attributes for playing a piece (power level, strength, or ability).

An interesting feature of HMD-based AR is that different kinds of information may be displayed to different users. In the case of displaying current attributes, private information about a playing piece could be displayed to the user who controls the piece. This private information could include visualization of potential placement of pieces or future actions. Szalav´ ari et al. [1998] investigated these issues with an AR version of the classic Chinese game Mah-Jong. They employed face-mapping for quick and accurate placement of game pieces to improve the game experience. Because Mah-Jong requires both public and private information, they developed what they term a powerful automatic privacy mechanism. The players hold the private information on handheld PIPs (personal information panels). For this game, a PIP is a magnetic, tracked passive prop. The game is played at a table and AR displays all the game pieces. The AR for this game is the combination of virtual game pieces and the physical game space (people and table).
A tabletop AR version of the fantasy game Jumanji, set in Singapore, was developed by Zhou et al. [2004]. Instead of employing dangerous creatures, the game virtually transports the user to Singapore shopping locations. The user employs dice with MXRToolkit3 fiducialmarkers (similar to ARToolkitmarkers) as the means for game control. The use of physical dice adds a nice tangible feel to the game. Minatani et al. [2007] developed an AR version of Othello with the ARToolkit as the tracking technology. What makes this a very interesting game is the fact that you play a remote user with a physical board. Your opponent and the opponent’s pieces are displayed to each player as virtual objects. The authors explored the rendering space to enable users to “feel” as if the other player was sitting at the same table.


\subsection{Eye Tracking}
Using tiny cameras to observe user pupils and determine
the direction of their gaze is a technology with potential for
AR. The difficulties are that it needs be incorporated into the
eye-wear, calibrated to the user to filter out involuntary eye
movement, and positioned at a fixed distance. With enough
error correction, gaze tracking alternatives for the mouse
such as Stanford‟s EyePoint17 [94] provides a dynamic history
of user‟s interests and intentions that may help the UI
adapt to the future contexts.
\subsection{Φωτορεαλιστική Απεικόνιση}

Το 2006, o Fischer et al. ανέπτυξαν ένα σύστημα ζωγραφικής επαυξημένης πραγματικότητας, το οποίο αλλάζει τα φυσικά και εικονικά αντικείμενα, με αποτέλεσμα να μπορούν να απεικονιστούν με 3 διαφορετικούς τρόπους: με τρόπο που θυμίζει γελοιογραφία που αποτελείται από χρωματιστές κηλίδες και σιλουέτες, με τρόπο που επιτρέπει τη χρήση μικρών πινελιών που προσομοιώνουν πουαντιγιστική τεχνοτροπία ζωγραφικής και, τέλος, ένα είδος ασπρόμαυρης τεχνικής απεικόνισης. Οι συγγραφείς ερεύνησαν έναν αριθμό διαφορετικών μεθόδων μη φωτορεαλιστικής απεικόνισης, με σκοπό να προσομοιώσουν διαφορετικές μορφές τέχνης και γελοιογραφιών [Fischer et al. 2005].


\subsection{Σκιές και Ανίχνευση Σύγκρουσης }


\subsection{Απτική Ανάδραση }

The haptic sense is divided into the kinaesthetic sense
(force, motion) and the tactile sense (tact, touch). Force
feedback devices like joysticks and steering wheels can
suggest impact or resistance and are well-known among
gamers. A popular 6DOF haptic device in teleoperation and
other areas is the PHANTOM (Fig. 11). It optionally provides
7DOF interaction through a pinch or scissors extension.
Tactile feedback devices convey parameters such as roughness,
rigidity, and temperature. Benali-Khoudja et al. [27]
survey tactile interfaces used in teleoperation, 3D surface
simulation, games, etc.
Data gloves use diverse technologies to sense and actuate
and are very reliable, flexible and widely used in VR for
gesture recognition. In AR however they are suitable only for
brief, casual use, as they impede the use of hands in real
world activities and are somewhat awkward looking for
general application. Buchmann et al. [37] connected buzzers
to the fingertips informing users whether they are „touching‟
a virtual object correctly for manipulation, much like the
CyberGlove with CyberTouch by SensAble15.

\section{Το μέλλον της Επαυξημένης Πραγματικότητας}

For Augmented Reality to become a mainstream tool, it must robustly provide useful information at rate that is synonymous with that of human sensory perception. The experimental results of this simple augmented interaction system provide evidence that real-time Augmented Reality is more than a theoretical vision. Using modern computer technology, it is clear that the first step towards the real-time computer perception of human behaviour can be taken. This can be as simple as the classification of basic human actions based on a pre-defined model or as complex as a continuous learning system able to mimic the communication performed by another human being. Many avenues are being explored in this field, all of which await the arrival of the required technology to process the observed information in real-time







There are some restrictions in AR. For example, a system is able to show the augmented view only from those viewpoints from where it has a real image. For example, the user can see a virtual building from ground level looking through a display, but is unable to see the scene from a bird's eye view. In order to provide such visualisations, applications often complement AR with virtual reality mode. Other limitations are due to restricted capacity of devices: their power consumption is too high and their processing, telecommunication and memory capacities are too low, the resolution of cameras is too low, etc. Engineers develop new and better devices, and the capacity of devices increases, they have more built-in sensors, therefore future devices will solve many of current obstacles. Cloud services will in turn help with computationally intensive tasks in future.

From the application developers’ point of view, the diversity of platforms is problematic: they need to port the application to different platforms, as a lot of the code is platform-dependent. HTML5 will be a step towards device-independent applications. It is supported by a large number of mobile devices. It enables presenting videos and audio on web pages as easily as presenting text and images is now, without a need for any plug-ins [271]. HTML5 offers a number of useful properties such as the user’s locations based on GPS or WLAN, canvas technology for dynamic graphics, etc. Although HTML5 is already available for application developers, the standard will be finished probably only on 2014 in W3C. Integration on global databases such as Google Earth, with GPS and local information for example from security cameras together with cloud computing, could lead to real applications similar to the one presented in [272], where the AR navigator is able to visualise cars coming out of the driver’s view (e.g. behind buildings). Another new type of application is security guard guidance system that is able to visualise people behind or inside buildings using AR as means for visualising information from security cameras. The usability of the see-through-devices needs to be improved before they are suitable for mass-market applications, especially the head-mounted see-through devices, which are clumsy as we discussed in Section 7.3. In future, the seethrough portable devices, such as the See-Through Laptop Samsung presented in 2010, might provide a better platform for mobile AR applications (see Figure 97).


Virtual retinal displays that render images with a laser directly on the user’s retina may become more common (see Section 8.2). Another alternative is the use of augmented reality contact lenses as for example envisioned in [274, 275] In future users may use devices intuitively by bending, twisting and squeezing, e.g. the Nokia Kinetic phone presented at Nokia World 2011. This kind of user interface works even if the user wears gloves in cold weather, where touchscreens are unpractical. The displays may be flexible and foldable, such as Polymer Vision’s Readius presented in 2008. Samsung has announced that their new mobile device line-up will feature flexible screens starting in 2012. The whole surface of a mobile device could be touchscreen as in Nokia’s GEM phone concept [276]. While waiting for reasonably sized foldable mobile device with interactive surface (and long-lasting batteries), people might find tablets to be a nice platform for AR applications: they have bigger screens than phones for visualisations, they are lighter than laptops, and they have built-in cameras and sensors. In the early days of virtual reality, Ivan Sutherland visioned “The ultimate display would, of course, be a room within which the computer can control the existence of matter. A chair displayed in such a room would be good enough to sit in. Handcuffs displayed in such a room would be confining, and a bullet displayed in such a room would be fatal.” [277]. It is hard to believe that researchers will ever build a virtual environment that will fulfil the last sentence. One of the exact benefits of virtual and augmented reality is that they enable safe environment for training and experiencing situations that would be too dangerous in reality. On the other hand, researchers have developed such physically altering environments that Sutherland visioned. Physically rendered environment is an environment where the system can alter the physical environment and manipulate grids of “moving physical pixels” (“moxels”). The system can raise and lower moxels and render physical shapes. It is a world-size version of the 3D pin art table where the user presses an object on the pins that will raise and create a 3D replica of the object’s shape. Today the quality of such systems is poor and their density is low. Furthermore, most current systems are able to model only vertical shapes, e.g. the Holodec presented in [278]. 3D holograms, furthermore, are able to visually render arbitrary 3D shapes, but without interaction possibilities. The University of Arizona presented a dynamic 3D hologram, which allows the three-dimensional projection, without the need for special eyewear. The resolution and speed (refreshes every two seconds) leave space for improvement, however [279–281]. In future we will probably see interactive large-scale 3D augmentations, virtual or physical. This will bring telepresence to a different level and enable new possibilities for mixed reality environments, e.g. telesurgery. Besides large 3D mixed environments, there is a trend towards mobile handheld AR with projective and 3D reconstruction capabilities.

Small projectors have also been demonstrated for augmented reality [282]. The integration of small projectors to mobile devices will give a boost for hand-held projective AR. As the development of the mobile devices continues (e.g. processing capacity and battery life), it will open way to applications that are currently computationally too demanding, such as robust feature tracking on mobile phones. In addition, 3D reconstruction will become feasible on mobile devices. Researchers have already demonstrated it using a mobile phone [283, 284]. The first mobile devices with a 3D display are already on the market; we may assume that they will become more common in the future. Easy mobile 3D reconstruction enables a new type of interaction with virtual worlds: people can scan real objects and bring them to virtual world (Second Life, Habbo Hotel, etc). The development of augmented reality enables a new type of interactive TV production; one of the first examples was BBC’s Bamzooki, an augmented reality TV game show that aired in 2009, where participants on the show shout instructions to control virtual autonomous game creatures called Zooks. It is easy to imagine that this kind of TV production will become more common in future. Current technology enables also real-time virtual staging (augmented virtuality), where the whole environment is virtual with real people acting in it. For example, Aalto University Media Centre Lume has a studio that enables this kind of production (see Figure 98). Future AR will bring film effects to live TV broadcasts.

Existing mobile object/image recognition services such as Google Goggles [247], Kooaba [290] and Snaptell [291] give us only a hint of all the future possibilities. One problem with current solutions is the long lag, typically tens of seconds, between snapping the picture and receiving the response to the query [292]. It is possible to speed up detection using low bit-rate local descriptors and data compression [292], but a better transmission rate and better indexing of data also enable faster queries. The business model of search software is also altering. The SmartAds service that Kooaba offers is an example of the Software-as-a-Service (SaaS) model. The user is charged for access and use of the search and recognition engine. It allows customers to turn their printed products into virtual hyperlinks to additional information [293]. Another new type of AR application is CrowdOptic [294], which received the Frost & Sullivan 2011 Annual Innovation Award for live-event technology. It is an AR application bound to certain events, e.g. football matches, concerts, etc. It lets users point their smartphones at an athlete or performer and see additional information about the target in real time. Users obtain coaching insights and the stats of the target, and they receive exclusive invitations, ticket discounts and other material through the system. In addition, the event organisers get information on crowd behaviour. The system detects where the attention of people is at any given moment, using GPS data, triangulations and analysing what is being photographed or videoed. Organisers can immediately consider the crowd’s interests in the production. We could call this real-time crowdsourcing or a crowd behaviour analysis tool. The trend in mobile AR browsers is to link information with other social media, use other technologies such as object recognition and face detection, etc. They take advantage of additional sensors and use remote computing and data storage facilities. Future mixed reality concept probably connects location-based services, usercreated content, social media, etc. with the physical world (magazines, billboards, buildings, places, etc.) It allows users to comment, share and link ideas, as well as attach bookmarks, tag physical objects and get information related to them. It provides a platform for interact with the concept of internet of things, where all objects are networked with information and services.

Future augmented reality development interconnects with robotics in several levels. ARDrone (see Figure 101) is a flying iPhone accessory (helicopter) equipped with a multitude of sensors and intelligence. It is controlled via iPhone using simple upper-level commands such as forward, rotate, up, hover, land, take-off, etc. It turns commands into signals for the four on-board motors using information from various sensors such as accelerometer and gyros. Several ARDrones operate in the same environment and users can play augmented reality games with them. It is easy to invent a useful application for such small robots. As we mentioned earlier (in Section 8.3.6) augmented reality has proved useful as an aid to robot operators. One straightforward idea is to use small remote-operated maintenance robots, which are able to augment maintenance instructions and other information for the human operator. Another idea is to use small autonomous robots capable of object recognition to search for missing items.

In James Cameron’s film Avatar (2009), humans are mining a valuable mineral on Pandora, an Earth-like moon with an atmosphere poisonous to humans. The venue is in Alpha Centauri star system in far future. Pandora is inhabited by the Na’vi, three-metre-tall, blue-skinned humanoids. In order to explore Pandora scientists create Na’vi-Human hybrid bodies, which a genetically matched human can mentally operate. The human operating a hybrid body has an illusion of being inside the avatar body, although lying in an avatar link tank at the base. This kind of avatar technology is far in the future, but perhaps not as far as we might think. Researchers have demonstrated that it is possible to create a perceptual illusion of body swapping, i.e. being in a body other than one’s own [295]. The illusion of being in a different body is possible to achieve even with an artificial body of extreme size. Test persons experienced being Barbie (30 cm) as well as a large doll (4 m) in the experiments reported in [296]. The key factor affecting the perception of being in another body is synchronous multisensory input: the user needs to feel touch when she or he sees the artificial body touched. In the abovementioned Barbie/doll experiment, the test person was lying on a table with a head-mounted video display. On the other table was a doll (of a different size). The camera providing the video images for the test person was mounted on a tripod on the place where the dolls head would have been, facing the body. This way the user had the feeling of looking at the doll’s body from a first-persons view. The multi-sensory input was created by touching the doll’s body (in view) and simultaneously touching the participant’s body (out-of view) at the corresponding location. Based on these findings it would be possible to create an illusion of being inside a virtual avatar as well. A human would explore the world through a virtual avatar’s eyes (and see the virtual body from a first-person perspective). With a haptic suit, it would be possible to “feel” virtual collisions that the avatar experiences. This could be the future of Second Life. Furthermore, in the field of Brain-Computer Interface (BCI) and Brain-Machine Interface (BMI), people have been able to create direct neural interfaces to operate artificial limbs, for example (e.g. [297, 298]). Studies with monkeys (which have neural systems that are considered to be very similar to those of humans) demonstrate the ability to control a computer or a robotic arm with their thoughts [299]. In addition, modern humanoid robots such as ASIMO [300] are able to mimic human movements: walk, run, climb stairs, grab with a hand, etc. Creating a real avatar experience becomes a matter of cross-disciplinary cooperation in the future. In principle, people could merge all these technologies and use direct neural interface to operate a humanoid robot. The robot would have cameras on head and these cameras would provide the view for the user (rendered using a retinal display). The robot’s microphones would record audio, which is played to the user’s headphones. The humanoid robot would naturally be equipped with a number of sensors, and their input would be transferred to the user using a multi-sensory (haptic, thermal, etc.) suit.
Embodied and tangible haptic input/output devices enable transferring sense of touch between the virtual and real world. The person using such a device is able to sense physical interaction with a remote or virtual person. Multi-sensory environments are a substantial research area. For example, researchers in CUTE centre (National University of Singapore and Keio University) and MXR Lab in Singapore have done a lot of research in the area of multisensory environments. Huggy pajama [301] is one of their research projects (see Figure 102), where the system is able to transfer a hug over the internet. The user at the other end touches an input device embedded with sensors. The information is transferred to the other end, where the other user receives the same touch from an output device equipped with a haptic interface. Kissenger (aka Kiss Messenger) is a similar system; with special input/output devices, it is able to transfer a kiss over the internet [302] (see Figure 103).Augmented reality systems most commonly employ haptic, visual and audio sensory feedback. The immersive feeling increases if even more senses receive input from the system.Sense of taste is composed of five basic tastes (sweet, salty, bitter, sour, umami), which are relatively easy to produce by blending the appropriate chemicals. Nonetheless, the actual gustation is a combination of taste, smell and food texture, plus some other factors. Therefore, it is possible to bluff sense of taste to some extent with scent and visuals as in the Meta Cookie demonstration we discussed in Section 2.5.2. However, humans can recognise thousands of different smells and are able to detect smells even in infinitesimal quantities. Therefore, it is impossible to produce a set of primary smells to produce all possible smells for the virtual environment (unlike primary colours (red, green, blue) for sense of sight). Sensation is ultimately formed in the human brain when the brain analyses electrical pulses coming from sense organs. Therefore senses can be provoked digitally, by feeding electrical pulses to sensory nerves. The digital taste interface presented in [304] produced sense of taste by actuating the tongue through electrical and thermal stimulations. The experimental results suggested that sourness and saltiness are the main sensations that could be evoked while there is evidence of sweet and bitter sensations too. In medical science, devices enabling digitally produced vision and audition have been used for over a decade (e.g. cochlear implants for deaf). Today’s multi-sensory interface devices are still clumsy and it will take a while before the input/output devices are mature enough to feel natural. In the future, digital gustatory devices become more accurate, and perhaps researchers will be able to produce a substantial amount of digital odours as well support a wide variety of virtual tastes and odours. The haptic devices will improve, immersive display systems become feasible, etc. The way users experience virtual environments is going to change. In future multi-sensory mixed reality environments people will be able to sense temperature, touch, taste, smell, etc., and the whole of the atmosphere. Such environments will support immersive vision and sound systems, and people will be able to sense physical interaction with virtual characters. Today people share their experiences in social media; they send multimedia messages; they use mobile video conferencing to show what they see (the “see-what-I-see” paradigm). The future mixed reality aims to enable a “sense-what-I-sense” paradigm. Telepresence is brought to a new level. People accept the idea that they could love or be loved by a robot [305], and people fall in love with celebrities they have never personally met. It is not farfetched to imagine an intimate relationship with a virtual character. The future multi-sensory mixed reality environment will be able to provide a “multi-sensorysecond- life” where people can interact, communicate and live together with avatars mastered by other humans, robots or computer. Naturally, this development will provoke some ethical issues, which we leave open in this discussion. Furthermore, researchers have been able to reconstruct images that people have seen from brain activity using functional magnetic resonance imaging (fMRI) [306]. The future brain interface could be bidirectional; the system reads from the user’s brain what she/he senses and simulates the other user’s brain accordingly. This would really be sharing experiences.


%thomas cie
My experience with the limitations of current technology is similar to problems reported in the literature. The availability of affordable sensors with the required precision and accuracy has been and still is a real issue. There is no sense in developing gaming technology that is way beyond the price of current consumer-grade technology. I found that the collaboration between technologists and game design/artists creates successful games. As with any electronic gaming, this process is a fusion of the power of modern computing technological advances and creative graphics, storytelling, gameplay, and design. Technology is never going to outperform good gameplay. New technology will enable game designers to develop different and innovative gaming styles


%krevelen
Imagine a technology with which you could see more than
others see, hear more than others hear, and perhaps even
touch, smell and taste things that others can not. What if we
had technology to perceive completely computational elements
and objects within our real world experience, entire
creatures and structures even that help us in our daily activities,
while interacting almost unconsciously through mere
gestures and speech?

Augmented reality (AR) is this technology to create a
“next generation, reality-based interface” [77] and is moving
from laboratories around the world into various industries
and consumer markets. AR supplements the real world with
virtual (computer-generated) objects that appear to coexist in
the same space as the real world. AR was recognised as an
emerging technology of 2007 [79], and with today‟s smart
phones and AR browsers we are starting to embrace this very
new and exciting kind of human-computer interaction


%krevelen
AR has come a long way but still has some distance to go
before industries, the military and the general public will
accept it as a familiar user interface. For example, Airbus
CIMPA still struggles to get their AR systems for assembly
support accepted by the workers [163]. On the other hand,
companies like Information in Place estimated that by 2014,
30\% of mobile workers will be using augmented reality.
Within 5-10 years, Feiner [57] believes that “augmented
reality will have a more profound effect on the way in which
we develop and interact with future computers.” With the
advent of such complementary technologies as tactile networks,
artificial intelligence, cybernetics, and (non-invasive)
brain-computer interfaces, AR might soon pave the way for
ubiquitous (anytime-anywhere) computing [162] of a more
natural kind [13] or even human-machine symbiosis as
Licklider [99] already envisioned in the 1950‟s.