%*******10********20********30********40********50********60********70********80

% For all chapters, use the newdefined chap{} instead of chapter{}
% This will make the text at the top-left of the page be the same as the chapter

\chap{Αξιολόγηση \& Συμπεράσματα}

%wang-popovi - 6d hands-χρησιμοποιεί διπλό view για να μη χανει το χερι
 However, all three approaches rely on
a single-view pinch detection technique that suffers from occlusions, restricting the hand orientations that can be tracked.
A unique feature of our approach is the use of two widebaseline
viewpoints. Our two-view approach resolves occlusions
from one view using information from the other, enabling
robust gesture (e.g. pinch) detection. Our contribution
is independent of the particular type of camera used (depth
or RGB).


%---
In this work, we have presented a thorough overview of the theory and applications of augmented reality. We have concentrated on marker-based tracking and lightweight single-camera approaches, but also gave an overview of alternative tracking methods and referred to how additional cameras, sensors and other devices are used in different types of AR applications. We discussed the ways in which basic AR applications can be enhanced and the ways in which interactions between real and virtual objects can be handled. In addition, the appendices give a comprehensive review of theoretical background of methods and algorithms used in augmented reality. We have also presented how the author has contributed to different issues in AR application development. In addition, we have reported practical experiences in AR application development and usability issues. Furthermore, we reported our research results in many areas of augmented reality. In the previous chapter, we discussed AR application development and application areas in which the use of AR is beneficial, and finally we had a glance at future possibilities of AR. In the following, we summarize the main issues of AR application development and design.

In the current work we have presented the design and implementation of an AR interface to interact with virtual content by means of a pinch gesture performed with bare hands, using the Leap Motion Controller -a depth sensing device- to obtain the information of hands, fingers and articulations. Firstly, we have introduced the basic concepts of interaction in AR, gesture recognition and its acceptance for interaction. A review of different approaches was shown, finding scarce implementations with this specific device, most of the research found included computer vision techniques applied with depth cameras that usually expose raw data while the device delivers the information of hand and fingers directly. Afterwards, we defined a set of guidelines to follow based on perceptual issues and recommendations from previous works, a description of the development frameworks and an architecture to integrate the AR and hand tracking technologies, including the definition of an algorithm to recognize pinch gestures through the device. An evaluation was conducted among different users in order to test the performance of the prototype with a simple grab-move-release task designed for this purpose. We found positive results related to the gesture but various issues in the AR perception. The Leap Motion’s technology is very promising in the sense that it has a potential for wide range of applications for gesture interaction, virtual and augmented reality environments and more robust and serious applications as sign language detection or control. Combined with an AR scenario, is potentially useful for virtual modelling and prototyping, collaborative environments or gaming.

\section{Παρατηρήσεις}


From the results and findings described previously, on the Test No. 1 5.1, there was no significant difference on time between the pinch-release movement task performed with both algorithms, invalidating our first thought, however, after looking at the learnability aspect in the pinchrelease movement within our implementation, which turned out to be non-significant as well, the research focus on other clues that could sustain our hypothesis: The user’s interaction within the virtual space improves easily with the time, chapter 1, leading to verify the additional data collected and measure the time when a hand is recognized and the pinch gesture is performed. The new results showed that our algorithm performed well along the attempts although there were random results in the attempts that does not reflect precisely if it was easier or not, - in general, this behaviour was seen in both cases- concluding that actually there is a decrease on time of each attempt when performing the gesture, validating the learnability aspect in the gesture performance, but not when moving the object to the specified area, in this case, the results were very smooth, meaning that the average time it took to grab-move-release was almost the same in all the samples. Furthermore, when comparing these results within both algorithms we found that there was a relevant improvement with our algorithm, validating our first assumption made, but on different events in the scene. On the qualitative results, the agreement with the learning effect statement is mutual among the users that stated to improve at some level on each attempt while on the SUS, the detection of problems that influenced our results corroborates the dispersion of data we have, specially during the grabbing task -that can be inferred from the time it took- that it was more difficult to initiate the grabbing action than manipulating the object with it. One problem that affected the performance initially was the depth perception of the users, where the point of view of the camera did not help the user to perceive the exact location of the objects in the z-axis(depth) and y-axis(height), even when the farther objects were smaller than the closer ones, it was observed that users struggle with this problem frequently; just after recognizing the environment, users take the tabletop marker as a reference to infer the locations above it, this achievement is important, as it shows the inner relationship between the real and virtual worlds perceived by the users, however, the awareness was not immediate and in this sense, more visual cues should guide the user through the augmentation to offer the perception of depth in a proper way like illumination and shadowing[55] or meshes on the scene[32]. Although the occlusion problem is not considered in this project, little evidence of its awareness should be taken into consideration. During the tests, we could see that there is an immediate and sufficient awareness from the users that a virtual hand is drawn on the top of their real one and it is controlled by their hand movements while other virtual objects react according to it. However, few participants initially tried to grab the objects focusing on their real hand rather than using the virtual, hence identifying the problem of occlusion at the time of trying to grab the objects and decreasing the level of immersion at a first stage. The use of just a real hand would improve the sensation of grabbing the object as in the real world. Lastly, the results of the second test shows the effect of the limits given by the interaction space of the device itself where there is a progressive increase of time to move objects around the center of the scenario, pointing increasing time intervals while moving objects on the boundaries of the interaction space.





We have designed and developed a prototype for a Gesture-based AR application, Despite it’s in an early stage, it could be used as basis for future developments in this area. Furthermore, a proposed integration of an architecture for mobile devices was designed and presented as a proof of concept where the data is sent through wireless connection, this will be useful in the near future, as currently development is this area is being carried out 1. To support the development, we have reviewed and analysed the capabilities of the device, based on few works published at the moment and presenting our experimental findings in the field of AR. Additionally, we conducted a usability study SUS to verify the user experience issues related and serve as basis for further improvements.

Σκοπός της παρούσας διπλωματικής εργασίας είναι η ανάπτυξη εφαρμογών επαυξημένης πραγματικότητας βάσει επίπεδου προτύπου, με αξιοποίηση μεθόδων και αλγορίθμων της φωτογραμμετρίας και της όρασης υπολογιστών. Στο πλαίσιο αυτό, έγινε μία περιγραφή της έννοιας της επαυξημένης πραγματικότητας, παρουσιάστηκε το θεωρητικό υπόβαθρο των διαδικασιών οι οποίες χρησιμοποιήθηκαν για την περάτωση των εφαρμογών και έγινε αναφορά στο προγραμματιστικό μέρος των εφαρμογών (περιβάλλον ανάπτυξης, γλώσσα προγραμματισμού και χρησιμοποιούμενες βιβλιοθήκες) ώστε στο τελευταίο κεφάλαιο της εργασίας να παρουσιαστούν τα αποτελέσματα των εφαρμογών και η ακριβής διαδικασία που ακολουθήθηκε. Έχοντας ολοκληρώσει την ανάπτυξη των εφαρμογών, κρίνεται αναγκαία η επισήμανση κάποιων παρατηρήσεων και συμπερασμάτων καθώς επίσης και διαφόρων προτάσεων για μελλοντική ενασχόληση και βελτίωση αυτών. Τα αποτελέσματα των εφαρμογών κρίνονται ικανοποιητικά. Τόσο η εφαρμογή επαυξημένης πραγματικότητας εσωτερικού χώρου όσο και η αντίστοιχη εξωτερικού χώρου αναγνωρίζουν επιτυχώς το πρότυπο επίπεδο αντικείμενο όταν αυτό τοποθετηθεί στο οπτικό πεδίο της κάμερας, ακόμη και στην περίπτωση κατά την οποία απεικονίζεται στη σκηνή ένα τμήμα του. Επίσης, η αναγνώρισή του γίνεται ανεξάρτητα από τις συνθήκες φωτισμού, τον προσανατολισμό του πρότυπου αντικειμένου και το μέγεθός του. Η σωστή αναγνώρισή του – η οποία οφείλεται κυρίως στο χρησιμοποιούμενο αλγόριθμο ανίχνευσης και περιγραφής των σημείων ενδιαφέροντος – σε συνδυασμό με την ορθότητα των αποτελεσμάτων της βαθμονόμησης είναι τα δύο βασικότερα στοιχεία που κρίνουν την επιτυχία επαύξησης του πραγματικού κόσμου και τη ρεαλιστικότητα των επαυξημένων σκηνών. Εξάλλου, η ρεαλιστική επαύξηση της πραγματικότητας, δηλαδή η ενσωμάτωση του εικονικού μοντέλου στον τρισδιάστατο χώρο έτσι ώστε να μην ξεχωρίζει από την πραγματική σκηνή αλλά να αποτελεί ένα με αυτή, είναι ένας από τους σημαντικότερους στόχους των εφαρμογών επαυξημένης πραγματικότητας, ο οποίος επιτεύχθηκε σε υψηλό βαθμό στην παρούσα εργασία. Μη ρεαλιστική είναι η επαύξηση της σκηνής στην περίπτωση που ένα αντικείμενο του πραγματικού κόσμου τοποθετηθεί μπροστά από τη θέση στην οποία προορίζεται να τοποθετηθεί το εικονικό μοντέλο, προς την πλευρά της κάμερας. Η ιδανική λύση θα ήταν η απόκρυψη του τμήματος του τρισδιάστατου μοντέλου επαύξησης που κρύβεται από το πραγματικό αντικείμενο. Ωστόσο, στις εφαρμογές που προγραμματίστηκαν, κάθε στιγμιότυπο της πραγματικής σκηνής ορίζεται ως εικόνα που καλύπτει τη σχεδιαστική επιφάνεια του παραθύρου επαύξησης της πραγματικότητας, αποτελώντας το φόντο του, χωρίς να εξάγεται από αυτό η πληροφορία της τρίτης διάστασης. Η τελευταία θα μπορούσε να χρησιμοποιηθεί στον ορισμό των αποκρύψεων. Συνεπώς, μία ενδιαφέρουσα μελλοντική επέκταση των εφαρμογών είναι η κατασκευή σε πραγματικό χρόνο του χάρτη βάθους (depth map) για κάθε στιγμιότυπο, για τον ορισμό των τμημάτων των εικονικών μοντέλων τα οποία αποκρύπτονται από τα αντικείμενα του πραγματικού κόσμου. Οι εφαρμογές που αναπτύχθηκαν έχουν ικανοποιητικά αποτελέσματα στην περίπτωση που το πρότυπο αντικείμενο είναι επίπεδο ή έχει μικρό ανάγλυφο σε σχέση με την απόσταση λήψης, λόγω της γεωμετρικής σχέσης του δισδιάστατου προβολικού μετασχηματισμού που θεωρήθηκε ότι συνδέει την εικόνα - πρότυπο και το εκάστοτε στιγμιότυπο. Συνεπώς, μία μελλοντική επέκταση των δυνατοτήτων των εφαρμογών θα μπορούσε να είναι η επαύξηση μη επίπεδων σκηνών του πραγματικού κόσμου, ανεξάρτητα από τη γεωμετρία τους. Η λύση σε ένα τέτοιο πρόβλημα μπορεί να προκύψει από την επιπολική γεωμετρία. Η εφαρμογή επαυξημένης πραγματικότητας εσωτερικού χώρου, η οποία υποστηρίζει τη συμπλήρωση της ορθοφωτογραφίας με την πληροφορία της τρίτης διάστασης, ταιριάζει με το μέσο που χρησιμοποιήθηκε για την επαύξηση σε πραγματικό χρόνο, δηλαδή την κάμερα του υπολογιστή, καθώς ο χρήστης μπορεί εύκολα να τοποθετήσει την εκτυπωμένη ορθοφωτογραφία μπροστά από την κάμερα του υπολογιστή του και να εξετάσει το ανάγλυφο της συγκεκριμένης περιοχής. Αντίθετα, δεν θα ήταν διατεθειμένος να μεταφέρει το φορητό υπολογιστή του στον πολυχώρο «Τεχνόπολις» ή στην περιοχή του νέου Μουσείου της Ακρόπολης για να παρακολουθήσει σε ζωντανό χρόνο την επαύξηση της πραγματικότητας στις δύο αυτές τοποθεσίες. Για το λόγο αυτό, η Εφαρμογή 2 δεν προγραμματίστηκε ώστε να είναι πραγματικού χρόνου και προϋποθέτει από το χρήστη να έχει εγγράψει ένα βίντεο μέσω της κινητής συσκευής του και εν συνεχεία να παρακολουθήσει, από το χώρο στον οποίο βρίσκεται ο υπολογιστής του, τις επαυξημένες σκηνές. Ωστόσο, θα ήταν προτιμότερη η επαύξηση της πραγματικότητας σε πραγματικό χρόνο μέσω μίας κινητής συσκευής χειρός. Έτσι, μελλοντικός στόχος είναι η τροποποίηση των εφαρμογών ώστε να εκτελούνται και σε ένα κινητό τηλέφωνο ή σε ένα tablet υπολογιστή, δηλαδή σε συσκευές χειρός που μεταφέρονται εύκολα και μπορούν να χρησιμοποιηθούν χωρίς δυσκολία από χρήστες εφαρμογών επαυξημένης πραγματικότητας. Άλλη πιθανή βελτίωση - διαφοροποίηση των εφαρμογών θα μπορούσε να είναι η εισαγωγή από το χρήστη της εικόνας - πρότυπο και του τρισδιάστατου μοντέλου και ο καθορισμός από αυτόν της επιθυμητής θέσης του τελευταίου σε σχέση με το πρότυπο αντικείμενο. Παράλληλα, η υποστήριξη της δυνατότητας βαθμονόμησης της κάμερας, με την οποία λαμβάνονται τα στιγμιότυπα προς επαύξηση, στην αρχή των εφαρμογών, μέσω λήψης βίντεο μίας σκακιέρας, θα ήταν μία ακόμη χρήσιμη συμπλήρωση των εφαρμογών.Η συμβολή των εφαρμογών επαυξημένης πραγματικότητας και η χρήση τους σε διάφορους τομείς έχει ήδη παρουσιαστεί στο πρώτο κεφάλαιο της παρούσας εργασίας. Όσον αφορά στις συγκεκριμένες εφαρμογές, η Εφαρμογή 1 μπορεί να χρησιμοποιηθεί για την οπτικοποίηση πληροφοριών τρίτης διάστασης και για την εξέταση του αναγλύφου μίας περιοχής με τρόπο παραστατικό, χωρίς να υφίσταται η ανάγκη τρισδιάστατης εκτύπωσης του ΨΜΕ. Μία πιθανή επέκταση της εφαρμογής θα μπορούσε να περιλαμβάνει την εισαγωγή διαφορετικών ΨΜΕ και πρότυπων εικόνων και την αυτόματη αναγνώριση της συγκεκριμένης ορθοφωτογραφίας ή άλλου είδους χάρτη που βρίσκεται στο οπτικό πεδίο της κάμερας, προκειμένου να επαυξηθεί με το κατάλληλο ΨΜΕ. Έτσι, θα μπορούσε να χρησιμοποιηθεί στη χαρτογραφία, καθώς παρέχει ένα ρεαλιστικό και καινοτόμο τρόπο αναπαράστασης της τοπογραφίας μίας περιοχής. Επίσης, θα μπορούσε να δώσει σε μαθητές τη δυνατότητα να εξετάζουν το ανάγλυφο περιοχών που απεικονίζονται στα σχολικά βιβλία γεωγραφίας τους, με τη χρήση της οθόνης ενός υπολογιστή. Επιπλέον, θα μπορούσε να χρησιμοποιηθεί από τουρίστες για την τρισδιάστατη παρατήρηση περιοχών που παρουσιάζουν ιδιαίτερο ενδιαφέρον και απεικονίζονται σε τουριστικούς χάρτες. Η χρησιμότητα της Εφαρμογής 2 θα μπορούσε να συνοψισθεί στην εξέταση αν τα συγκεκριμένα μοντέλα θα μπορούσαν να τοποθετηθούν στις καθορισμένες από την εφαρμογή θέσεις, από αισθητικής άποψης. Ωστόσο, τόσο τα μοντέλα όσο και οι περιοχές που χρησιμοποιήθηκαν είναι ενδεικτικά και περισσότερη σημασία δόθηκε όχι σε αυτά, αλλά στη διαδικασία που ακολουθήθηκε. Έτσι, ανεξάρτητα από τα συγκεκριμένα μοντέλα και τις αντίστοιχες περιοχές, ενδιαφέρουσες χρήσεις της εφαρμογής θα μπορούσαν να είναι η επαύξηση ενός κτηρίου με τμήμα του το οποίο υπήρχε στο παρελθόν αλλά σήμερα έχει καταστραφεί, ή η τοποθέτηση ενός κτηρίου, που δεν έχει ακόμη κατασκευαστεί, στη θέση στην οποία σχεδιάζεται να τοποθετηθεί, για να εξεταστεί αν είναι συμβατό με τον περιβάλλοντα χώρο. Τέλος, λαμβάνοντας υπ΄ όψιν τη μελλοντική επέκταση της εφαρμογής για την υποστήριξη ενός τρισδιάστατου πρότυπου αντικειμένου, ενδιαφέρουσα χρήση της θα μπορούσε να είναι η επαύξηση ενός ημι-κατεστραμμένου αγάλματος ή αρχαίου μνημείου με το τμήμα το οποίο λείπει από τη σημερινή κατάστασή του, για την οπτικοποίηση της μορφής του, όπως ήταν στο παρελθόν.


\section{Περιορισμοί}
During the simulation tests, some limitations and drawbacks emerged, that made us redesign some aspects of our applications. First of all, during blob detection, when the user decides to select and manipulate a virtual chess piece, his hand might be too close to the table, and this has as a result for our program to detect the hand and the table as a single blob, hence the pinch gesture detection fails. These problems are a common thing when using other modern sensors such as the Leap Motion. However, we decided that since, changing the way a blob is detected by the RealSense SDK would be excessive, we could simulate the visual design of a real chessboard which is usually a cube with a speci c height above the tabletop surface. Therefore, in this application, the chessboard is rendered as a grid of multiple cubes with a height of approximately 3,5cm, since this was the minimum value that our program could correctly detect the hand blob. Another problem that has caught our attention during the test simulations, is that due to the wrong detection of a pinch point in 3D space, the user may accidentally move a chess piece in another location than the one he intented to. For example, he may want to move one of his chess pawns 2 squares away from its initial position, but during the translation, the pinch may not be detected and therefore the pawn may move only 1 square away from its initial position. We considered that there may be several solutions for this problem. For instance, instead of the current approach, we could implement a timer effect before a valid move takes place. Then, users would have to pinch for a speci c amount of seconds before a move could be considered as valid. However, we opted not to do this, because we wanted the user to play chess just like in real life and also we could measure and evaluate the wrong and right moves he played during a chess game.

The next limitations were considered for the design, implementation and testing purposes:  As pointed in [40], the Leap Motion’s API stores detected information in a constructed data type that is not modifiable and as there is no easy way to access to a full depth map and potentially correct detected data or make use of the depth map. However, the confidence on the exposed data is enough to work efficiently at the moment.  As seen before, the Vuforia framework is intended to work with mobile devices rather than desktop applications, the see-through concept in AR is fundamental and it cannot be achieved using a desktop screen. However, the integration proposed with mobile devices is difficult at a certain point as the Leap Motion requires a USB connection. It has been managed to send coordinates through a wireless connection but latency issues[54] and the lack of support libraries for mobile platforms requires a higher degree of knowledge to overcome these problems; additionally, the performance in a prototyping stage decreased with this approach. Hence, it has been decided to work in a desktop environment using multi-platform tools, ultimately managing the interaction in a fully controlled environment and without the lack of resources that influence the performance of the application. The details of the proof of concept for mobile devices were explained in the previous chapter 3.7.  The Leap Motion’s limitations at some positions of the hands, the tracking fails when placing the hand perpendicularly to the sensor as the fingers shapes are not in its field of view.The Test No.2 included a deeper analysis, measuring the position data and time, it would include a description of which hands are used in the right/left regions of the space to observe its performance with outside-in/inside-out movements directed to the space, as sometimes, users tend to perform the pinch-grab gesture from outside the range of the sensor and the easiest solution was to use the left-hand to move content on the right and vice versa. However, due to time constraints, it was not possible.

\section{Αξιολόγηση}
\subsection{Αξιολόγηση SUS}
%jimenez
In the System Usability Scale questionnaire, the average score obtained was 70.87 with a standard deviation of 12.60 and median of 71.25 collected from the 20 participants, that according to [51] is an acceptable result (it states to be acceptable above 70). Furthermore, information can be inferred from the questionnaire. The Table 7 shows the results from the SUS, where the columns Q1...Qn present the selected answer from each user and its corresponding calculated Score. As explained in the previous chapter 4, the assessment is based on a ranking for even and odd questions that has certain weight according to the answer selected. The odd questions tend to show positive aspects while the even questions remark negative. In the Figure 33, the average of results for each statement shows that the lowest ranks are even questions, meaning that most of the participants have agreed with the negative aspects such as "the need to learn a lot things before get going well with the system", "unnecessarily complex" ranked the lowest; statements such as the "need for technical support", "inconsistency on the system" and "cumbersome handling" were ranked negatively as well but in a lower degree. This could also have been leaned by users that showed more difficulties during the experiment and thus agree strongly with the negative aspects. Anyhow, the tendency proves the need to solve most of the technical issues present in the implementation that also caused scattered results in the previous test. On the other hand, the positive statements also present good results that lean the balance up. Although they are contradictory between each other (positive and negative), the positives are ranked higher at some degree, however we should consider that the majority of users didn’t have experience with the technologies and a first impression could have lead to more optimistic answers.

\subsection{Ερωτηματολόγια}

As additional support to gather information about the user experience, the open questionnaire, seen on chapter 4 was held to the participants along the SUS test after the conclusion of the experiment. No personal data was collected during this process and only information about the previous user experience, Table 8 was requested. We have decided to categorize the answers from users and conductor’s observations in two sections to summarize the relevant findings: 1. Pinch Gesture Performance: The responsiveness of the gesture was well perceived, nevertheless, when some participants used the hands too close to the borders and tried to grab the objects, the tracking failed, more specially when performing the gesture as a horizontal movement from outside-into the space; the frustration and tiredness caused by keeping the hand in the air while pinching really depended on how the user adapted to the system. People who started to slowly touch the objects performed better, improved easily and didn’t stated problems of tiredness while others that started grabbing it with faster movements revealed more frustration and stated to be tired in some way; the majority agreed on that practise is necessary for control but it was easy to get used to it, just as using another interaction device for the first time, during the series of attempts in both scenes, the users felt more comfortable and performed the task with easiness at some degree. 2. AR Scene perception: The main problem identified during the experiment was the depth perception from the field of view of the user different to the camera. It was identified differently from one user to another, either from front-back and up-down movements, i.e. while certain user is confident on moving the object to the back, in fact is moving it upper. Moreover, in words of a participant, "Once I identify where the object was in relation to the real video using the marker in the table as reference it was easier to get the idea of the virtual world", was a shared though among participants. Here, two main problems are identified, the lack of visual feedback from the environment (not interaction) and the position of the camera in relation to the user’s field of view; The occlusion problem was not perceived as expected, mainly because of the virtual hand overlaying the real one was used to interact with the content, only a few noticed the problem at first sight because, at some point, they were trying to use their real hand instead of the virtual. Additionally, two experienced users gave their opinion about the overall application for improvements, suggesting to deactivate the collision detection when not grabbing the object to avoid bouncing effects produced by the physics engine in Unity. To tackle the problem of depth perception, another user suggested the use of 3D glasses, despite this requires a special display and lenses -which is out of the scope-, could be of interest for latter improvements. The depth perception [54] is a common issue to solve in this scenario, as more visual cues should be implemented to indicate the user how are the objects located in the space, and whether the camera’s field of view should be located at the position of the eye (glasses) to overcome theadjustment of the user’s perception when moving between real and virtual world. However, the use of the real tabletop as reference to perceive the virtual objects in the space succeed at providing the insights of the definition of Augmented Reality.

\section{Συνολική Απόδοση Συστήματος}

